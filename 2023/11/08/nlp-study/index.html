<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="我的NLP学习路径和笔记, REXWind&#39;s Blog">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>我的NLP学习路径和笔记 | REXWind&#39;s Blog</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">REXWind&#39;s Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">REXWind&#39;s Blog</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/REXWindW" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/REXWindW" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('输入密码才给你看！')).toString(CryptoJS.enc.Hex)) {
                alert('密码错啦，再试试看');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/img/photo/flower_window.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">我的NLP学习路径和笔记</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                                <span class="chip bg-color">算法学习笔记</span>
                            </a>
                        
                            <a href="/tags/DL/">
                                <span class="chip bg-color">DL</span>
                            </a>
                        
                            <a href="/tags/NLP/">
                                <span class="chip bg-color">NLP</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/DL/" class="post-category">
                                DL
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2023-11-08
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    26 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="我的NLP学习路径"><a href="#我的NLP学习路径" class="headerlink" title="我的NLP学习路径"></a>我的NLP学习路径</h1><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><ul>
<li>本科all in ACM遗憾打铜，现在有点后悔。导致研一入学时基本零基础，跟着李沐d2l学到第五章后开始学习NLP。</li>
</ul>
<h2 id="一些网址记录"><a href="#一些网址记录" class="headerlink" title="一些网址记录"></a>一些网址记录</h2><ul>
<li>LLM综述<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/612181615">人工智能 LLM 革命破晓：一文读懂当下超大语言模型发展现状 /麦克船长LLM革命系列2 - 知乎 (zhihu.com)</a></li>
<li>李沐的D2L<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/">《动手学深度学习》 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></li>
</ul>
<h2 id="给自己的话"><a href="#给自己的话" class="headerlink" title="给自己的话"></a>给自己的话</h2><h3 id="2023-11-1"><a href="#2023-11-1" class="headerlink" title="2023.11.1"></a>2023.11.1</h3><ul>
<li>刚完成情感分类的任务，感觉自己的代码能力有点差，主要是写的太少了，还是要多结合一下kaggle竞赛之类的练习，coding这块不能落下</li>
</ul>
<h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>这篇公式全<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/114538417">深入浅出Word2Vec原理解析 - 知乎 (zhihu.com)</a></p>
<h3 id="skip-gram模型"><a href="#skip-gram模型" class="headerlink" title="skip-gram模型"></a>skip-gram模型</h3><p>好理解<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27234078">理解 Word2Vec 之 Skip-Gram 模型 - 知乎 (zhihu.com)</a></p>
<ul>
<li><p>概念：window_size，skip_size</p>
</li>
<li><p>第一个线性层对应词向量，因为输入是one hot，所以和linear相乘即取对应行的词向量！因此这个hidden layer的权重被叫做”查找表“（lookup table）。</p>
</li>
<li><p>因为onehot是稀疏的，实际上不用onehot和矩阵相乘，只需要找到lookup table中对应的行，以节省相乘的计算资源。pytorch提供了<code>torch.nn.Embedding</code>。</p>
</li>
<li><p>训练skipgram是个fake task，实际目的是在训练过程中得到这些词向量</p>
</li>
<li><p>negative sampling方法，为什么能减少计算：</p>
</li>
</ul>
<p>先后的两个loss，其中yij代表ij是否成对。原本是对所有词都计算损失，现在是只用对抽取的正样本和负样本计算loss。<br>$$<br>\text{loss} = -\sum_{j=1}^V \left( y_{ij} \log(\sigma(v_i^\top u_j)) + (1 - y_{ij}) \log(1 - \sigma(v_i^\top u_j)) \right)<br>\<br>\text{loss} = -\left( \log(\sigma(v_i^\top u_j)) + \sum_{k=1}^{K} \log(\sigma(-v_i^\top u_k)) \right)<br>$$</p>
<h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><ul>
<li>后面还是看沐神的了，<a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html">14.1. 词嵌入（word2vec） — 动手学深度学习 2.0.0 documentation (d2l.ai)</a>后面CBOW的功能和SkipGram类似，不过是反过来，给上下文求中心词。给定一个大小为左右2m的窗口，利用softmax函数计算中心词的概率。</li>
</ul>
<p>$$<br>P(w_c \mid w_{o_1}, \ldots, w_{o_{2m}}) = \frac{\text{exp}\left(\frac{1}{2m}\mathbf{u}<em>c^\top (\mathbf{v}</em>{o_1} + \ldots, + \mathbf{v}<em>{o</em>{2m}}) \right)}{ \sum_{i \in \mathcal{V}} \text{exp}\left(\frac{1}{2m}\mathbf{u}<em>i^\top (\mathbf{v}</em>{o_1} + \ldots, + \mathbf{v}<em>{o</em>{2m}}) \right)}.<br>$$</p>
<ul>
<li>关于这里的u和v，即对中心词和上下文是两个不同的embedding矩阵进行处理，在训练完成后，取中心词的矩阵作为词向量表。</li>
</ul>
<h3 id="课后问题"><a href="#课后问题" class="headerlink" title="课后问题"></a>课后问题</h3><ul>
<li>让我们以跳元模型为例来思考word2vec设计。跳元模型中两个词向量的点积与余弦相似度之间有什么关系？对于语义相似的一对词，为什么它们的词向量（由跳元模型训练）的余弦相似度可能很高？</li>
</ul>
<pre class="line-numbers language-none"><code class="language-none">因为语义相近的一对词，在训练时上下文对应的词元也比较相似。所以会被训练成比较相近的向量

因为考虑之前计算P的公式是向量点积
如果两个词相近，他们在词向量上：
（1）cosine similarity 接近1
（2）点积较大<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h2 id="近似训练"><a href="#近似训练" class="headerlink" title="近似训练"></a>近似训练</h2><p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_natural-language-processing-pretraining/approx-training.html">14.2. 近似训练 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p>
<ul>
<li>在计算loss的时候softmax需要遍历整个词表，所以词数量大的时候费用高。故引入近似训练</li>
<li>因为使用了sigmoid，为了使其接近1，$\frac{1}{1+e^{-x}}$是x趋近于无穷才好，这样词向量会变得很大，引入随机的噪声（负采样）</li>
</ul>
<p>$$<br>P(w^{(t+j)} \mid w^{(t)}) =P(D=1\mid w^{(t)}, w^{(t+j)})\prod_{k=1,\ w_k \sim P(w)}^K P(D=0\mid w^{(t)}, w_k).<br>$$</p>
<ul>
<li>这样loss计算线性的复杂度依赖于k，而不是整个词表</li>
</ul>
<p>$$<br>\text{Loss}(w_i) = -\log(\sigma(\mathbf{v}<em>c \cdot \mathbf{v}</em>{w_i})) - \sum_{j=1}^{k} \log(\sigma(-\mathbf{v}<em>c \cdot \mathbf{v}</em>{n_j}))<br>$$</p>
<h2 id="word2vec数据集构建"><a href="#word2vec数据集构建" class="headerlink" title="word2vec数据集构建"></a>word2vec数据集构建</h2><p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word-embedding-dataset.html">14.3. 用于预训练词嵌入的数据集 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p>
<ul>
<li>高频词下采样（高频词概率删除）</li>
<li>负采样和下采样要区分一下概念</li>
<li>根据word2vec论文中的建议，将噪声词w的采样概率P(w)设置为其在字典中的相对频率</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">sampling_weights <span class="token operator">=</span> <span class="token punctuation">[</span>counter<span class="token punctuation">[</span>vocab<span class="token punctuation">.</span>to_tokens<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">**</span><span class="token number">0.75</span>
                        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<ul>
<li><p>代码里面很多实现细节，比如这个draw出来的词要确认一下不在上下文中，才能作为负采样。draw也会抽取重复的元素，不用进行去重。</p>
</li>
<li><p>还有这段随机生成的代码，之前不懂为什么这么写，其实这里是个小优化，一次批量的随机生成和context长度相同的随机词，之后调用draw每次返回一个，目的是省资源。</p>
</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">RandomGenerator</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""根据n个采样权重在{1,...,n}中随机抽取"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sampling_weights<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Exclude</span>
        self<span class="token punctuation">.</span>population <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sampling_weights<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>sampling_weights <span class="token operator">=</span> sampling_weights
        self<span class="token punctuation">.</span>candidates <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>i <span class="token operator">=</span> <span class="token number">0</span>

    <span class="token keyword">def</span> <span class="token function">draw</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>i <span class="token operator">==</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>candidates<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># 这里比较精髓，批量生成，单次返回一个</span>
            <span class="token comment"># 缓存k个随机采样结果</span>
            self<span class="token punctuation">.</span>candidates <span class="token operator">=</span> random<span class="token punctuation">.</span>choices<span class="token punctuation">(</span>
                self<span class="token punctuation">.</span>population<span class="token punctuation">,</span> self<span class="token punctuation">.</span>sampling_weights<span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>i <span class="token operator">=</span> <span class="token number">0</span>
        self<span class="token punctuation">.</span>i <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>candidates<span class="token punctuation">[</span>self<span class="token punctuation">.</span>i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span>
   
<span class="token keyword">def</span> <span class="token function">get_negatives</span><span class="token punctuation">(</span>all_contexts<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> counter<span class="token punctuation">,</span> K<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""返回负采样中的噪声词"""</span>
    <span class="token comment"># 索引为1、2、...（索引0是词表中排除的未知标记）</span>
    sampling_weights <span class="token operator">=</span> <span class="token punctuation">[</span>counter<span class="token punctuation">[</span>vocab<span class="token punctuation">.</span>to_tokens<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">**</span><span class="token number">0.75</span>
                        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
    all_negatives<span class="token punctuation">,</span> generator <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> RandomGenerator<span class="token punctuation">(</span>sampling_weights<span class="token punctuation">)</span>
    <span class="token keyword">for</span> contexts <span class="token keyword">in</span> all_contexts<span class="token punctuation">:</span><span class="token comment"># 遍历每组中心词和context</span>
        negatives <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">while</span> <span class="token builtin">len</span><span class="token punctuation">(</span>negatives<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>contexts<span class="token punctuation">)</span> <span class="token operator">*</span> K<span class="token punctuation">:</span> <span class="token comment"># 这里看出这个K参数是生成k倍于context的负采样</span>
            neg <span class="token operator">=</span> generator<span class="token punctuation">.</span>draw<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># 噪声词不能是上下文词</span>
            <span class="token keyword">if</span> neg <span class="token keyword">not</span> <span class="token keyword">in</span> contexts<span class="token punctuation">:</span>
                negatives<span class="token punctuation">.</span>append<span class="token punctuation">(</span>neg<span class="token punctuation">)</span>
        all_negatives<span class="token punctuation">.</span>append<span class="token punctuation">(</span>negatives<span class="token punctuation">)</span>
    <span class="token keyword">return</span> all_negatives

all_negatives <span class="token operator">=</span> get_negatives<span class="token punctuation">(</span>all_contexts<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> counter<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>这里这个batchify感觉也挺巧妙，把之前分开来的center，context和negative整合到一个batch中。它计算所有context和negative的最大长度，然后为了保证batch的长度统一，后面的都用0填充</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">batchify</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""返回带有负采样的跳元模型的小批量样本"""</span>
    max_len <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>c<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token builtin">len</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span> <span class="token keyword">for</span> _<span class="token punctuation">,</span> c<span class="token punctuation">,</span> n <span class="token keyword">in</span> data<span class="token punctuation">)</span>
    centers<span class="token punctuation">,</span> contexts_negatives<span class="token punctuation">,</span> masks<span class="token punctuation">,</span> labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> center<span class="token punctuation">,</span> context<span class="token punctuation">,</span> negative <span class="token keyword">in</span> data<span class="token punctuation">:</span>
        cur_len <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>context<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token builtin">len</span><span class="token punctuation">(</span>negative<span class="token punctuation">)</span>
        centers <span class="token operator">+=</span> <span class="token punctuation">[</span>center<span class="token punctuation">]</span>
        contexts_negatives <span class="token operator">+=</span> \
            <span class="token punctuation">[</span>context <span class="token operator">+</span> negative <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>max_len <span class="token operator">-</span> cur_len<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment"># 填充0</span>
        masks <span class="token operator">+=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> cur_len <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>max_len <span class="token operator">-</span> cur_len<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment"># mask，无效的0填充的mask=0</span>
        labels <span class="token operator">+=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>context<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>max_len <span class="token operator">-</span> <span class="token builtin">len</span><span class="token punctuation">(</span>context<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment"># 正样本1负样本0</span>
    <span class="token comment"># centers展平？</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>centers<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span> 
        contexts_negatives<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>masks<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>关于这个reshape，<code>-1</code> 的意思是根据张量的大小自动推断该维度的大小，而 <code>1</code> 表示该维度的大小为 1。这里的作用和tensor.flatten()是一样的。（多维向量展平）</li>
</ul>
<h2 id="实现word2vec训练"><a href="#实现word2vec训练" class="headerlink" title="实现word2vec训练"></a>实现word2vec训练</h2><p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec-pretraining.html">14.4. 预训练word2vec — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p>
<ul>
<li>之前在处理数据集的时候为了batch大小的一致性，引入mask，这在后面的bceloss有体现，即作为nn.functional.binary_cross_entropy_with_logits的参数传入</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SigmoidBCELoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 带掩码的二元交叉熵损失</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> target<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        out <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>binary_cross_entropy_with_logits<span class="token punctuation">(</span>
            inputs<span class="token punctuation">,</span> target<span class="token punctuation">,</span> weight<span class="token operator">=</span>mask<span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token string">"none"</span><span class="token punctuation">)</span><span class="token comment"># 调用</span>
        <span class="token keyword">return</span> out<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

loss <span class="token operator">=</span> SigmoidBCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>使用bceloss是因为配合负采样的方法。sigmoidbceloss就是将yhat进行sigmoid后再和y计算bceloss</li>
<li>我有点不懂这两个embedding层为什么能加到一个sequential里面去，不是变成顺序执行了吗？</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_embeddings<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                 embedding_dim<span class="token operator">=</span>embed_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_embeddings<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                 embedding_dim<span class="token operator">=</span>embed_size<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>gpt说其实不是，是可以这么用的，这样放在里面并不一定是串行</li>
<li>然后应用这组embeddinglayer的参数，d2l举例的是通过余弦相似度查找最接近的词</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_similar_tokens</span><span class="token punctuation">(</span>query_token<span class="token punctuation">,</span> k<span class="token punctuation">,</span> embed<span class="token punctuation">)</span><span class="token punctuation">:</span>
    W <span class="token operator">=</span> embed<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data
    x <span class="token operator">=</span> W<span class="token punctuation">[</span>vocab<span class="token punctuation">[</span>query_token<span class="token punctuation">]</span><span class="token punctuation">]</span>
    <span class="token comment"># 计算余弦相似性。增加1e-9以获得数值稳定性</span>
    cos <span class="token operator">=</span> torch<span class="token punctuation">.</span>mv<span class="token punctuation">(</span>W<span class="token punctuation">,</span> x<span class="token punctuation">)</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>W <span class="token operator">*</span> W<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span>
                                      torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>x <span class="token operator">*</span> x<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">9</span><span class="token punctuation">)</span>
    topk <span class="token operator">=</span> torch<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>cos<span class="token punctuation">,</span> k<span class="token operator">=</span>k<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'int32'</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> topk<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">:</span>  <span class="token comment"># 删除输入词</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'cosine sim=</span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">float</span><span class="token punctuation">(</span>cos<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>vocab<span class="token punctuation">.</span>to_tokens<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>

get_similar_tokens<span class="token punctuation">(</span><span class="token string">'chip'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>这里出现了两个新的函数，一个是torch.mv，实现了类似于matmul的功能，matmul是矩阵x矩阵，mv只能应用于矩阵x向量，mv针对于这种情况专门优化了</li>
<li>还有topk：topk(k, dim=None, largest=True, sorted=True) 这里输入参数k=3，使用k=k+1是因为第一名肯定是这个词本身吧</li>
</ul>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p><a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_recurrent-modern/lstm.html">9.2. 长短期记忆网络（LSTM） — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p>
<p>[<a target="_blank" rel="noopener" href="https://www.zhihu.com/tardis/zm/art/104475016?source_id=1005">干货]深入浅出LSTM及其Python代码实现 (zhihu.com)</a></p>
<ul>
<li>在循环神经网络RNN中就引入了隐状态的概念,在RNN中是通过隐状态传递之前的信息</li>
</ul>
<p>$$<br>\mathbf{H}<em>t = \phi(\mathbf{X}<em>t \mathbf{W}</em>{xh} + \mathbf{H}</em>{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h)\<br>\mathbf{O}_t = \mathbf{H}<em>t \mathbf{W}</em>{hq} + \mathbf{b}_q.<br>$$</p>
<ul>
<li>在LSTM中，隐状态H和记忆细胞C有什么区别？GPT:H通常用于短期信息的编码，而C则用于长期信息的编码</li>
<li>门单元计算如下,有h个隐藏单元和d的输入大小。这里面X的大小是$d\cdot h$，W的大小$h\cdot h$</li>
</ul>
<p>$$<br>\begin{split}\begin{aligned}<br>\mathbf{I}<em>t &amp;= \sigma(\mathbf{X}<em>t \mathbf{W}</em>{xi} + \mathbf{H}</em>{t-1} \mathbf{W}_{hi} + \mathbf{b}<em>i),\<br>\mathbf{F}<em>t &amp;= \sigma(\mathbf{X}<em>t \mathbf{W}</em>{xf} + \mathbf{H}</em>{t-1} \mathbf{W}</em>{hf} + \mathbf{b}<em>f),\<br>\mathbf{O}<em>t &amp;= \sigma(\mathbf{X}<em>t \mathbf{W}</em>{xo} + \mathbf{H}</em>{t-1} \mathbf{W}</em>{ho} + \mathbf{b}_o),<br>\end{aligned}\end{split}<br>$$</p>
<ul>
<li>隐状态H是一直传递下去，C的传递收到几个门控的影响。</li>
</ul>
<p><img src="https://zh-v2.d2l.ai/_images/lstm-3.svg"></p>
<ul>
<li><p>图里面涉及记忆细胞更新的部分使用tanh，即输出在-1到1之间，而其他门控使用sigmoid控制在01之间</p>
</li>
<li><p>对应到代码里很直观</p>
</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">lstm</span><span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> state<span class="token punctuation">,</span> params<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token punctuation">[</span>W_xi<span class="token punctuation">,</span> W_hi<span class="token punctuation">,</span> b_i<span class="token punctuation">,</span> W_xf<span class="token punctuation">,</span> W_hf<span class="token punctuation">,</span> b_f<span class="token punctuation">,</span> W_xo<span class="token punctuation">,</span> W_ho<span class="token punctuation">,</span> b_o<span class="token punctuation">,</span> W_xc<span class="token punctuation">,</span> W_hc<span class="token punctuation">,</span> b_c<span class="token punctuation">,</span>
     W_hq<span class="token punctuation">,</span> b_q<span class="token punctuation">]</span> <span class="token operator">=</span> params
    <span class="token punctuation">(</span>H<span class="token punctuation">,</span> C<span class="token punctuation">)</span> <span class="token operator">=</span> state
    outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> X <span class="token keyword">in</span> inputs<span class="token punctuation">:</span>
        I <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xi<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_hi<span class="token punctuation">)</span> <span class="token operator">+</span> b_i<span class="token punctuation">)</span>
        F <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xf<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_hf<span class="token punctuation">)</span> <span class="token operator">+</span> b_f<span class="token punctuation">)</span>
        O <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xo<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_ho<span class="token punctuation">)</span> <span class="token operator">+</span> b_o<span class="token punctuation">)</span> <span class="token comment"># 这里的O和RNN中的O不同，是个门，RNN中的O直接是输出</span>
        C_tilda <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xc<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_hc<span class="token punctuation">)</span> <span class="token operator">+</span> b_c<span class="token punctuation">)</span> <span class="token comment"># 候选记忆元</span>
        C <span class="token operator">=</span> F <span class="token operator">*</span> C <span class="token operator">+</span> I <span class="token operator">*</span> C_tilda <span class="token comment"># 记住多少</span>
        H <span class="token operator">=</span> O <span class="token operator">*</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>C<span class="token punctuation">)</span>
        Y <span class="token operator">=</span> <span class="token punctuation">(</span>H @ W_hq<span class="token punctuation">)</span> <span class="token operator">+</span> b_q <span class="token comment"># 注意这里的输出就是把隐状态乘上Hq再加偏置</span>
        outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Y<span class="token punctuation">)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>H<span class="token punctuation">,</span> C<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h2 id="实战：transformers库实现电影情感分析"><a href="#实战：transformers库实现电影情感分析" class="headerlink" title="实战：transformers库实现电影情感分析"></a>实战：transformers库实现电影情感分析</h2><p>目前打算跟pku的项目列表写一次<a target="_blank" rel="noopener" href="https://github.com/PKU-TANGENT/nlp-tutorial">PKU-TANGENT/nlp-tutorial: NLP新手入门教程 (github.com)</a></p>
<ul>
<li><p>kaggle竞赛地址：<a target="_blank" rel="noopener" href="https://www.kaggle.com/competitions/sentiment-analysis-on-movie-reviews">Sentiment Analysis on Movie Reviews | Kaggle</a></p>
</li>
<li><p>这里发现一个很好的博客<a target="_blank" rel="noopener" href="https://jinhanlei.github.io/">Jin blog (jinhanlei.github.io)</a>讲了transformers和pytorch的配合使用</p>
</li>
<li><p>主体代码参考这段<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40605573/article/details/113140270">【精选】（六）使用Transformers进行情感分析_基于transformer的情感分析-CSDN博客</a></p>
</li>
</ul>
<h3 id="写的过程中遇到的问题"><a href="#写的过程中遇到的问题" class="headerlink" title="写的过程中遇到的问题"></a>写的过程中遇到的问题</h3><ul>
<li>关于dataloder怎么移动到gpu上，其实是在训练代码中读入之后再移动到device</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://geek-docs.com/pytorch/pytorch-questions/526_pytorch_load_pytorch_dataloader_into_gpu.html">Pytorch 将Pytorch的Dataloader加载到GPU中|极客教程 (geek-docs.com)</a></p>
<ul>
<li>用tqdm显示进度条</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43941438/article/details/128171441">深度学习Pytorch通过tqdm实现进度条打印训练过程信息_pytorch训练进度条-CSDN博客</a></p>
<h3 id="工作总结"><a href="#工作总结" class="headerlink" title="工作总结"></a>工作总结</h3><ul>
<li>基本上模型结构照搬博客里给的内容，输出层从单层的Linear改成了一个1024隐藏层的MLP。</li>
<li>博客中是输出0/1表示positive和negative，kaggle中的是一个5分类问题，我这里通过MLP输出到一个1*5的大小再通过softmax，把label变成onehot编码后计算损失。输出结果用argmax。</li>
<li>原文中使用了torchtext.data，但是这个包在0.9.0后被移除了，先处理好格式后再使用torch的dataloader也挺方便。</li>
</ul>
<h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><ul>
<li>在BiLSTM中，隐状态H是正向和反向两个LSTM的输出拼接。<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47802053">详解BiLSTM及代码实现 - 知乎 (zhihu.com)</a></li>
<li>ELMo中embedding使用的是卷积的方法<a target="_blank" rel="noopener" href="https://blog.csdn.net/Magical_Bubble/article/details/89160032">ELMo解读（论文 + PyTorch源码）_elmo代码-CSDN博客</a></li>
<li>这个比较好懂,而且解释的非常详细。<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/51679783">ELMo原理解析及简单上手使用 - 知乎 (zhihu.com)</a></li>
<li>ELMo大致了解就是一个多层的BiLM，对同一层中的正反LSTM进行concat连接，然后给每一层的输出加权相加得到最终输出。</li>
</ul>
<p>$$<br>h_{k, j}^{L M}=\left[\overrightarrow{h_{k, j}^{L M}} ; \overleftarrow{h_{k, j}^{L M}}\right]<br>\<br>\mathbf{E L M o}<em>k^{\text {task }}=E\left(R_k ; \Theta^{\text {task }}\right)=\gamma^{\text {task }} \sum</em>{j=0}^L s_j^{\text {task }} \mathbf{h}_{k, j}^{L M}<br>$$</p>
<ul>
<li>RNN，LSTM的长程梯度消失问题，对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有有效信息，即便 LSTM 加了门控机制可以选择性遗忘和记忆，随着所需翻译的句子难度怎能更加，这个结构的效果仍然不理想。</li>
</ul>
<h2 id="多头自注意力"><a href="#多头自注意力" class="headerlink" title="多头自注意力"></a>多头自注意力</h2><ul>
<li>关于多头注意力，看李沐的教学<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/multihead-attention.html">10.5. 多头注意力 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></li>
<li>沐神的代码：</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#@save</span>
<span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""多头注意力"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> key_size<span class="token punctuation">,</span> query_size<span class="token punctuation">,</span> value_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span>
                 num_heads<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> d2l<span class="token punctuation">.</span>DotProductAttention<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>query_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_k <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>key_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_v <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>value_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_o <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>使用GPT让他对forward函数的中间变量大小进行标注</li>
<li><code>B</code>: batch_size</li>
<li><code>Tq</code>: 查询的个数（queries）</li>
<li><code>Tk</code>: 键的个数（keys）</li>
<li><code>Tv</code>: 值的个数（values）</li>
<li><code>H</code>: 注意力头的个数（num_heads）</li>
<li><code>D</code>: 隐藏单元的个数（num_hiddens）</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> queries<span class="token punctuation">,</span> keys<span class="token punctuation">,</span> values<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># queries，keys，values的形状:</span>
    <span class="token comment"># (B，Tq，D)， (B，Tk，D)， (B，Tv，D)</span>
    <span class="token comment"># valid_lens的形状:</span>
    <span class="token comment"># (B,) 或 (B，Tq)</span>
    <span class="token comment"># 经过变换后，输出的queries，keys，values的形状:</span>
    <span class="token comment"># (B*H，Tq，D/H)， (B*H，Tk，D/H)， (B*H，Tv，D/H)</span>
    queries <span class="token operator">=</span> transpose_qkv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_q<span class="token punctuation">(</span>queries<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>  <span class="token comment"># (B*H, Tq, D/H)</span>
    keys <span class="token operator">=</span> transpose_qkv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_k<span class="token punctuation">(</span>keys<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>        <span class="token comment"># (B*H, Tk, D/H)</span>
    values <span class="token operator">=</span> transpose_qkv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_v<span class="token punctuation">(</span>values<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>    <span class="token comment"># (B*H, Tv, D/H)</span>

    <span class="token keyword">if</span> valid_lens <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token comment"># 在轴0，将第一项（标量或者矢量）复制H次，</span>
        <span class="token comment"># 然后如此复制第二项，然后诸如此类。</span>
        valid_lens <span class="token operator">=</span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>
            valid_lens<span class="token punctuation">,</span> repeats<span class="token operator">=</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># (B*H,)</span>

    <span class="token comment"># output的形状: (B*H, Tq, D/H)</span>
    output <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>queries<span class="token punctuation">,</span> keys<span class="token punctuation">,</span> values<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span>

    <span class="token comment"># output_concat的形状: (B, Tq, D)</span>
    output_concat <span class="token operator">=</span> transpose_output<span class="token punctuation">(</span>output<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>W_o<span class="token punctuation">(</span>output_concat<span class="token punctuation">)</span>  <span class="token comment"># (B, Tq, D)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>其实代码很难懂，但是评论区有人画了一张图来表示中间shape的变化，非常形象</li>
</ul>
<p><img src="/img/nlp_study/multihead_attention.png" alt="multihead_attention"></p>
<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><ul>
<li>代码依旧是看李沐d2l的代码<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/bert.html">14.8. 来自Transformers的双向编码器表示（BERT） — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></li>
<li>这篇相对详细 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/403495863">读懂BERT，看这一篇就够了 - 知乎 (zhihu.com)</a></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_tokens_and_segments</span><span class="token punctuation">(</span>tokens_a<span class="token punctuation">,</span> tokens_b<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""获取输入序列的词元及其片段索引"""</span>
    tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'&lt;cls&gt;'</span><span class="token punctuation">]</span> <span class="token operator">+</span> tokens_a <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">'&lt;sep&gt;'</span><span class="token punctuation">]</span>
    <span class="token comment"># 0和1分别标记片段A和B</span>
    segments <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>tokens_a<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment"># A句的分割嵌入为0</span>
    <span class="token keyword">if</span> tokens_b <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        tokens <span class="token operator">+=</span> tokens_b <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">'&lt;sep&gt;'</span><span class="token punctuation">]</span>
        segments <span class="token operator">+=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>tokens_b<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># B句的设置为1</span>
    <span class="token keyword">return</span> tokens<span class="token punctuation">,</span> segments<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="三种embedding"><a href="#三种embedding" class="headerlink" title="三种embedding"></a>三种embedding</h3><ul>
<li>bert的输入是句子的三层表征：token，分割（a句/b句），位置。对应到代码中的token_embedding,segement_embedding和pos_embedding(是learnable的参数)</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">BERTEncoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""BERT编码器"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> norm_shape<span class="token punctuation">,</span> ffn_num_input<span class="token punctuation">,</span>
                 ffn_num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span>
                 max_len<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> key_size<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span> query_size<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span> value_size<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span>
                 <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>BERTEncoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>token_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>segment_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>blks <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 串联encoder block</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>blks<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>EncoderBlock<span class="token punctuation">(</span>
                key_size<span class="token punctuation">,</span> query_size<span class="token punctuation">,</span> value_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> norm_shape<span class="token punctuation">,</span>
                ffn_num_input<span class="token punctuation">,</span> ffn_num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数</span>
        self<span class="token punctuation">.</span>pos_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> max_len<span class="token punctuation">,</span>
                                                      num_hiddens<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token punctuation">,</span> segments<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）</span>
        X <span class="token operator">=</span> self<span class="token punctuation">.</span>token_embedding<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>segment_embedding<span class="token punctuation">(</span>segments<span class="token punctuation">)</span>
        X <span class="token operator">=</span> X <span class="token operator">+</span> self<span class="token punctuation">.</span>pos_embedding<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment"># 加入位置编码</span>
        <span class="token keyword">for</span> blk <span class="token keyword">in</span> self<span class="token punctuation">.</span>blks<span class="token punctuation">:</span>
            X <span class="token operator">=</span> blk<span class="token punctuation">(</span>X<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span>
        <span class="token keyword">return</span> X<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>从代码中也可以看出，bert的embedding是三种embedding的直接求和</li>
<li>因为NLP没有CV的Imagenet这样庞大数据集，所以bert主要还是采用无监督学习的方式。两个任务：Masked Language Model（随机添加mask/替换句子）和Next Sentence Prediction（预测两个句子是不是连在一起）</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MaskLM</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""BERT的掩蔽语言模型任务"""</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_inputs<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MaskLM<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>mlp <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                 nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                 nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                 nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> pred_positions<span class="token punctuation">)</span><span class="token punctuation">:</span>
        num_pred_positions <span class="token operator">=</span> pred_positions<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        pred_positions <span class="token operator">=</span> pred_positions<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 输入是2维的所以reshape一下</span>
        batch_size <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        batch_idx <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
        batch_idx <span class="token operator">=</span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>batch_idx<span class="token punctuation">,</span> num_pred_positions<span class="token punctuation">)</span> <span class="token comment"># 将batch_idx中的每个元素重复num_pred_positions次</span>
        <span class="token comment"># 假设batch_size=2，num_pred_positions=3</span>
        <span class="token comment"># 那么batch_idx是np.array（[0,0,0,1,1,1]）</span>
        
        masked_X <span class="token operator">=</span> X<span class="token punctuation">[</span>batch_idx<span class="token punctuation">,</span> pred_positions<span class="token punctuation">]</span>
        <span class="token comment"># 在每个batch中找到对应被掩盖的位置的输出，单独拎出来</span>
        masked_X <span class="token operator">=</span> masked_X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_pred_positions<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># masked_X shape: [batch_size, sequence_length, feature_dim]</span>
    		<span class="token comment"># 一开始有疑问为什么没有用mask替换的代码</span>
        <span class="token comment"># 因为已经在数据预处理阶段被替换为掩盖标记</span>
        mlm_Y_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>masked_X<span class="token punctuation">)</span> <span class="token comment"># 经过一个MLP得到预测</span>
        <span class="token keyword">return</span> mlm_Y_hat
      
      
mlm <span class="token operator">=</span> MaskLM<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span>
mlm_positions <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># batchsize = 2</span>
mlm_Y_hat <span class="token operator">=</span> mlm<span class="token punctuation">(</span>encoded_X<span class="token punctuation">,</span> mlm_positions<span class="token punctuation">)</span> 

<span class="token comment"># 计算损失</span>
mlm_Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>
mlm_l <span class="token operator">=</span> loss<span class="token punctuation">(</span>mlm_Y_hat<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> mlm_Y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>上面很多理解都写到注释里面去了</li>
</ul>
<h3 id="我的疑问"><a href="#我的疑问" class="headerlink" title="我的疑问"></a>我的疑问</h3><ul>
<li>问：那这些被mask的位置之间可能相隔很远没有关联，每组不同的输入他们的位置关系也会变化，为什么要传入同一个MLP中呢？</li>
</ul>
<pre class="line-numbers language-none"><code class="language-none">GPT：
- 您提出的观点是正确的：在掩蔽语言模型（Masked Language Model, MLM）任务中，被掩盖的位置可能在输入文本中相隔很远，它们之间可能没有直接的关联。此外，不同的输入示例可能具有不同的掩盖位置，因此这些位置之间的关系会变化。
- MLP模型的目标是理解上下文并预测掩盖位置上的单词。尽管这些位置之间可能没有直接的关联，但上下文中的其他单词提供了相关信息，帮助模型进行预测。共享的MLP可以捕获这些上下文信息。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<ul>
<li>感觉这个回答还是有点玄学的，不过从代码来看，确实是直接把pred_position位置的特征单独拿出来，不管他们在句中的位置间隔是否固定，直接一起传到MLP里面去。</li>
</ul>
<p><img src="https://pic3.zhimg.com/80/v2-55dfeb40658afd90b10e039c64e0ec06_1440w.webp"></p>
<ul>
<li><p>这里也解释了为什么引入CLS token：对于一些token级别的任务<em>（如，序列标注和问答任务）</em>，就把Ti输入到额外的输出层中进行预测。对于一些句子级别的任务<em>（如，自然语言推断和情感分类任务）</em>，就把C输入到额外的输出层中，这里也就解释了为什么要在每一个token序列前都要插入特定的分类token。</p>
</li>
<li><p>关于elmo，bert，GPT1的区别</p>
</li>
</ul>
<p><img src="https://pic1.zhimg.com/80/v2-1cd7dfb971e9812bb11bcfc4d5e5cc2c_1440w.webp"></p>
<h2 id="Huggingface-Transformers"><a href="#Huggingface-Transformers" class="headerlink" title="Huggingface Transformers"></a>Huggingface Transformers</h2><ul>
<li>在transformers库源代码目录下src/transformers/下有各种主要功能的代码，比如/src/transformers/models/bert/modeling_bert.py中的bert源码</li>
<li>这个是transformers库文件结构的介绍视频，里面一整个系列都是带读transformers库源码的，之后有时间再看。<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Tm4y1M7nQ">https://www.bilibili.com/video/BV1Tm4y1M7nQ</a></li>
<li>关于transformers库如何配合pytorch使用<a target="_blank" rel="noopener" href="https://jinhanlei.github.io/posts/Transformers%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8-%E5%9B%9B-%E7%BB%93%E5%90%88Transformers%E5%92%8CPyTorch%E4%BF%AE%E6%94%B9%E6%A8%A1%E5%9E%8B/">Transformers快速入门（四）：结合Transformers和PyTorch修改模型 | Jin blog (jinhanlei.github.io)</a></li>
</ul>
<h3 id="Transformers库BLIP2代码阅读"><a href="#Transformers库BLIP2代码阅读" class="headerlink" title="Transformers库BLIP2代码阅读"></a>Transformers库BLIP2代码阅读</h3><h4 id="视觉embedding部分"><a href="#视觉embedding部分" class="headerlink" title="视觉embedding部分"></a>视觉embedding部分</h4><ul>
<li>通过一个ViT，把不同分辨率的图片映射</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Blip2VisionEmbeddings</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> Blip2VisionConfig<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config
        self<span class="token punctuation">.</span>embed_dim <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size
        self<span class="token punctuation">.</span>image_size <span class="token operator">=</span> config<span class="token punctuation">.</span>image_size
        self<span class="token punctuation">.</span>patch_size <span class="token operator">=</span> config<span class="token punctuation">.</span>patch_size

        self<span class="token punctuation">.</span>class_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># CLS token，是可学习的</span>

        self<span class="token punctuation">.</span>patch_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span> 
            in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span>self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>self<span class="token punctuation">.</span>patch_size<span class="token punctuation">,</span> stride<span class="token operator">=</span>self<span class="token punctuation">.</span>patch_size
        <span class="token punctuation">)</span> <span class="token comment"># 注意这里stride == patch_size，说明是每个patch和卷积核卷积提取特征</span>
        <span class="token comment"># 从3个通道到embed_dim个通道，是为了映射到和单词同样的维度。这里</span>

        self<span class="token punctuation">.</span>num_patches <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>image_size <span class="token operator">//</span> self<span class="token punctuation">.</span>patch_size<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span> <span class="token comment"># 适用于不同分辨率的图片，图片变大则patch变多</span>
        self<span class="token punctuation">.</span>num_positions <span class="token operator">=</span> self<span class="token punctuation">.</span>num_patches <span class="token operator">+</span> <span class="token number">1</span>

        self<span class="token punctuation">.</span>position_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_positions<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 使用可学习的位置编码</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pixel_values<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        batch_size <span class="token operator">=</span> pixel_values<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        target_dtype <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>dtype
        patch_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embedding<span class="token punctuation">(</span>pixel_values<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>target_dtype<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># shape = [*, width, grid, grid]</span>
        <span class="token comment"># 通过卷积，分patch</span>
        patch_embeds <span class="token operator">=</span> patch_embeds<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>

        class_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>class_embedding<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>target_dtype<span class="token punctuation">)</span>
        embeddings <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>class_embeds<span class="token punctuation">,</span> patch_embeds<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 把CLS token和图片embedding 接在一起</span>
        embeddings <span class="token operator">=</span> embeddings <span class="token operator">+</span> self<span class="token punctuation">.</span>position_embedding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span> embeddings<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>target_dtype<span class="token punctuation">)</span> <span class="token comment"># 加上位置编码</span>
        <span class="token keyword">return</span> embeddings<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Blip2Attention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Multi-headed attention from 'Attention Is All You Need' paper"""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config
        self<span class="token punctuation">.</span>embed_dim <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> config<span class="token punctuation">.</span>num_attention_heads
        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> self<span class="token punctuation">.</span>embed_dim <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>head_dim <span class="token operator">*</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">!=</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
                <span class="token string-interpolation"><span class="token string">f"embed_dim must be divisible by num_heads (got `embed_dim`: </span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">}</span></span><span class="token string"> and `num_heads`:"</span></span>
                <span class="token string-interpolation"><span class="token string">f" </span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">}</span></span><span class="token string">)."</span></span>
            <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>scale <span class="token operator">=</span> self<span class="token punctuation">.</span>head_dim<span class="token operator">**</span><span class="token operator">-</span><span class="token number">0.5</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>config<span class="token punctuation">.</span>attention_dropout<span class="token punctuation">)</span>

        <span class="token comment"># small tweak here compared to CLIP, no bias here</span>
        self<span class="token punctuation">.</span>qkv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">,</span> <span class="token number">3</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token comment"># 不单独建Wq，Wk，Wv，直接和在一起和input相乘，所以是3*embed_dim</span>
        <span class="token comment"># 如果是单独一个q，就是q = nn.Linear(self.embed_dim, self.embed_dim, bias=False)</span>

        <span class="token keyword">if</span> config<span class="token punctuation">.</span>qkv_bias<span class="token punctuation">:</span>
            q_bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 参数设置是否使用bias</span>
            v_bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            q_bias <span class="token operator">=</span> <span class="token boolean">None</span>
            v_bias <span class="token operator">=</span> <span class="token boolean">None</span>

        <span class="token keyword">if</span> q_bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            qkv_bias <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>q_bias<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>v_bias<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span> v_bias<span class="token punctuation">)</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>qkv<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>qkv_bias<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>projection <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span>
        <span class="token comment"># 模态投影层projection layer</span>

    <span class="token keyword">def</span> <span class="token function">_shape</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tensor<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> seq_len<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> bsz<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> tensor<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>
        head_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        output_attentions<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Input shape: Batch x Time x Channel"""</span>

        bsz<span class="token punctuation">,</span> tgt_len<span class="token punctuation">,</span> embed_dim <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>

        mixed_qkv <span class="token operator">=</span> self<span class="token punctuation">.</span>qkv<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span> <span class="token comment"># 这里为了方便计算，不单独建Wq，Wk，Wv，直接和在一起和input相乘</span>

        mixed_qkv <span class="token operator">=</span> mixed_qkv<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> tgt_len<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> embed_dim <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span>
            <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span>
        <span class="token punctuation">)</span> <span class="token comment"># 把维度重排，原本2维度上拼接起来的qkv结果，这里给他移动到第0维，方便后面拆分</span>
        <span class="token comment"># 这里的5个维度：qkv拼接，batch大小，注意力头数，序列长度，embed_dim</span>
        query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">,</span> value_states <span class="token operator">=</span> mixed_qkv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> mixed_qkv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> mixed_qkv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
        <span class="token comment"># 在第0维上拆分成q k v</span>

        <span class="token comment"># Take the dot product between "query" and "key" to get the raw attention scores.</span>
        attention_scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 相当于q(seq_len, embed_dim) * k(embed_dim, seq_len) 相乘</span>

        attention_scores <span class="token operator">=</span> attention_scores <span class="token operator">*</span> self<span class="token punctuation">.</span>scale
        <span class="token comment"># 缩放操作是为了确保在计算 softmax 函数时，分数不会因为维度的不同而导致数值问题</span>

        <span class="token comment"># Normalize the attention scores to probabilities.</span>
        attention_probs <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attention_scores<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 通过softmax计算得到attention_probs</span>

        <span class="token comment"># This is actually dropping out entire tokens to attend to, which might</span>
        <span class="token comment"># seem a bit unusual, but is taken from the original Transformer paper.</span>
        attention_probs <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>attention_probs<span class="token punctuation">)</span>
        <span class="token comment"># 这里介绍了attention进行dropout的方式，直接对attention_prob进行dropout，而不是计算后再对结果dropout</span>

        <span class="token comment"># Mask heads if we want to</span>
        <span class="token keyword">if</span> head_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            attention_probs <span class="token operator">=</span> attention_probs <span class="token operator">*</span> head_mask

        context_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attention_probs<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

        new_context_layer_shape <span class="token operator">=</span> context_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">,</span><span class="token punctuation">)</span>
        context_layer <span class="token operator">=</span> context_layer<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>new_context_layer_shape<span class="token punctuation">)</span>

        output <span class="token operator">=</span> self<span class="token punctuation">.</span>projection<span class="token punctuation">(</span>context_layer<span class="token punctuation">)</span>
        <span class="token comment"># 经过projection layer输出结果</span>

        outputs <span class="token operator">=</span> <span class="token punctuation">(</span>output<span class="token punctuation">,</span> attention_probs<span class="token punctuation">)</span> <span class="token keyword">if</span> output_attentions <span class="token keyword">else</span> <span class="token punctuation">(</span>output<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> outputs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>值得注意的是里面把WqWkWv相接到同一个Linear中，输入经过线性层后再拆分出qkv。</li>
<li>而且在attention中的dropout，直接对attention_prob进行dropout，而不是计算后再对结果dropout</li>
</ul>
<h2 id="阅读LLM综述"><a href="#阅读LLM综述" class="headerlink" title="阅读LLM综述"></a>阅读LLM综述</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/612181615">人工智能 LLM 革命破晓：一文读懂当下超大语言模型发展现状 /麦克船长LLM革命系列2 - 知乎 (zhihu.com)</a></p>
<ul>
<li>之前一直不理解为什么叫few-shot prompt：在无监督训练好的 GPT-3，使用时用少量示例（加在prompt里面）就可以得到有较好的输出反馈，这就叫 Few-Shot Prompt。pretrain后的模型通过finetune或者In-context learning完成不同类型的任务，又分zeroshot，oneshot，fewshot，在综述的11.5节有详细区分。</li>
</ul>
<h2 id="topk，topp和tempreture"><a href="#topk，topp和tempreture" class="headerlink" title="topk，topp和tempreture"></a>topk，topp和tempreture</h2><ul>
<li><p>参考这一篇博客<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/613428710">Top-k &amp; Top-p, Temperature - 知乎 (zhihu.com)</a></p>
</li>
<li><p>topk即从前k个高概率的token中随机选，topp是从概率和为topp的前几个token中随机，而tempreture是在softmax中除上了一个系数T，来控制模型的随机性，T越大生成的内容越随机</p>
</li>
</ul>
<h2 id="RoPE编码"><a href="#RoPE编码" class="headerlink" title="RoPE编码"></a>RoPE编码</h2><ul>
<li>参考1<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/647832706">超超超超超简单！从结果推导RoPE旋转位置编码 - 知乎 (zhihu.com)</a></li>
<li>参考2<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/641274061">【论文阅读】RoPE为何成为大模型中最常见的位置编码？ - 知乎 (zhihu.com)</a></li>
<li>$\langle f(q, m), f(k, n) \rangle = g(q, k, m - n)$怎么理解？</li>
<li>f(q,m)和f(k,n)表示查询qk的位置编码，做内积之后，要和g(q, k, m-n)算出来的相等，即结果是和相对位置m-n相关的，要找这样一个映射（位置编码方式）f。</li>
<li>一开始的疑问是，为什么$e^{m\theta i}$和$e^{n\theta i}$乘积不是$e^{(m+n)\theta i}$而是$e^{(m-n)\theta i}$，因为<strong>两个复向量q，k的内积等于q乘（k的共轭复数），再取实部</strong>即$\langle q,k\rangle = Re(q^*k)$。复数内积参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43800577/article/details/126869092">复向量的内积（例题详解）-CSDN博客</a></li>
</ul>
<p><img src="/img/nlp_study/neiji.png" alt="neiji"></p>
<ul>
<li><p>共轭复数的定义是将复数的虚部取负值,对于复数 $re^{\theta i}$，其共轭复数为$re^{-\theta i}$。</p>
</li>
<li><p>感觉论文中的2D的例子，可以发现</p>
</li>
</ul>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">REXWind</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://example.com/2023/11/08/nlp-study/">http://example.com/2023/11/08/nlp-study/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">REXWind</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                                    <span class="chip bg-color">算法学习笔记</span>
                                </a>
                            
                                <a href="/tags/DL/">
                                    <span class="chip bg-color">DL</span>
                                </a>
                            
                                <a href="/tags/NLP/">
                                    <span class="chip bg-color">NLP</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">要恰饭的嘛</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    
        <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; padding-left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: 'bec4341381595d92c4ac',
        clientSecret: 'ab90d7a821a71c6e4d0468e8fad3713614a46f6b',
        repo: 'REXWindW.github.io',
        owner: 'REXWindW',
        admin: "REXWindW",
        id: '2023-11-08T09-00-00',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2024/03/05/llm-study/">
                    <div class="card-image">
                        
                        <img src="/img/photo/tower.jpg" class="responsive-img" alt="llama代码解读（一）LlamaDecoderLayer">
                        
                        <span class="card-title">llama代码解读（一）LlamaDecoderLayer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            我的LLM学习路径
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-03-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/DL/" class="post-category">
                                    DL
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                        <span class="chip bg-color">算法学习笔记</span>
                    </a>
                    
                    <a href="/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                    <a href="/tags/DL/">
                        <span class="chip bg-color">DL</span>
                    </a>
                    
                    <a href="/tags/NLP/">
                        <span class="chip bg-color">NLP</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2023/10/09/d2l-practice/">
                    <div class="card-image">
                        
                        <img src="/img/erciyuan/xiaoyang.png" class="responsive-img" alt="李沐d2l课后练习">
                        
                        <span class="card-title">李沐d2l课后练习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            课后联系
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2023-10-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/DL/" class="post-category">
                                    DL
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
                        <span class="chip bg-color">算法学习笔记</span>
                    </a>
                    
                    <a href="/tags/DL/">
                        <span class="chip bg-color">DL</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2020-2025</span>
            
            <span id="year">2020</span>
            <a href="/about" target="_blank">REXWind</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">79.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2020";
                        var startMonth = "10";
                        var startDate = "20";
                        var startHour = "0";
                        var startMinute = "42";
                        var startSecond = "13";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/REXWindW" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:114864532@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1114864532" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1114864532" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
