<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>吃饭日记2509</title>
      <link href="2025/09/30/food2509/"/>
      <url>2025/09/30/food2509/</url>
      
        <content type="html"><![CDATA[<h1 id="吃饭日记2509"><a href="#吃饭日记2509" class="headerlink" title="吃饭日记2509"></a>吃饭日记2509</h1><h2 id="我吃了什么"><a href="#我吃了什么" class="headerlink" title="我吃了什么"></a>我吃了什么</h2><h3 id="比格披萨自助"><a href="#比格披萨自助" class="headerlink" title="比格披萨自助"></a>比格披萨自助</h3><ul><li>69块钱，好像晚到一点还能优惠到59乍一看还挺划算</li><li>但是太容易吃腻了，最后感觉都没吃几块披萨</li><li>披萨的种类好多，但是好多都很黑暗啊而且料不是</li></ul><p><img src="/img/food2509/pisa1.jpg" alt="pisa1"></p><ul><li>很恐怖的山楂披萨</li></ul><p><img src="/img/food2509/pisa2.jpg" alt="pisa2"></p><ul><li>但是同一个商场里买的铜锣烧很好吃哈哈哈，最喜欢吃抹茶味的东西了。</li></ul><p><img src="/img/food2509/tongluoshao.jpg" alt="tongluoshao"></p><h3 id="日料"><a href="#日料" class="headerlink" title="日料"></a>日料</h3><ul><li>好亏的一家日料，店好偏没什么人，牛丼肉也不是很多</li><li>不过味道勉强还行</li><li>可惜住的地方太偏了，其他日料店或者卖牛丼的店都好远好远，也只能吃这家解解馋了</li></ul><p><img src="/img/food2509/niudong.jpg" alt="niudong"></p><ul><li>吃到了最喜欢吃的鹅肝手握</li></ul><p><img src="/img/food2509/egan.jpg" alt="egan"></p><ul><li>寿司拼盘感觉沙拉酱好多，没有那么好吃（这家吃了两次，好亏啊TAT）</li></ul><p><img src="/img/food2509/sushi.jpg" alt="sushi"></p><h3 id="螺蛳粉"><a href="#螺蛳粉" class="headerlink" title="螺蛳粉"></a>螺蛳粉</h3><ul><li>街边螺蛳粉，疯狂加了好多料也才二十多，好吃不贵！</li><li>这家的汤好浓啊，和之前杭电吃的还是区别挺大的。</li></ul><p><img src="/img/food2509/luoshifen.jpg" alt="luoshifen"></p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吃饭 </tag>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>llama代码解读（二）</title>
      <link href="2024/03/19/llm-study-2/"/>
      <url>2024/03/19/llm-study-2/</url>
      
        <content type="html"><![CDATA[<h1 id="llama代码解读（二）"><a href="#llama代码解读（二）" class="headerlink" title="llama代码解读（二）"></a>llama代码解读（二）</h1><p>参考Llama源码深入解析 - 知乎 (zhihu.com)](<a href="https://zhuanlan.zhihu.com/p/648365207">https://zhuanlan.zhihu.com/p/648365207</a>)</p><ul><li>上篇主要介绍了llama的解码层（Attention+MLP)</li></ul><h2 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h2><h3 id="因果掩码Casual-Mask"><a href="#因果掩码Casual-Mask" class="headerlink" title="因果掩码Casual Mask"></a>因果掩码Casual Mask</h3><ul><li>用于构造一个上三角的mask，实现模型只能看到前面的</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_make_causal_mask</span><span class="token punctuation">(</span>    input_ids_shape<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">,</span> dtype<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span>     device<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">,</span> past_key_values_length<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Make causal mask used for bi-directional self-attention.    """</span>    bsz<span class="token punctuation">,</span> tgt_len <span class="token operator">=</span> input_ids_shape    mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">(</span>tgt_len<span class="token punctuation">,</span> tgt_len<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>finfo<span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>    <span class="token comment"># 这里用torch.finfo(dtype).min返回的是float最小值-inf</span>    <span class="token comment"># 用torch.full填充出大小为(tgt_len, tgt_len)的矩阵</span>        mask_cond <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>mask<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span> <span class="token comment"># 0到tgt_len的</span>    mask<span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>mask_cond <span class="token operator">&lt;</span> <span class="token punctuation">(</span>mask_cond <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>mask<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>    <span class="token comment"># view相当于reshape</span>        mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token punctuation">)</span>    <span class="token keyword">if</span> past_key_values_length <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>        mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>tgt_len<span class="token punctuation">,</span> past_key_values_length<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> mask<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> mask<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> tgt_len<span class="token punctuation">,</span> tgt_len <span class="token operator">+</span> past_key_values_length<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p><code>mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)</code>这里用torch.finfo(dtype).min返回的是float最小值-inf，用torch.full填充出大小为(tgt_len, tgt_len)的矩阵</p></li><li><p><code>mask.masked_fill_(condition, value)</code>当前列索引小于对应的行索引时填0</p></li><li><p>这里还带了一个expand_mask，改变大小[bsz, seq_len]到[bsz, 1, tgt_seq_len, src_seq_len]</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_expand_mask</span><span class="token punctuation">(</span>mask<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> dtype<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> tgt_len<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.    """</span>    bsz<span class="token punctuation">,</span> src_len <span class="token operator">=</span> mask<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>    tgt_len <span class="token operator">=</span> tgt_len <span class="token keyword">if</span> tgt_len <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> src_len    expanded_mask <span class="token operator">=</span> mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> tgt_len<span class="token punctuation">,</span> src_len<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token punctuation">)</span>    inverted_mask <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">-</span> expanded_mask    <span class="token keyword">return</span> inverted_mask<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>inverted_mask<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">bool</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>finfo<span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>这段其实有点一知半解（</li><li>上一篇博客中介绍的，attention score大小为(bs, n_local_heads, slen, cache_len + slen)</li><li>举例，在(bsz, 1, i, j)位置，即i为query，对其他位置的k询问时计算出的分数，而模型不能看到序列后面的东西，所以这里j&gt;i位置的mask为-inf</li><li>最后通过inverted_mask把三角翻转了一下</li></ul><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul><li>定义了一个pretrainedmodel，包含_init_weights方法，用于初始化参数，对线性层和embedding层用正太分布初始化。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LlamaPreTrainedModel</span><span class="token punctuation">(</span>PreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>    config_class <span class="token operator">=</span> LlamaConfig    base_model_prefix <span class="token operator">=</span> <span class="token string">"model"</span>    supports_gradient_checkpointing <span class="token operator">=</span> <span class="token boolean">True</span>    _no_split_modules <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"LlamaDecoderLayer"</span><span class="token punctuation">]</span>    _skip_keys_device_placement <span class="token operator">=</span> <span class="token string">"past_key_values"</span>    <span class="token keyword">def</span> <span class="token function">_init_weights</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> module<span class="token punctuation">)</span><span class="token punctuation">:</span>        std <span class="token operator">=</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>initializer_range        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">)</span><span class="token punctuation">:</span>            module<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>mean<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> std<span class="token operator">=</span>std<span class="token punctuation">)</span>            <span class="token keyword">if</span> module<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>                module<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">)</span><span class="token punctuation">:</span>            module<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>mean<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> std<span class="token operator">=</span>std<span class="token punctuation">)</span>            <span class="token keyword">if</span> module<span class="token punctuation">.</span>padding_idx <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>                module<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span>module<span class="token punctuation">.</span>padding_idx<span class="token punctuation">]</span><span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">_set_gradient_checkpointing</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> module<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> LlamaModel<span class="token punctuation">)</span><span class="token punctuation">:</span>            module<span class="token punctuation">.</span>gradient_checkpointing <span class="token operator">=</span> value<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>gradient_checkpoint是一个时间换显存的方法，这里默认设置为False，官方文档：</li></ul><pre class="line-numbers language-none"><code class="language-none">Gradient Checkpoint是通过以更长的计算时间为代价，换取更少的显存占用。相比于原本需要存储所有中间变量以供反向传播使用，使用了checkpoint的部分不存储中间变量而是在反向传播过程中重新计算这些中间变量。模型中的任何部分都可以使用gradient checkpoint。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="分部分解释"><a href="#分部分解释" class="headerlink" title="分部分解释"></a>分部分解释</h4><ul><li>主要分为以下几个部分</li></ul><p>（1）_prepare_decoder_attention_mask：注释里也写明白了，分两步制作mask（生成三角，填充成四维）</p><p>（2）</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LlamaModel</span><span class="token punctuation">(</span>LlamaPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 定义为上面的子类</span>    <span class="token triple-quoted-string string">"""    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]    Args:        config: LlamaConfig    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> LlamaConfig<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>padding_idx <span class="token operator">=</span> config<span class="token punctuation">.</span>pad_token_id        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> config<span class="token punctuation">.</span>vocab_size        self<span class="token punctuation">.</span>embed_tokens <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>padding_idx<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>LlamaDecoderLayer<span class="token punctuation">(</span>config<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>num_hidden_layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> LlamaRMSNorm<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>rms_norm_eps<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>gradient_checkpointing <span class="token operator">=</span> <span class="token boolean">False</span> <span class="token comment"># gradient_checkpoint是一个节省显存的方法，这里不开启</span>        <span class="token comment"># Initialize weights and apply final processing</span>        self<span class="token punctuation">.</span>post_init<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">get_input_embeddings</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>embed_tokens    <span class="token keyword">def</span> <span class="token function">set_input_embeddings</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>embed_tokens <span class="token operator">=</span> value    <span class="token comment"># Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask</span>    <span class="token keyword">def</span> <span class="token function">_prepare_decoder_attention_mask</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> attention_mask<span class="token punctuation">,</span> input_shape<span class="token punctuation">,</span> inputs_embeds<span class="token punctuation">,</span> past_key_values_length<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 用来准备attention mask</span>        <span class="token comment"># create causal mask</span>        <span class="token comment"># [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]</span>        combined_attention_mask <span class="token operator">=</span> <span class="token boolean">None</span>        <span class="token keyword">if</span> input_shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>            combined_attention_mask <span class="token operator">=</span> _make_causal_mask<span class="token punctuation">(</span>                input_shape<span class="token punctuation">,</span>                inputs_embeds<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span>                device<span class="token operator">=</span>inputs_embeds<span class="token punctuation">.</span>device<span class="token punctuation">,</span>                past_key_values_length<span class="token operator">=</span>past_key_values_length<span class="token punctuation">,</span>            <span class="token punctuation">)</span>        <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            <span class="token comment"># [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]</span>            expanded_attn_mask <span class="token operator">=</span> _expand_mask<span class="token punctuation">(</span>attention_mask<span class="token punctuation">,</span> inputs_embeds<span class="token punctuation">.</span>dtype<span class="token punctuation">,</span> tgt_len<span class="token operator">=</span>input_shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>                  inputs_embeds<span class="token punctuation">.</span>device            <span class="token punctuation">)</span>            combined_attention_mask <span class="token operator">=</span> <span class="token punctuation">(</span>                expanded_attn_mask <span class="token keyword">if</span> combined_attention_mask <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">else</span> expanded_attn_mask <span class="token operator">+</span> combined_attention_mask            <span class="token punctuation">)</span>        <span class="token keyword">return</span> combined_attention_mask    <span class="token decorator annotation punctuation">@add_start_docstrings_to_model_forward</span><span class="token punctuation">(</span>LLAMA_INPUTS_DOCSTRING<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        input_ids<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>LongTensor <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        past_key_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>List<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        inputs_embeds<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        output_attentions<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        output_hidden_states<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        return_dict<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Union<span class="token punctuation">[</span>Tuple<span class="token punctuation">,</span> BaseModelOutputWithPast<span class="token punctuation">]</span><span class="token punctuation">:</span>        output_attentions <span class="token operator">=</span> output_attentions <span class="token keyword">if</span> output_attentions <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>output_attentions        output_hidden_states <span class="token operator">=</span> <span class="token punctuation">(</span>            output_hidden_states <span class="token keyword">if</span> output_hidden_states <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>output_hidden_states        <span class="token punctuation">)</span>        use_cache <span class="token operator">=</span> use_cache <span class="token keyword">if</span> use_cache <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>use_cache        return_dict <span class="token operator">=</span> return_dict <span class="token keyword">if</span> return_dict <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>use_return_dict        <span class="token comment"># retrieve input_ids and inputs_embeds</span>        <span class="token keyword">if</span> input_ids <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> inputs_embeds <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time"</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> input_ids <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            batch_size<span class="token punctuation">,</span> seq_length <span class="token operator">=</span> input_ids<span class="token punctuation">.</span>shape        <span class="token keyword">elif</span> inputs_embeds <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">,</span> _ <span class="token operator">=</span> inputs_embeds<span class="token punctuation">.</span>shape        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"You have to specify either decoder_input_ids or decoder_inputs_embeds"</span><span class="token punctuation">)</span>        seq_length_with_past <span class="token operator">=</span> seq_length        past_key_values_length <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">if</span> past_key_values <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            past_key_values_length <span class="token operator">=</span> past_key_values<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>            seq_length_with_past <span class="token operator">=</span> seq_length_with_past <span class="token operator">+</span> past_key_values_length        <span class="token keyword">if</span> position_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            device <span class="token operator">=</span> input_ids<span class="token punctuation">.</span>device <span class="token keyword">if</span> input_ids <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> inputs_embeds<span class="token punctuation">.</span>device            position_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>                past_key_values_length<span class="token punctuation">,</span> seq_length <span class="token operator">+</span> past_key_values_length<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device            <span class="token punctuation">)</span>            position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> seq_length<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            position_ids <span class="token operator">=</span> position_ids<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> seq_length<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> inputs_embeds <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            inputs_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>embed_tokens<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>        <span class="token comment"># embed positions</span>        <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            attention_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>                <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_length_with_past<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">bool</span><span class="token punctuation">,</span> device<span class="token operator">=</span>inputs_embeds<span class="token punctuation">.</span>device            <span class="token punctuation">)</span>        attention_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>_prepare_decoder_attention_mask<span class="token punctuation">(</span>            attention_mask<span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_length<span class="token punctuation">)</span><span class="token punctuation">,</span> inputs_embeds<span class="token punctuation">,</span> past_key_values_length        <span class="token punctuation">)</span>        hidden_states <span class="token operator">=</span> inputs_embeds        <span class="token keyword">if</span> self<span class="token punctuation">.</span>gradient_checkpointing <span class="token keyword">and</span> self<span class="token punctuation">.</span>training<span class="token punctuation">:</span>            <span class="token keyword">if</span> use_cache<span class="token punctuation">:</span>                logger<span class="token punctuation">.</span>warning_once<span class="token punctuation">(</span>                    <span class="token string">"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."</span>                <span class="token punctuation">)</span>                use_cache <span class="token operator">=</span> <span class="token boolean">False</span>        <span class="token comment"># decoder layers</span>        all_hidden_states <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> output_hidden_states <span class="token keyword">else</span> <span class="token boolean">None</span>        all_self_attns <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> output_attentions <span class="token keyword">else</span> <span class="token boolean">None</span>        next_decoder_cache <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> use_cache <span class="token keyword">else</span> <span class="token boolean">None</span>        <span class="token keyword">for</span> idx<span class="token punctuation">,</span> decoder_layer <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>layers<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> output_hidden_states<span class="token punctuation">:</span>                all_hidden_states <span class="token operator">+=</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>            past_key_value <span class="token operator">=</span> past_key_values<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token keyword">if</span> past_key_values <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token boolean">None</span>            <span class="token keyword">if</span> self<span class="token punctuation">.</span>gradient_checkpointing <span class="token keyword">and</span> self<span class="token punctuation">.</span>training<span class="token punctuation">:</span>                <span class="token keyword">def</span> <span class="token function">create_custom_forward</span><span class="token punctuation">(</span>module<span class="token punctuation">)</span><span class="token punctuation">:</span>                    <span class="token keyword">def</span> <span class="token function">custom_forward</span><span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>                        <span class="token comment"># None for past_key_value</span>                        <span class="token keyword">return</span> module<span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">,</span> output_attentions<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>                    <span class="token keyword">return</span> custom_forward                layer_outputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>checkpoint<span class="token punctuation">.</span>checkpoint<span class="token punctuation">(</span>                    create_custom_forward<span class="token punctuation">(</span>decoder_layer<span class="token punctuation">)</span><span class="token punctuation">,</span>                    hidden_states<span class="token punctuation">,</span>                    attention_mask<span class="token punctuation">,</span>                    position_ids<span class="token punctuation">,</span>                    <span class="token boolean">None</span><span class="token punctuation">,</span>                <span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                layer_outputs <span class="token operator">=</span> decoder_layer<span class="token punctuation">(</span>                    hidden_states<span class="token punctuation">,</span>                    attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>                    position_ids<span class="token operator">=</span>position_ids<span class="token punctuation">,</span>                    past_key_value<span class="token operator">=</span>past_key_value<span class="token punctuation">,</span>                    output_attentions<span class="token operator">=</span>output_attentions<span class="token punctuation">,</span>                    use_cache<span class="token operator">=</span>use_cache<span class="token punctuation">,</span>                <span class="token punctuation">)</span>            hidden_states <span class="token operator">=</span> layer_outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>            <span class="token keyword">if</span> use_cache<span class="token punctuation">:</span>                next_decoder_cache <span class="token operator">+=</span> <span class="token punctuation">(</span>layer_outputs<span class="token punctuation">[</span><span class="token number">2</span> <span class="token keyword">if</span> output_attentions <span class="token keyword">else</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> output_attentions<span class="token punctuation">:</span>                all_self_attns <span class="token operator">+=</span> <span class="token punctuation">(</span>layer_outputs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">)</span>        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>        <span class="token comment"># add hidden states from the last decoder layer</span>        <span class="token keyword">if</span> output_hidden_states<span class="token punctuation">:</span>            all_hidden_states <span class="token operator">+=</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>        next_cache <span class="token operator">=</span> next_decoder_cache <span class="token keyword">if</span> use_cache <span class="token keyword">else</span> <span class="token boolean">None</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> return_dict<span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>v <span class="token keyword">for</span> v <span class="token keyword">in</span> <span class="token punctuation">[</span>hidden_states<span class="token punctuation">,</span> next_cache<span class="token punctuation">,</span> all_hidden_states<span class="token punctuation">,</span> all_self_attns<span class="token punctuation">]</span> <span class="token keyword">if</span> v <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> BaseModelOutputWithPast<span class="token punctuation">(</span>            last_hidden_state<span class="token operator">=</span>hidden_states<span class="token punctuation">,</span>            past_key_values<span class="token operator">=</span>next_cache<span class="token punctuation">,</span>            hidden_states<span class="token operator">=</span>all_hidden_states<span class="token punctuation">,</span>            attentions<span class="token operator">=</span>all_self_attns<span class="token punctuation">,</span>        <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法学习笔记 </tag>
            
            <tag> LLM </tag>
            
            <tag> DL </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>llama代码解读（一）LlamaDecoderLayer</title>
      <link href="2024/03/05/llm-study/"/>
      <url>2024/03/05/llm-study/</url>
      
        <content type="html"><![CDATA[<h1 id="llama代码解读（一）LlamaDecoderLayer"><a href="#llama代码解读（一）LlamaDecoderLayer" class="headerlink" title="llama代码解读（一）LlamaDecoderLayer"></a>llama代码解读（一）LlamaDecoderLayer</h1><p>这篇好理解<a href="https://blog.csdn.net/weixin_44826203/article/details/129255185">Meta最新模型LLaMA细节与代码详解_llama模型结构-CSDN博客</a></p><p>后续又找到一篇更详细，写得更好的<a href="https://zhuanlan.zhihu.com/p/648365207">Llama源码深入解析 - 知乎 (zhihu.com)</a></p><ul><li>pre-norm和post-norm区别<a href="https://zhuanlan.zhihu.com/p/657659526">PreNorm/PostNorm/DeepNorm/RMSNorm是什么 - 知乎 (zhihu.com)</a></li><li>Llama使用pre-norm：$ x_{t+1} = x_t + F(Norm(x_t)) $</li><li>不同于一般的LayerNorm，RMS Norm取消了bias和减去均值，作者认为这种模式简化了Layer Norm，节约时间。这里的gi也是一个可训练的参数。目的是不改变词向量的方向。</li></ul><p><img src="/img/llm_study/rms.png" alt="rms"></p><h2 id="BLOCK代码"><a href="#BLOCK代码" class="headerlink" title="BLOCK代码"></a>BLOCK代码</h2><p>参考<a href="https://zhuanlan.zhihu.com/p/648365207">Llama源码深入解析 - 知乎 (zhihu.com)</a></p><ul><li>总的来说一个transformer block由注意力和前馈网络组成</li><li>观察代码发现<code>h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_cis, mask)</code>是prenorm，这里面的frees_cis是rope编码的位置信息</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LlamaDecoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> LlamaConfig<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size        self<span class="token punctuation">.</span>self_attn <span class="token operator">=</span> LlamaAttention<span class="token punctuation">(</span>config<span class="token operator">=</span>config<span class="token punctuation">)</span> <span class="token comment"># 注意力</span>        self<span class="token punctuation">.</span>mlp <span class="token operator">=</span> LlamaMLP<span class="token punctuation">(</span>config<span class="token punctuation">)</span> <span class="token comment"># FFN</span>        self<span class="token punctuation">.</span>input_layernorm <span class="token operator">=</span> LlamaRMSNorm<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>rms_norm_eps<span class="token punctuation">)</span> <span class="token comment"># RMS Norm</span>        self<span class="token punctuation">.</span>post_attention_layernorm <span class="token operator">=</span> LlamaRMSNorm<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span>config<span class="token punctuation">.</span>rms_norm_eps<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        past_key_value<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        output_attentions<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        Args:            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.            output_attentions (`bool`, *optional*):                Whether or not to return the attentions tensors of all attention layers. See `attentions` under                returned tensors for more detail.            use_cache (`bool`, *optional*):                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding                (see `past_key_values`).            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states        """</span>        residual <span class="token operator">=</span> hidden_states        <span class="token comment"># 在 self attention 层之前，保存输入状态 hidden_states，以便在后续进行残差连接</span>        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>input_layernorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span>        <span class="token comment"># 输入attention前先layernorm一下，这里有点困惑为什么llama在这里用layernorm，后面又改用RMS了</span>        <span class="token comment"># Self Attention</span>        hidden_states<span class="token punctuation">,</span> self_attn_weights<span class="token punctuation">,</span> present_key_value <span class="token operator">=</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span> <span class="token comment"># 调用之前写的注意力</span>            hidden_states<span class="token operator">=</span>hidden_states<span class="token punctuation">,</span>            attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>            position_ids<span class="token operator">=</span>position_ids<span class="token punctuation">,</span>            past_key_value<span class="token operator">=</span>past_key_value<span class="token punctuation">,</span>            output_attentions<span class="token operator">=</span>output_attentions<span class="token punctuation">,</span>            use_cache<span class="token operator">=</span>use_cache<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        hidden_states <span class="token operator">=</span> residual <span class="token operator">+</span> hidden_states <span class="token comment"># 残差连接</span>        <span class="token comment"># Fully Connected</span>        residual <span class="token operator">=</span> hidden_states <span class="token comment"># MLP前也残差一下</span>        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>post_attention_layernorm<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span> <span class="token comment"># norm</span>        hidden_states <span class="token operator">=</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span> <span class="token comment"># MLP</span>        hidden_states <span class="token operator">=</span> residual <span class="token operator">+</span> hidden_states        outputs <span class="token operator">=</span> <span class="token punctuation">(</span>hidden_states<span class="token punctuation">,</span><span class="token punctuation">)</span>         <span class="token keyword">if</span> output_attentions<span class="token punctuation">:</span> <span class="token comment"># 这里输出attention weight，可能是为了后续做可视化或者调试</span>            outputs <span class="token operator">+=</span> <span class="token punctuation">(</span>self_attn_weights<span class="token punctuation">,</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> use_cache<span class="token punctuation">:</span>            outputs <span class="token operator">+=</span> <span class="token punctuation">(</span>present_key_value<span class="token punctuation">,</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> outputs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="RoPE编码部分"><a href="#RoPE编码部分" class="headerlink" title="RoPE编码部分"></a>RoPE编码部分</h3><ul><li>找到一篇讲的很清楚的博客，推荐直接看这个<a href="https://www.zhihu.com/tardis/zm/art/647109286">十分钟读懂旋转编码（RoPE） (zhihu.com)</a></li><li>核心思想就是使$\langle f_q(x_m,m),f_k(x_n,n)\rangle = g(x_m,x_n,m-n)$即一对不同位置的位置编码，两者内积只与输入的词向量和<strong>相对位置</strong>$m-n$有关</li><li>对于一个二维向量来说，即乘上一个旋转矩阵</li></ul><p><img src="/img/llm_study/rope1.png" alt="rope1"></p><ul><li>扩展到多维，即用二维拼接</li></ul><p><img src="/img/llm_study/rope2.png" alt="rope2"></p><ul><li>再进一步，因为这个矩阵的稀疏性，可以简化为按位相乘求和，如下</li></ul><p><img src="/img/llm_study/rope3.png" alt="rope3"></p><ul><li>在代码中，根据上面的定义$ \theta_i = \frac{1}{10000^{2(i-1)/d}}$计算出theta</li><li>使用torch.outer()和torch.polar的用法在注释中给出</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>polar<span class="token punctuation">(</span><span class="token builtin">abs</span><span class="token punctuation">,</span> angle<span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span> → Tensor<span class="token comment"># 即输入模长和角度，输出为复数</span><span class="token comment"># 使用例</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token builtin">abs</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float64<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> angle <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>np<span class="token punctuation">.</span>pi <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>pi <span class="token operator">/</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float64<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> z <span class="token operator">=</span> torch<span class="token punctuation">.</span>polar<span class="token punctuation">(</span><span class="token builtin">abs</span><span class="token punctuation">,</span> angle<span class="token punctuation">)</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> ztensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">0.0000</span><span class="token operator">+</span><span class="token number">1.0000j</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1.4142</span><span class="token operator">-</span><span class="token number">1.4142j</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>complex128<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 生成旋转矩阵</span><span class="token keyword">def</span> <span class="token function">precompute_freqs_cis</span><span class="token punctuation">(</span>dim<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> seq_len<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> theta<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">10000.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 计算词向量元素两两分组之后，每组元素对应的旋转角度\theta_i</span>    freqs <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span>theta <span class="token operator">**</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> dim<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>dim <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> dim<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># 生成 token 序列索引 t = [0, 1,..., seq_len-1]</span>    t <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span> device<span class="token operator">=</span>freqs<span class="token punctuation">.</span>device<span class="token punctuation">)</span>    <span class="token comment"># freqs.shape = [seq_len, dim // 2] </span>    freqs <span class="token operator">=</span> torch<span class="token punctuation">.</span>outer<span class="token punctuation">(</span>t<span class="token punctuation">,</span> freqs<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 计算m * \theta</span>    <span class="token comment"># 这里outer的用法，假设t为[1,2,3] freq为[e1,e2],这里用e代替一下theta</span>    <span class="token comment"># 则outer(t,freq)得到</span>    <span class="token comment">#[1e1,1e2]</span>    <span class="token comment">#[2e1,2e2]</span>    <span class="token comment">#[3e1,3e2]</span>        <span class="token comment"># 计算结果是个复数向量</span>    <span class="token comment"># </span>    <span class="token comment"># 如果poler输入为[x, y]</span>    <span class="token comment"># 则poler输出为[cos(x) + sin(x)i, cos(y) + sin(y)i]</span>    freqs_cis <span class="token operator">=</span> torch<span class="token punctuation">.</span>polar<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>freqs<span class="token punctuation">)</span><span class="token punctuation">,</span> freqs<span class="token punctuation">)</span>     <span class="token keyword">return</span> freqs_cis<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="RMS-Norm"><a href="#RMS-Norm" class="headerlink" title="RMS Norm"></a>RMS Norm</h3><ul><li>RMS Norm和普通layerNorm的区别主要有2，取消了bias，并且没有使用均值</li><li>博客作者说，这么做的目的是使norm<strong>不改变词向量的方向，只改变其长度</strong>，感觉超级有道理。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LlamaRMSNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        LlamaRMSNorm is equivalent to T5LayerNorm        """</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>variance_epsilon <span class="token operator">=</span> eps    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_states<span class="token punctuation">)</span><span class="token punctuation">:</span>        input_dtype <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>dtype        hidden_states <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>        variance <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># 立方和去平均</span>        hidden_states <span class="token operator">=</span> hidden_states <span class="token operator">*</span> torch<span class="token punctuation">.</span>rsqrt<span class="token punctuation">(</span>variance <span class="token operator">+</span> self<span class="token punctuation">.</span>variance_epsilon<span class="token punctuation">)</span> <span class="token comment"># 开根后当分母</span>        <span class="token comment"># 注意这里加上了eps，防止分母为0</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>weight <span class="token operator">*</span> hidden_states<span class="token punctuation">.</span>to<span class="token punctuation">(</span>input_dtype<span class="token punctuation">)</span> <span class="token comment"># 乘上参数g</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>eps是一个小的常数，用于防止分母为零的情况。记得以前打acm的时候也是这么叫的</p></li><li><p>weight设置为learnable的parameters，对应RMS Norm公式中的gi</p></li><li><p>解释一下<code>torch.rsqrt(...)</code>: 是 PyTorch 中的平方根的倒数（reciprocal square root）函数。</p></li></ul><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>参考<a href="https://blog.csdn.net/weixin_44826203/article/details/129255185">Meta最新模型LLaMA细节与代码详解_llama模型结构-CSDN博客</a></p><ul><li><p>这里面除了基础的Wq，Wk，Wv还多了一个Wo，大小为(head数量*head长度，dim)，相当于是把多头的结果合并一下。</p><p>输入：这个维度的长度同所有头接起来的长度<br>输出：同原始输入的维度</p></li><li><p>总过程大概如下：</p><p>1.经过线性层Wqkv得到Xqkv</p><p>2.Xq和Xk中加入旋转位置编码</p><p>3.<strong>缓存Xq和Xk</strong></p><p>4.计算softmax</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Attention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">:</span> ModelArgs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>n_local_heads <span class="token operator">=</span> args<span class="token punctuation">.</span>n_heads <span class="token operator">//</span> fs_init<span class="token punctuation">.</span>get_model_parallel_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 这里算出当前这个并行单元需要计算多少个头，比如一共有8个head，2个并行的单元，则当前单元需要计算两个head</span>        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>dim <span class="token operator">//</span> args<span class="token punctuation">.</span>n_heads <span class="token comment"># args.dim是总共的dim数，得每个头的dim数</span>        self<span class="token punctuation">.</span>wq <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span> <span class="token comment"># Wq</span>            args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span>            args<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> <span class="token comment"># 这里看出是把几个头前后相接后计算的</span>            bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            gather_output<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">,</span> <span class="token comment"># lambda表达式，这里是返回x本身作为初始化方法</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>wk <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span> <span class="token comment"># Wk</span>            args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span>            args<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span>            bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            gather_output<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>wv <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span> <span class="token comment"># Wv</span>            args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span>            args<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span>            bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            gather_output<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>wo <span class="token operator">=</span> RowParallelLinear<span class="token punctuation">(</span> <span class="token comment"># 将多个头的输出进行线性变换，将其映射回原始输入的维度</span>            args<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">,</span> <span class="token comment"># 输入维度：这个维度的长度同所有头接起来的长度</span>            args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> <span class="token comment"># 输出维度：同原始输入的维度</span>            bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>            input_is_parallel<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>            init_method<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">,</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>cache_k <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span> <span class="token comment"># 缓存机制</span>            <span class="token punctuation">(</span>args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        <span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>cache_v <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>            <span class="token punctuation">(</span>args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        <span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> start_pos<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> freqs_cis<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> _ <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        xq<span class="token punctuation">,</span> xk<span class="token punctuation">,</span> xv <span class="token operator">=</span> self<span class="token punctuation">.</span>wq<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>wk<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>wv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment"># 从前面Wqkv的定义可以看出是把几个头前后相接后计算的，这里需要修改一下矩阵尺寸</span>        <span class="token comment"># 调用view改变形状(batch size,sequence length,头数,头大小)</span>        xq <span class="token operator">=</span> xq<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        xk <span class="token operator">=</span> xk<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        xv <span class="token operator">=</span> xv<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span>        xq<span class="token punctuation">,</span> xk <span class="token operator">=</span> apply_rotary_emb<span class="token punctuation">(</span>xq<span class="token punctuation">,</span> xk<span class="token punctuation">,</span> freqs_cis<span class="token operator">=</span>freqs_cis<span class="token punctuation">)</span> <span class="token comment"># 对qk应用rope编码</span>        self<span class="token punctuation">.</span>cache_k <span class="token operator">=</span> self<span class="token punctuation">.</span>cache_k<span class="token punctuation">.</span>to<span class="token punctuation">(</span>xq<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>cache_v <span class="token operator">=</span> self<span class="token punctuation">.</span>cache_v<span class="token punctuation">.</span>to<span class="token punctuation">(</span>xq<span class="token punctuation">)</span> <span class="token comment"># 把cache移动到和xq相同的设备上</span>        self<span class="token punctuation">.</span>cache_k<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> start_pos <span class="token punctuation">:</span> start_pos <span class="token operator">+</span> seqlen<span class="token punctuation">]</span> <span class="token operator">=</span> xk        self<span class="token punctuation">.</span>cache_v<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> start_pos <span class="token punctuation">:</span> start_pos <span class="token operator">+</span> seqlen<span class="token punctuation">]</span> <span class="token operator">=</span> xv <span class="token comment"># 时间步骤start_pos </span>        keys <span class="token operator">=</span> self<span class="token punctuation">.</span>cache_k<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> <span class="token punctuation">:</span> start_pos <span class="token operator">+</span> seqlen<span class="token punctuation">]</span> <span class="token comment"># 从cache中读</span>        values <span class="token operator">=</span> self<span class="token punctuation">.</span>cache_v<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> <span class="token punctuation">:</span> start_pos <span class="token operator">+</span> seqlen<span class="token punctuation">]</span>        xq <span class="token operator">=</span> xq<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token comment"># 交换seqlen和n_head</span>        keys <span class="token operator">=</span> keys<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        values <span class="token operator">=</span> values<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>xq<span class="token punctuation">,</span> keys<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span> <span class="token comment"># kq乘得score</span>        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            scores <span class="token operator">=</span> scores <span class="token operator">+</span> mask  <span class="token comment"># (bs, n_local_heads, slen, cache_len + slen)</span>            <span class="token comment"># attention mask后面解释一下为什么这里是相加</span>        scores <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>scores<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>xq<span class="token punctuation">)</span> <span class="token comment"># softmax</span>        output <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> values<span class="token punctuation">)</span>  <span class="token comment"># (bs, n_local_heads, slen, head_dim) # score乘上v</span>        output <span class="token operator">=</span> output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>            <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span>        <span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>wo<span class="token punctuation">(</span>output<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="kv-cache"><a href="#kv-cache" class="headerlink" title="kv_cache"></a>kv_cache</h4><p>解释来自：<a href="https://zhuanlan.zhihu.com/p/630832593">大模型推理性能优化之KV Cache解读 - 知乎 (zhihu.com)</a></p><p>在上面的推理过程中，每 step 内，输入一个 token序列，经过Embedding层将输入token序列变为一个三维张量[b, s, h]，经过一通计算，最后经logits层将计算结果映射至词表空间，输出张量维度为[b, s, vocab_size]。</p><p><strong>当前轮输出token与输入tokens拼接</strong>，并作为下一轮的输入tokens，反复多次。可以看出第t+1轮输入数据只比第t轮输入数据<strong>新增了一个token</strong>，其他全部相同！因此第t+1轮推理时必然包含了第t轮的部分计算。KV Cache的出发点就在这里，缓存当前轮可重复利用的计算结果，下一轮计算时直接读取缓存结果。</p><ul><li><p>这里的cache大小为(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)</p><p>假设在某个时间步骤 <code>t</code>，<code>xk_t</code> 和 <code>xv_t</code> 是当前时间步骤的键和值。将它们存储到缓存中的操作类似于：</p><pre class="line-numbers language-none"><code class="language-none">self.cache_k[:, t, :, :] = xk_tself.cache_v[:, t, :, :] = xv_t<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>然后，在后续时间步骤，如果遇到之前的 <code>t</code>，就可以直接使用缓存中的值，而不必重新计算 <code>xk_t</code> 和 <code>xv_t</code>。这样，可以避免重复计算相同位置的键和值，提高了计算效率。</p></li><li><p>KV Cache是Transformer推理性能优化的一项重要工程化技术，各大推理框架都已实现并将其进行了封装。</p></li></ul><h4 id="Attention-Mask"><a href="#Attention-Mask" class="headerlink" title="Attention Mask"></a>Attention Mask</h4><ul><li>这里用一个图解释一下为什么是score+mask，因为这里mask掩盖的位置值为-inf，通过softmax的取对数之后会变为0。</li></ul><p><img src="/img/llm_study/am.png" alt="am"></p><h3 id="FeedForward-Layer"><a href="#FeedForward-Layer" class="headerlink" title="FeedForward Layer"></a>FeedForward Layer</h3><h4 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h4><ul><li>这里通过self.intermediate_size // self.config.pretraining_tp（中间层的大小和预训练阶段的切分份数）计算出每个切片的大小，把输入切分。</li><li>其实比较好理解的是，<strong>假设原本输入维度是1024，输出2048，这里就把它拆分成四个256*2048。</strong></li><li>问了一下GPT这种切片的意义，说是减小内存占用：将线性层的权重切分成多个小块，每个小块的大小相对较小，可以降低模型的内存占用。这对于训练大型模型时，特别是在显存受限的情况下，可以是一种优化策略。</li></ul><h4 id="门控"><a href="#门控" class="headerlink" title="门控"></a>门控</h4><ul><li>通过一个gate和中间层输出的对应位相乘</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">intermediate_states <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>act_fn<span class="token punctuation">(</span>gate_proj<span class="token punctuation">)</span> <span class="token operator">*</span> up_proj<span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>ACT2FN是激活函数的字典，它的代码如下：</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">ACT2FN <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"gelu"</span><span class="token punctuation">:</span> gelu<span class="token punctuation">,</span> <span class="token string">"relu"</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>relu<span class="token punctuation">,</span> <span class="token string">"swish"</span><span class="token punctuation">:</span> swish<span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LlamaMLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size        self<span class="token punctuation">.</span>intermediate_size <span class="token operator">=</span> config<span class="token punctuation">.</span>intermediate_size                <span class="token comment"># 门控</span>        self<span class="token punctuation">.</span>gate_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>                <span class="token comment"># 两层Linear，上采样和下采样</span>        self<span class="token punctuation">.</span>up_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>down_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>intermediate_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>act_fn <span class="token operator">=</span> ACT2FN<span class="token punctuation">[</span>config<span class="token punctuation">.</span>hidden_act<span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>            <span class="token builtin">slice</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>intermediate_size <span class="token operator">//</span> self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp <span class="token comment"># 计算slice大小</span>            gate_proj_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>gate_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>            up_proj_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>up_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>            down_proj_slices <span class="token operator">=</span> self<span class="token punctuation">.</span>down_proj<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>            gate_proj <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>                <span class="token punctuation">[</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> gate_proj_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>            <span class="token punctuation">)</span><span class="token comment"># 把x和gate_proj的相乘，结果在最后一个维度dim=-1接起来</span>            up_proj <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> up_proj_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>                        intermediate_states <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>act_fn<span class="token punctuation">(</span>gate_proj<span class="token punctuation">)</span> <span class="token operator">*</span> up_proj<span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>            <span class="token comment"># 这里的act_fn是激活函数，门控在这里发挥作用，和up_proj对应位置相乘实现门控</span>                        down_proj <span class="token operator">=</span> <span class="token punctuation">[</span>                F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>intermediate_states<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> down_proj_slices<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pretraining_tp<span class="token punctuation">)</span>            <span class="token punctuation">]</span>            down_proj <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>down_proj<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            down_proj <span class="token operator">=</span> self<span class="token punctuation">.</span>down_proj<span class="token punctuation">(</span>self<span class="token punctuation">.</span>act_fn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>gate_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>up_proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> down_proj<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>我有点不理解的地方是，为什么up_proj和down_proj被定义成MLP的线性层，后面把线性层的数值传递给slice后，又用来存放输入的中间结果和输出了。是为了省内存吗？</li></ul>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法学习笔记 </tag>
            
            <tag> LLM </tag>
            
            <tag> DL </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我的NLP学习路径和笔记</title>
      <link href="2023/11/08/nlp-study/"/>
      <url>2023/11/08/nlp-study/</url>
      
        <content type="html"><![CDATA[<h1 id="我的NLP学习路径"><a href="#我的NLP学习路径" class="headerlink" title="我的NLP学习路径"></a>我的NLP学习路径</h1><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><ul><li>本科all in ACM遗憾打铜，现在有点后悔。导致研一入学时基本零基础，跟着李沐d2l学到第五章后开始学习NLP。</li></ul><h2 id="一些网址记录"><a href="#一些网址记录" class="headerlink" title="一些网址记录"></a>一些网址记录</h2><ul><li>LLM综述<a href="https://zhuanlan.zhihu.com/p/612181615">人工智能 LLM 革命破晓：一文读懂当下超大语言模型发展现状 /麦克船长LLM革命系列2 - 知乎 (zhihu.com)</a></li><li>李沐的D2L<a href="https://zh-v2.d2l.ai/">《动手学深度学习》 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></li></ul><h2 id="给自己的话"><a href="#给自己的话" class="headerlink" title="给自己的话"></a>给自己的话</h2><h3 id="2023-11-1"><a href="#2023-11-1" class="headerlink" title="2023.11.1"></a>2023.11.1</h3><ul><li>刚完成情感分类的任务，感觉自己的代码能力有点差，主要是写的太少了，还是要多结合一下kaggle竞赛之类的练习，coding这块不能落下</li></ul><h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>这篇公式全<a href="https://zhuanlan.zhihu.com/p/114538417">深入浅出Word2Vec原理解析 - 知乎 (zhihu.com)</a></p><h3 id="skip-gram模型"><a href="#skip-gram模型" class="headerlink" title="skip-gram模型"></a>skip-gram模型</h3><p>好理解<a href="https://zhuanlan.zhihu.com/p/27234078">理解 Word2Vec 之 Skip-Gram 模型 - 知乎 (zhihu.com)</a></p><ul><li><p>概念：window_size，skip_size</p></li><li><p>第一个线性层对应词向量，因为输入是one hot，所以和linear相乘即取对应行的词向量！因此这个hidden layer的权重被叫做”查找表“（lookup table）。</p></li><li><p>因为onehot是稀疏的，实际上不用onehot和矩阵相乘，只需要找到lookup table中对应的行，以节省相乘的计算资源。pytorch提供了<code>torch.nn.Embedding</code>。</p></li><li><p>训练skipgram是个fake task，实际目的是在训练过程中得到这些词向量</p></li><li><p>negative sampling方法，为什么能减少计算：</p></li></ul><p>先后的两个loss，其中yij代表ij是否成对。原本是对所有词都计算损失，现在是只用对抽取的正样本和负样本计算loss。<br>$$<br>\text{loss} = -\sum_{j=1}^V \left( y_{ij} \log(\sigma(v_i^\top u_j)) + (1 - y_{ij}) \log(1 - \sigma(v_i^\top u_j)) \right)<br>\<br>\text{loss} = -\left( \log(\sigma(v_i^\top u_j)) + \sum_{k=1}^{K} \log(\sigma(-v_i^\top u_k)) \right)<br>$$</p><h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><ul><li>后面还是看沐神的了，<a href="https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html">14.1. 词嵌入（word2vec） — 动手学深度学习 2.0.0 documentation (d2l.ai)</a>后面CBOW的功能和SkipGram类似，不过是反过来，给上下文求中心词。给定一个大小为左右2m的窗口，利用softmax函数计算中心词的概率。</li></ul><p>$$<br>P(w_c \mid w_{o_1}, \ldots, w_{o_{2m}}) = \frac{\text{exp}\left(\frac{1}{2m}\mathbf{u}<em>c^\top (\mathbf{v}</em>{o_1} + \ldots, + \mathbf{v}<em>{o</em>{2m}}) \right)}{ \sum_{i \in \mathcal{V}} \text{exp}\left(\frac{1}{2m}\mathbf{u}<em>i^\top (\mathbf{v}</em>{o_1} + \ldots, + \mathbf{v}<em>{o</em>{2m}}) \right)}.<br>$$</p><ul><li>关于这里的u和v，即对中心词和上下文是两个不同的embedding矩阵进行处理，在训练完成后，取中心词的矩阵作为词向量表。</li></ul><h3 id="课后问题"><a href="#课后问题" class="headerlink" title="课后问题"></a>课后问题</h3><ul><li>让我们以跳元模型为例来思考word2vec设计。跳元模型中两个词向量的点积与余弦相似度之间有什么关系？对于语义相似的一对词，为什么它们的词向量（由跳元模型训练）的余弦相似度可能很高？</li></ul><pre class="line-numbers language-none"><code class="language-none">因为语义相近的一对词，在训练时上下文对应的词元也比较相似。所以会被训练成比较相近的向量因为考虑之前计算P的公式是向量点积如果两个词相近，他们在词向量上：（1）cosine similarity 接近1（2）点积较大<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="近似训练"><a href="#近似训练" class="headerlink" title="近似训练"></a>近似训练</h2><p><a href="https://zh.d2l.ai/chapter_natural-language-processing-pretraining/approx-training.html">14.2. 近似训练 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p><ul><li>在计算loss的时候softmax需要遍历整个词表，所以词数量大的时候费用高。故引入近似训练</li><li>因为使用了sigmoid，为了使其接近1，$\frac{1}{1+e^{-x}}$是x趋近于无穷才好，这样词向量会变得很大，引入随机的噪声（负采样）</li></ul><p>$$<br>P(w^{(t+j)} \mid w^{(t)}) =P(D=1\mid w^{(t)}, w^{(t+j)})\prod_{k=1,\ w_k \sim P(w)}^K P(D=0\mid w^{(t)}, w_k).<br>$$</p><ul><li>这样loss计算线性的复杂度依赖于k，而不是整个词表</li></ul><p>$$<br>\text{Loss}(w_i) = -\log(\sigma(\mathbf{v}<em>c \cdot \mathbf{v}</em>{w_i})) - \sum_{j=1}^{k} \log(\sigma(-\mathbf{v}<em>c \cdot \mathbf{v}</em>{n_j}))<br>$$</p><h2 id="word2vec数据集构建"><a href="#word2vec数据集构建" class="headerlink" title="word2vec数据集构建"></a>word2vec数据集构建</h2><p><a href="https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word-embedding-dataset.html">14.3. 用于预训练词嵌入的数据集 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p><ul><li>高频词下采样（高频词概率删除）</li><li>负采样和下采样要区分一下概念</li><li>根据word2vec论文中的建议，将噪声词w的采样概率P(w)设置为其在字典中的相对频率</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">sampling_weights <span class="token operator">=</span> <span class="token punctuation">[</span>counter<span class="token punctuation">[</span>vocab<span class="token punctuation">.</span>to_tokens<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">**</span><span class="token number">0.75</span>                        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li><p>代码里面很多实现细节，比如这个draw出来的词要确认一下不在上下文中，才能作为负采样。draw也会抽取重复的元素，不用进行去重。</p></li><li><p>还有这段随机生成的代码，之前不懂为什么这么写，其实这里是个小优化，一次批量的随机生成和context长度相同的随机词，之后调用draw每次返回一个，目的是省资源。</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">RandomGenerator</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""根据n个采样权重在{1,...,n}中随机抽取"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sampling_weights<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># Exclude</span>        self<span class="token punctuation">.</span>population <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sampling_weights<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>sampling_weights <span class="token operator">=</span> sampling_weights        self<span class="token punctuation">.</span>candidates <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>i <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">def</span> <span class="token function">draw</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>i <span class="token operator">==</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>candidates<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># 这里比较精髓，批量生成，单次返回一个</span>            <span class="token comment"># 缓存k个随机采样结果</span>            self<span class="token punctuation">.</span>candidates <span class="token operator">=</span> random<span class="token punctuation">.</span>choices<span class="token punctuation">(</span>                self<span class="token punctuation">.</span>population<span class="token punctuation">,</span> self<span class="token punctuation">.</span>sampling_weights<span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>i <span class="token operator">=</span> <span class="token number">0</span>        self<span class="token punctuation">.</span>i <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>candidates<span class="token punctuation">[</span>self<span class="token punctuation">.</span>i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span>   <span class="token keyword">def</span> <span class="token function">get_negatives</span><span class="token punctuation">(</span>all_contexts<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> counter<span class="token punctuation">,</span> K<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""返回负采样中的噪声词"""</span>    <span class="token comment"># 索引为1、2、...（索引0是词表中排除的未知标记）</span>    sampling_weights <span class="token operator">=</span> <span class="token punctuation">[</span>counter<span class="token punctuation">[</span>vocab<span class="token punctuation">.</span>to_tokens<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">**</span><span class="token number">0.75</span>                        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>    all_negatives<span class="token punctuation">,</span> generator <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> RandomGenerator<span class="token punctuation">(</span>sampling_weights<span class="token punctuation">)</span>    <span class="token keyword">for</span> contexts <span class="token keyword">in</span> all_contexts<span class="token punctuation">:</span><span class="token comment"># 遍历每组中心词和context</span>        negatives <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">while</span> <span class="token builtin">len</span><span class="token punctuation">(</span>negatives<span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>contexts<span class="token punctuation">)</span> <span class="token operator">*</span> K<span class="token punctuation">:</span> <span class="token comment"># 这里看出这个K参数是生成k倍于context的负采样</span>            neg <span class="token operator">=</span> generator<span class="token punctuation">.</span>draw<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token comment"># 噪声词不能是上下文词</span>            <span class="token keyword">if</span> neg <span class="token keyword">not</span> <span class="token keyword">in</span> contexts<span class="token punctuation">:</span>                negatives<span class="token punctuation">.</span>append<span class="token punctuation">(</span>neg<span class="token punctuation">)</span>        all_negatives<span class="token punctuation">.</span>append<span class="token punctuation">(</span>negatives<span class="token punctuation">)</span>    <span class="token keyword">return</span> all_negativesall_negatives <span class="token operator">=</span> get_negatives<span class="token punctuation">(</span>all_contexts<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> counter<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>这里这个batchify感觉也挺巧妙，把之前分开来的center，context和negative整合到一个batch中。它计算所有context和negative的最大长度，然后为了保证batch的长度统一，后面的都用0填充</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">batchify</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""返回带有负采样的跳元模型的小批量样本"""</span>    max_len <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>c<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token builtin">len</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span> <span class="token keyword">for</span> _<span class="token punctuation">,</span> c<span class="token punctuation">,</span> n <span class="token keyword">in</span> data<span class="token punctuation">)</span>    centers<span class="token punctuation">,</span> contexts_negatives<span class="token punctuation">,</span> masks<span class="token punctuation">,</span> labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> center<span class="token punctuation">,</span> context<span class="token punctuation">,</span> negative <span class="token keyword">in</span> data<span class="token punctuation">:</span>        cur_len <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>context<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token builtin">len</span><span class="token punctuation">(</span>negative<span class="token punctuation">)</span>        centers <span class="token operator">+=</span> <span class="token punctuation">[</span>center<span class="token punctuation">]</span>        contexts_negatives <span class="token operator">+=</span> \            <span class="token punctuation">[</span>context <span class="token operator">+</span> negative <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>max_len <span class="token operator">-</span> cur_len<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment"># 填充0</span>        masks <span class="token operator">+=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> cur_len <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>max_len <span class="token operator">-</span> cur_len<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment"># mask，无效的0填充的mask=0</span>        labels <span class="token operator">+=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>context<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>max_len <span class="token operator">-</span> <span class="token builtin">len</span><span class="token punctuation">(</span>context<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment"># 正样本1负样本0</span>    <span class="token comment"># centers展平？</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>centers<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>         contexts_negatives<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>masks<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>关于这个reshape，<code>-1</code> 的意思是根据张量的大小自动推断该维度的大小，而 <code>1</code> 表示该维度的大小为 1。这里的作用和tensor.flatten()是一样的。（多维向量展平）</li></ul><h2 id="实现word2vec训练"><a href="#实现word2vec训练" class="headerlink" title="实现word2vec训练"></a>实现word2vec训练</h2><p><a href="https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec-pretraining.html">14.4. 预训练word2vec — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p><ul><li>之前在处理数据集的时候为了batch大小的一致性，引入mask，这在后面的bceloss有体现，即作为nn.functional.binary_cross_entropy_with_logits的参数传入</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SigmoidBCELoss</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 带掩码的二元交叉熵损失</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> target<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        out <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>binary_cross_entropy_with_logits<span class="token punctuation">(</span>            inputs<span class="token punctuation">,</span> target<span class="token punctuation">,</span> weight<span class="token operator">=</span>mask<span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token string">"none"</span><span class="token punctuation">)</span><span class="token comment"># 调用</span>        <span class="token keyword">return</span> out<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>loss <span class="token operator">=</span> SigmoidBCELoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>使用bceloss是因为配合负采样的方法。sigmoidbceloss就是将yhat进行sigmoid后再和y计算bceloss</li><li>我有点不懂这两个embedding层为什么能加到一个sequential里面去，不是变成顺序执行了吗？</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_embeddings<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">,</span>                                 embedding_dim<span class="token operator">=</span>embed_size<span class="token punctuation">)</span><span class="token punctuation">,</span>                    nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_embeddings<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">,</span>                                 embedding_dim<span class="token operator">=</span>embed_size<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li>gpt说其实不是，是可以这么用的，这样放在里面并不一定是串行</li><li>然后应用这组embeddinglayer的参数，d2l举例的是通过余弦相似度查找最接近的词</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_similar_tokens</span><span class="token punctuation">(</span>query_token<span class="token punctuation">,</span> k<span class="token punctuation">,</span> embed<span class="token punctuation">)</span><span class="token punctuation">:</span>    W <span class="token operator">=</span> embed<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data    x <span class="token operator">=</span> W<span class="token punctuation">[</span>vocab<span class="token punctuation">[</span>query_token<span class="token punctuation">]</span><span class="token punctuation">]</span>    <span class="token comment"># 计算余弦相似性。增加1e-9以获得数值稳定性</span>    cos <span class="token operator">=</span> torch<span class="token punctuation">.</span>mv<span class="token punctuation">(</span>W<span class="token punctuation">,</span> x<span class="token punctuation">)</span> <span class="token operator">/</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>W <span class="token operator">*</span> W<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span>                                      torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>x <span class="token operator">*</span> x<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">9</span><span class="token punctuation">)</span>    topk <span class="token operator">=</span> torch<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>cos<span class="token punctuation">,</span> k<span class="token operator">=</span>k<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">'int32'</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> topk<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">:</span>  <span class="token comment"># 删除输入词</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'cosine sim=</span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">float</span><span class="token punctuation">(</span>cos<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{</span>vocab<span class="token punctuation">.</span>to_tokens<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>get_similar_tokens<span class="token punctuation">(</span><span class="token string">'chip'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>这里出现了两个新的函数，一个是torch.mv，实现了类似于matmul的功能，matmul是矩阵x矩阵，mv只能应用于矩阵x向量，mv针对于这种情况专门优化了</li><li>还有topk：topk(k, dim=None, largest=True, sorted=True) 这里输入参数k=3，使用k=k+1是因为第一名肯定是这个词本身吧</li></ul><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p><a href="https://zh-v2.d2l.ai/chapter_recurrent-modern/lstm.html">9.2. 长短期记忆网络（LSTM） — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p><p>[<a href="https://www.zhihu.com/tardis/zm/art/104475016?source_id=1005">干货]深入浅出LSTM及其Python代码实现 (zhihu.com)</a></p><ul><li>在循环神经网络RNN中就引入了隐状态的概念,在RNN中是通过隐状态传递之前的信息</li></ul><p>$$<br>\mathbf{H}<em>t = \phi(\mathbf{X}<em>t \mathbf{W}</em>{xh} + \mathbf{H}</em>{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h)\<br>\mathbf{O}_t = \mathbf{H}<em>t \mathbf{W}</em>{hq} + \mathbf{b}_q.<br>$$</p><ul><li>在LSTM中，隐状态H和记忆细胞C有什么区别？GPT:H通常用于短期信息的编码，而C则用于长期信息的编码</li><li>门单元计算如下,有h个隐藏单元和d的输入大小。这里面X的大小是$d\cdot h$，W的大小$h\cdot h$</li></ul><p>$$<br>\begin{split}\begin{aligned}<br>\mathbf{I}<em>t &amp;= \sigma(\mathbf{X}<em>t \mathbf{W}</em>{xi} + \mathbf{H}</em>{t-1} \mathbf{W}_{hi} + \mathbf{b}<em>i),\<br>\mathbf{F}<em>t &amp;= \sigma(\mathbf{X}<em>t \mathbf{W}</em>{xf} + \mathbf{H}</em>{t-1} \mathbf{W}</em>{hf} + \mathbf{b}<em>f),\<br>\mathbf{O}<em>t &amp;= \sigma(\mathbf{X}<em>t \mathbf{W}</em>{xo} + \mathbf{H}</em>{t-1} \mathbf{W}</em>{ho} + \mathbf{b}_o),<br>\end{aligned}\end{split}<br>$$</p><ul><li>隐状态H是一直传递下去，C的传递收到几个门控的影响。</li></ul><p><img src="https://zh-v2.d2l.ai/_images/lstm-3.svg"></p><ul><li><p>图里面涉及记忆细胞更新的部分使用tanh，即输出在-1到1之间，而其他门控使用sigmoid控制在01之间</p></li><li><p>对应到代码里很直观</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">lstm</span><span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> state<span class="token punctuation">,</span> params<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token punctuation">[</span>W_xi<span class="token punctuation">,</span> W_hi<span class="token punctuation">,</span> b_i<span class="token punctuation">,</span> W_xf<span class="token punctuation">,</span> W_hf<span class="token punctuation">,</span> b_f<span class="token punctuation">,</span> W_xo<span class="token punctuation">,</span> W_ho<span class="token punctuation">,</span> b_o<span class="token punctuation">,</span> W_xc<span class="token punctuation">,</span> W_hc<span class="token punctuation">,</span> b_c<span class="token punctuation">,</span>     W_hq<span class="token punctuation">,</span> b_q<span class="token punctuation">]</span> <span class="token operator">=</span> params    <span class="token punctuation">(</span>H<span class="token punctuation">,</span> C<span class="token punctuation">)</span> <span class="token operator">=</span> state    outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> X <span class="token keyword">in</span> inputs<span class="token punctuation">:</span>        I <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xi<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_hi<span class="token punctuation">)</span> <span class="token operator">+</span> b_i<span class="token punctuation">)</span>        F <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xf<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_hf<span class="token punctuation">)</span> <span class="token operator">+</span> b_f<span class="token punctuation">)</span>        O <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xo<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_ho<span class="token punctuation">)</span> <span class="token operator">+</span> b_o<span class="token punctuation">)</span> <span class="token comment"># 这里的O和RNN中的O不同，是个门，RNN中的O直接是输出</span>        C_tilda <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span><span class="token punctuation">(</span>X @ W_xc<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>H @ W_hc<span class="token punctuation">)</span> <span class="token operator">+</span> b_c<span class="token punctuation">)</span> <span class="token comment"># 候选记忆元</span>        C <span class="token operator">=</span> F <span class="token operator">*</span> C <span class="token operator">+</span> I <span class="token operator">*</span> C_tilda <span class="token comment"># 记住多少</span>        H <span class="token operator">=</span> O <span class="token operator">*</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>C<span class="token punctuation">)</span>        Y <span class="token operator">=</span> <span class="token punctuation">(</span>H @ W_hq<span class="token punctuation">)</span> <span class="token operator">+</span> b_q <span class="token comment"># 注意这里的输出就是把隐状态乘上Hq再加偏置</span>        outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Y<span class="token punctuation">)</span>    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>H<span class="token punctuation">,</span> C<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="实战：transformers库实现电影情感分析"><a href="#实战：transformers库实现电影情感分析" class="headerlink" title="实战：transformers库实现电影情感分析"></a>实战：transformers库实现电影情感分析</h2><p>目前打算跟pku的项目列表写一次<a href="https://github.com/PKU-TANGENT/nlp-tutorial">PKU-TANGENT/nlp-tutorial: NLP新手入门教程 (github.com)</a></p><ul><li><p>kaggle竞赛地址：<a href="https://www.kaggle.com/competitions/sentiment-analysis-on-movie-reviews">Sentiment Analysis on Movie Reviews | Kaggle</a></p></li><li><p>这里发现一个很好的博客<a href="https://jinhanlei.github.io/">Jin blog (jinhanlei.github.io)</a>讲了transformers和pytorch的配合使用</p></li><li><p>主体代码参考这段<a href="https://blog.csdn.net/weixin_40605573/article/details/113140270">【精选】（六）使用Transformers进行情感分析_基于transformer的情感分析-CSDN博客</a></p></li></ul><h3 id="写的过程中遇到的问题"><a href="#写的过程中遇到的问题" class="headerlink" title="写的过程中遇到的问题"></a>写的过程中遇到的问题</h3><ul><li>关于dataloder怎么移动到gpu上，其实是在训练代码中读入之后再移动到device</li></ul><p><a href="https://geek-docs.com/pytorch/pytorch-questions/526_pytorch_load_pytorch_dataloader_into_gpu.html">Pytorch 将Pytorch的Dataloader加载到GPU中|极客教程 (geek-docs.com)</a></p><ul><li>用tqdm显示进度条</li></ul><p><a href="https://blog.csdn.net/weixin_43941438/article/details/128171441">深度学习Pytorch通过tqdm实现进度条打印训练过程信息_pytorch训练进度条-CSDN博客</a></p><h3 id="工作总结"><a href="#工作总结" class="headerlink" title="工作总结"></a>工作总结</h3><ul><li>基本上模型结构照搬博客里给的内容，输出层从单层的Linear改成了一个1024隐藏层的MLP。</li><li>博客中是输出0/1表示positive和negative，kaggle中的是一个5分类问题，我这里通过MLP输出到一个1*5的大小再通过softmax，把label变成onehot编码后计算损失。输出结果用argmax。</li><li>原文中使用了torchtext.data，但是这个包在0.9.0后被移除了，先处理好格式后再使用torch的dataloader也挺方便。</li></ul><h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><ul><li>在BiLSTM中，隐状态H是正向和反向两个LSTM的输出拼接。<a href="https://zhuanlan.zhihu.com/p/47802053">详解BiLSTM及代码实现 - 知乎 (zhihu.com)</a></li><li>ELMo中embedding使用的是卷积的方法<a href="https://blog.csdn.net/Magical_Bubble/article/details/89160032">ELMo解读（论文 + PyTorch源码）_elmo代码-CSDN博客</a></li><li>这个比较好懂,而且解释的非常详细。<a href="https://zhuanlan.zhihu.com/p/51679783">ELMo原理解析及简单上手使用 - 知乎 (zhihu.com)</a></li><li>ELMo大致了解就是一个多层的BiLM，对同一层中的正反LSTM进行concat连接，然后给每一层的输出加权相加得到最终输出。</li></ul><p>$$<br>h_{k, j}^{L M}=\left[\overrightarrow{h_{k, j}^{L M}} ; \overleftarrow{h_{k, j}^{L M}}\right]<br>\<br>\mathbf{E L M o}<em>k^{\text {task }}=E\left(R_k ; \Theta^{\text {task }}\right)=\gamma^{\text {task }} \sum</em>{j=0}^L s_j^{\text {task }} \mathbf{h}_{k, j}^{L M}<br>$$</p><ul><li>RNN，LSTM的长程梯度消失问题，对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有有效信息，即便 LSTM 加了门控机制可以选择性遗忘和记忆，随着所需翻译的句子难度怎能更加，这个结构的效果仍然不理想。</li></ul><h2 id="多头自注意力"><a href="#多头自注意力" class="headerlink" title="多头自注意力"></a>多头自注意力</h2><ul><li>关于多头注意力，看李沐的教学<a href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/multihead-attention.html">10.5. 多头注意力 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></li><li>沐神的代码：</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#@save</span><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""多头注意力"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> key_size<span class="token punctuation">,</span> query_size<span class="token punctuation">,</span> value_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span>                 num_heads<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> d2l<span class="token punctuation">.</span>DotProductAttention<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W_q <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>query_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W_k <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>key_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W_v <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>value_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>W_o <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>使用GPT让他对forward函数的中间变量大小进行标注</li><li><code>B</code>: batch_size</li><li><code>Tq</code>: 查询的个数（queries）</li><li><code>Tk</code>: 键的个数（keys）</li><li><code>Tv</code>: 值的个数（values）</li><li><code>H</code>: 注意力头的个数（num_heads）</li><li><code>D</code>: 隐藏单元的个数（num_hiddens）</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> queries<span class="token punctuation">,</span> keys<span class="token punctuation">,</span> values<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># queries，keys，values的形状:</span>    <span class="token comment"># (B，Tq，D)， (B，Tk，D)， (B，Tv，D)</span>    <span class="token comment"># valid_lens的形状:</span>    <span class="token comment"># (B,) 或 (B，Tq)</span>    <span class="token comment"># 经过变换后，输出的queries，keys，values的形状:</span>    <span class="token comment"># (B*H，Tq，D/H)， (B*H，Tk，D/H)， (B*H，Tv，D/H)</span>    queries <span class="token operator">=</span> transpose_qkv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_q<span class="token punctuation">(</span>queries<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>  <span class="token comment"># (B*H, Tq, D/H)</span>    keys <span class="token operator">=</span> transpose_qkv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_k<span class="token punctuation">(</span>keys<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>        <span class="token comment"># (B*H, Tk, D/H)</span>    values <span class="token operator">=</span> transpose_qkv<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_v<span class="token punctuation">(</span>values<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>    <span class="token comment"># (B*H, Tv, D/H)</span>    <span class="token keyword">if</span> valid_lens <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>        <span class="token comment"># 在轴0，将第一项（标量或者矢量）复制H次，</span>        <span class="token comment"># 然后如此复制第二项，然后诸如此类。</span>        valid_lens <span class="token operator">=</span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>            valid_lens<span class="token punctuation">,</span> repeats<span class="token operator">=</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># (B*H,)</span>    <span class="token comment"># output的形状: (B*H, Tq, D/H)</span>    output <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>queries<span class="token punctuation">,</span> keys<span class="token punctuation">,</span> values<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span>    <span class="token comment"># output_concat的形状: (B, Tq, D)</span>    output_concat <span class="token operator">=</span> transpose_output<span class="token punctuation">(</span>output<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>W_o<span class="token punctuation">(</span>output_concat<span class="token punctuation">)</span>  <span class="token comment"># (B, Tq, D)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>其实代码很难懂，但是评论区有人画了一张图来表示中间shape的变化，非常形象</li></ul><p><img src="/img/nlp_study/multihead_attention.png" alt="multihead_attention"></p><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><ul><li>代码依旧是看李沐d2l的代码<a href="https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/bert.html">14.8. 来自Transformers的双向编码器表示（BERT） — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></li><li>这篇相对详细 <a href="https://zhuanlan.zhihu.com/p/403495863">读懂BERT，看这一篇就够了 - 知乎 (zhihu.com)</a></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_tokens_and_segments</span><span class="token punctuation">(</span>tokens_a<span class="token punctuation">,</span> tokens_b<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""获取输入序列的词元及其片段索引"""</span>    tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'&lt;cls&gt;'</span><span class="token punctuation">]</span> <span class="token operator">+</span> tokens_a <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">'&lt;sep&gt;'</span><span class="token punctuation">]</span>    <span class="token comment"># 0和1分别标记片段A和B</span>    segments <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>tokens_a<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment"># A句的分割嵌入为0</span>    <span class="token keyword">if</span> tokens_b <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>        tokens <span class="token operator">+=</span> tokens_b <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">'&lt;sep&gt;'</span><span class="token punctuation">]</span>        segments <span class="token operator">+=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>tokens_b<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># B句的设置为1</span>    <span class="token keyword">return</span> tokens<span class="token punctuation">,</span> segments<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="三种embedding"><a href="#三种embedding" class="headerlink" title="三种embedding"></a>三种embedding</h3><ul><li>bert的输入是句子的三层表征：token，分割（a句/b句），位置。对应到代码中的token_embedding,segement_embedding和pos_embedding(是learnable的参数)</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">BERTEncoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""BERT编码器"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> norm_shape<span class="token punctuation">,</span> ffn_num_input<span class="token punctuation">,</span>                 ffn_num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span>                 max_len<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> key_size<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span> query_size<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span> value_size<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span>                 <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>BERTEncoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>token_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>segment_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>blks <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 串联encoder block</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>blks<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>EncoderBlock<span class="token punctuation">(</span>                key_size<span class="token punctuation">,</span> query_size<span class="token punctuation">,</span> value_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> norm_shape<span class="token punctuation">,</span>                ffn_num_input<span class="token punctuation">,</span> ffn_num_hiddens<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数</span>        self<span class="token punctuation">.</span>pos_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> max_len<span class="token punctuation">,</span>                                                      num_hiddens<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token punctuation">,</span> segments<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）</span>        X <span class="token operator">=</span> self<span class="token punctuation">.</span>token_embedding<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>segment_embedding<span class="token punctuation">(</span>segments<span class="token punctuation">)</span>        X <span class="token operator">=</span> X <span class="token operator">+</span> self<span class="token punctuation">.</span>pos_embedding<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment"># 加入位置编码</span>        <span class="token keyword">for</span> blk <span class="token keyword">in</span> self<span class="token punctuation">.</span>blks<span class="token punctuation">:</span>            X <span class="token operator">=</span> blk<span class="token punctuation">(</span>X<span class="token punctuation">,</span> valid_lens<span class="token punctuation">)</span>        <span class="token keyword">return</span> X<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>从代码中也可以看出，bert的embedding是三种embedding的直接求和</li><li>因为NLP没有CV的Imagenet这样庞大数据集，所以bert主要还是采用无监督学习的方式。两个任务：Masked Language Model（随机添加mask/替换句子）和Next Sentence Prediction（预测两个句子是不是连在一起）</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MaskLM</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""BERT的掩蔽语言模型任务"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> num_inputs<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>MaskLM<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>mlp <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span>                                 nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                                 nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span>                                 nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">,</span> pred_positions<span class="token punctuation">)</span><span class="token punctuation">:</span>        num_pred_positions <span class="token operator">=</span> pred_positions<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        pred_positions <span class="token operator">=</span> pred_positions<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 输入是2维的所以reshape一下</span>        batch_size <span class="token operator">=</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        batch_idx <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>        batch_idx <span class="token operator">=</span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>batch_idx<span class="token punctuation">,</span> num_pred_positions<span class="token punctuation">)</span> <span class="token comment"># 将batch_idx中的每个元素重复num_pred_positions次</span>        <span class="token comment"># 假设batch_size=2，num_pred_positions=3</span>        <span class="token comment"># 那么batch_idx是np.array（[0,0,0,1,1,1]）</span>                masked_X <span class="token operator">=</span> X<span class="token punctuation">[</span>batch_idx<span class="token punctuation">,</span> pred_positions<span class="token punctuation">]</span>        <span class="token comment"># 在每个batch中找到对应被掩盖的位置的输出，单独拎出来</span>        masked_X <span class="token operator">=</span> masked_X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> num_pred_positions<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># masked_X shape: [batch_size, sequence_length, feature_dim]</span>    <span class="token comment"># 一开始有疑问为什么没有用mask替换的代码</span>        <span class="token comment"># 因为已经在数据预处理阶段被替换为掩盖标记</span>        mlm_Y_hat <span class="token operator">=</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>masked_X<span class="token punctuation">)</span> <span class="token comment"># 经过一个MLP得到预测</span>        <span class="token keyword">return</span> mlm_Y_hat            mlm <span class="token operator">=</span> MaskLM<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">)</span>mlm_positions <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># batchsize = 2</span>mlm_Y_hat <span class="token operator">=</span> mlm<span class="token punctuation">(</span>encoded_X<span class="token punctuation">,</span> mlm_positions<span class="token punctuation">)</span> <span class="token comment"># 计算损失</span>mlm_Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>mlm_l <span class="token operator">=</span> loss<span class="token punctuation">(</span>mlm_Y_hat<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> mlm_Y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>上面很多理解都写到注释里面去了</li></ul><h3 id="我的疑问"><a href="#我的疑问" class="headerlink" title="我的疑问"></a>我的疑问</h3><ul><li>问：那这些被mask的位置之间可能相隔很远没有关联，每组不同的输入他们的位置关系也会变化，为什么要传入同一个MLP中呢？</li></ul><pre class="line-numbers language-none"><code class="language-none">GPT：- 您提出的观点是正确的：在掩蔽语言模型（Masked Language Model, MLM）任务中，被掩盖的位置可能在输入文本中相隔很远，它们之间可能没有直接的关联。此外，不同的输入示例可能具有不同的掩盖位置，因此这些位置之间的关系会变化。- MLP模型的目标是理解上下文并预测掩盖位置上的单词。尽管这些位置之间可能没有直接的关联，但上下文中的其他单词提供了相关信息，帮助模型进行预测。共享的MLP可以捕获这些上下文信息。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>感觉这个回答还是有点玄学的，不过从代码来看，确实是直接把pred_position位置的特征单独拿出来，不管他们在句中的位置间隔是否固定，直接一起传到MLP里面去。</li></ul><p><img src="https://pic3.zhimg.com/80/v2-55dfeb40658afd90b10e039c64e0ec06_1440w.webp"></p><ul><li><p>这里也解释了为什么引入CLS token：对于一些token级别的任务<em>（如，序列标注和问答任务）</em>，就把Ti输入到额外的输出层中进行预测。对于一些句子级别的任务<em>（如，自然语言推断和情感分类任务）</em>，就把C输入到额外的输出层中，这里也就解释了为什么要在每一个token序列前都要插入特定的分类token。</p></li><li><p>关于elmo，bert，GPT1的区别</p></li></ul><p><img src="https://pic1.zhimg.com/80/v2-1cd7dfb971e9812bb11bcfc4d5e5cc2c_1440w.webp"></p><h2 id="Huggingface-Transformers"><a href="#Huggingface-Transformers" class="headerlink" title="Huggingface Transformers"></a>Huggingface Transformers</h2><ul><li>在transformers库源代码目录下src/transformers/下有各种主要功能的代码，比如/src/transformers/models/bert/modeling_bert.py中的bert源码</li><li>这个是transformers库文件结构的介绍视频，里面一整个系列都是带读transformers库源码的，之后有时间再看。<a href="https://www.bilibili.com/video/BV1Tm4y1M7nQ">https://www.bilibili.com/video/BV1Tm4y1M7nQ</a></li><li>关于transformers库如何配合pytorch使用<a href="https://jinhanlei.github.io/posts/Transformers%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8-%E5%9B%9B-%E7%BB%93%E5%90%88Transformers%E5%92%8CPyTorch%E4%BF%AE%E6%94%B9%E6%A8%A1%E5%9E%8B/">Transformers快速入门（四）：结合Transformers和PyTorch修改模型 | Jin blog (jinhanlei.github.io)</a></li></ul><h3 id="Transformers库BLIP2代码阅读"><a href="#Transformers库BLIP2代码阅读" class="headerlink" title="Transformers库BLIP2代码阅读"></a>Transformers库BLIP2代码阅读</h3><h4 id="视觉embedding部分"><a href="#视觉embedding部分" class="headerlink" title="视觉embedding部分"></a>视觉embedding部分</h4><ul><li>通过一个ViT，把不同分辨率的图片映射</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Blip2VisionEmbeddings</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">:</span> Blip2VisionConfig<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config        self<span class="token punctuation">.</span>embed_dim <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size        self<span class="token punctuation">.</span>image_size <span class="token operator">=</span> config<span class="token punctuation">.</span>image_size        self<span class="token punctuation">.</span>patch_size <span class="token operator">=</span> config<span class="token punctuation">.</span>patch_size        self<span class="token punctuation">.</span>class_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># CLS token，是可学习的</span>        self<span class="token punctuation">.</span>patch_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>             in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span>self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>self<span class="token punctuation">.</span>patch_size<span class="token punctuation">,</span> stride<span class="token operator">=</span>self<span class="token punctuation">.</span>patch_size        <span class="token punctuation">)</span> <span class="token comment"># 注意这里stride == patch_size，说明是每个patch和卷积核卷积提取特征</span>        <span class="token comment"># 从3个通道到embed_dim个通道，是为了映射到和单词同样的维度。这里</span>        self<span class="token punctuation">.</span>num_patches <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>image_size <span class="token operator">//</span> self<span class="token punctuation">.</span>patch_size<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span> <span class="token comment"># 适用于不同分辨率的图片，图片变大则patch变多</span>        self<span class="token punctuation">.</span>num_positions <span class="token operator">=</span> self<span class="token punctuation">.</span>num_patches <span class="token operator">+</span> <span class="token number">1</span>        self<span class="token punctuation">.</span>position_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_positions<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 使用可学习的位置编码</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pixel_values<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>        batch_size <span class="token operator">=</span> pixel_values<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        target_dtype <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>dtype        patch_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embedding<span class="token punctuation">(</span>pixel_values<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>target_dtype<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># shape = [*, width, grid, grid]</span>        <span class="token comment"># 通过卷积，分patch</span>        patch_embeds <span class="token operator">=</span> patch_embeds<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        class_embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>class_embedding<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>target_dtype<span class="token punctuation">)</span>        embeddings <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>class_embeds<span class="token punctuation">,</span> patch_embeds<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 把CLS token和图片embedding 接在一起</span>        embeddings <span class="token operator">=</span> embeddings <span class="token operator">+</span> self<span class="token punctuation">.</span>position_embedding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span> embeddings<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>target_dtype<span class="token punctuation">)</span> <span class="token comment"># 加上位置编码</span>        <span class="token keyword">return</span> embeddings<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Blip2Attention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Multi-headed attention from 'Attention Is All You Need' paper"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>config <span class="token operator">=</span> config        self<span class="token punctuation">.</span>embed_dim <span class="token operator">=</span> config<span class="token punctuation">.</span>hidden_size        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> config<span class="token punctuation">.</span>num_attention_heads        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> self<span class="token punctuation">.</span>embed_dim <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads        <span class="token keyword">if</span> self<span class="token punctuation">.</span>head_dim <span class="token operator">*</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">!=</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">:</span>            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>                <span class="token string-interpolation"><span class="token string">f"embed_dim must be divisible by num_heads (got `embed_dim`: </span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">}</span></span><span class="token string"> and `num_heads`:"</span></span>                <span class="token string-interpolation"><span class="token string">f" </span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>num_heads<span class="token punctuation">}</span></span><span class="token string">)."</span></span>            <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>scale <span class="token operator">=</span> self<span class="token punctuation">.</span>head_dim<span class="token operator">**</span><span class="token operator">-</span><span class="token number">0.5</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>config<span class="token punctuation">.</span>attention_dropout<span class="token punctuation">)</span>        <span class="token comment"># small tweak here compared to CLIP, no bias here</span>        self<span class="token punctuation">.</span>qkv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">,</span> <span class="token number">3</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        <span class="token comment"># 不单独建Wq，Wk，Wv，直接和在一起和input相乘，所以是3*embed_dim</span>        <span class="token comment"># 如果是单独一个q，就是q = nn.Linear(self.embed_dim, self.embed_dim, bias=False)</span>        <span class="token keyword">if</span> config<span class="token punctuation">.</span>qkv_bias<span class="token punctuation">:</span>            q_bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 参数设置是否使用bias</span>            v_bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            q_bias <span class="token operator">=</span> <span class="token boolean">None</span>            v_bias <span class="token operator">=</span> <span class="token boolean">None</span>        <span class="token keyword">if</span> q_bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            qkv_bias <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>q_bias<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>v_bias<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span> v_bias<span class="token punctuation">)</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>qkv<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>qkv_bias<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>projection <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span>        <span class="token comment"># 模态投影层projection layer</span>    <span class="token keyword">def</span> <span class="token function">_shape</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tensor<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> seq_len<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> bsz<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> tensor<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        hidden_states<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>        head_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        output_attentions<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Input shape: Batch x Time x Channel"""</span>        bsz<span class="token punctuation">,</span> tgt_len<span class="token punctuation">,</span> embed_dim <span class="token operator">=</span> hidden_states<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        mixed_qkv <span class="token operator">=</span> self<span class="token punctuation">.</span>qkv<span class="token punctuation">(</span>hidden_states<span class="token punctuation">)</span> <span class="token comment"># 这里为了方便计算，不单独建Wq，Wk，Wv，直接和在一起和input相乘</span>        mixed_qkv <span class="token operator">=</span> mixed_qkv<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> tgt_len<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> embed_dim <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span>            <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span>        <span class="token punctuation">)</span> <span class="token comment"># 把维度重排，原本2维度上拼接起来的qkv结果，这里给他移动到第0维，方便后面拆分</span>        <span class="token comment"># 这里的5个维度：qkv拼接，batch大小，注意力头数，序列长度，embed_dim</span>        query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">,</span> value_states <span class="token operator">=</span> mixed_qkv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> mixed_qkv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> mixed_qkv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>        <span class="token comment"># 在第0维上拆分成q k v</span>        <span class="token comment"># Take the dot product between "query" and "key" to get the raw attention scores.</span>        attention_scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># 相当于q(seq_len, embed_dim) * k(embed_dim, seq_len) 相乘</span>        attention_scores <span class="token operator">=</span> attention_scores <span class="token operator">*</span> self<span class="token punctuation">.</span>scale        <span class="token comment"># 缩放操作是为了确保在计算 softmax 函数时，分数不会因为维度的不同而导致数值问题</span>        <span class="token comment"># Normalize the attention scores to probabilities.</span>        attention_probs <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attention_scores<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># 通过softmax计算得到attention_probs</span>        <span class="token comment"># This is actually dropping out entire tokens to attend to, which might</span>        <span class="token comment"># seem a bit unusual, but is taken from the original Transformer paper.</span>        attention_probs <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>attention_probs<span class="token punctuation">)</span>        <span class="token comment"># 这里介绍了attention进行dropout的方式，直接对attention_prob进行dropout，而不是计算后再对结果dropout</span>        <span class="token comment"># Mask heads if we want to</span>        <span class="token keyword">if</span> head_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            attention_probs <span class="token operator">=</span> attention_probs <span class="token operator">*</span> head_mask        context_layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attention_probs<span class="token punctuation">,</span> value_states<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>        new_context_layer_shape <span class="token operator">=</span> context_layer<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">,</span><span class="token punctuation">)</span>        context_layer <span class="token operator">=</span> context_layer<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>new_context_layer_shape<span class="token punctuation">)</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>projection<span class="token punctuation">(</span>context_layer<span class="token punctuation">)</span>        <span class="token comment"># 经过projection layer输出结果</span>        outputs <span class="token operator">=</span> <span class="token punctuation">(</span>output<span class="token punctuation">,</span> attention_probs<span class="token punctuation">)</span> <span class="token keyword">if</span> output_attentions <span class="token keyword">else</span> <span class="token punctuation">(</span>output<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> outputs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>值得注意的是里面把WqWkWv相接到同一个Linear中，输入经过线性层后再拆分出qkv。</li><li>而且在attention中的dropout，直接对attention_prob进行dropout，而不是计算后再对结果dropout</li></ul><h2 id="阅读LLM综述"><a href="#阅读LLM综述" class="headerlink" title="阅读LLM综述"></a>阅读LLM综述</h2><p><a href="https://zhuanlan.zhihu.com/p/612181615">人工智能 LLM 革命破晓：一文读懂当下超大语言模型发展现状 /麦克船长LLM革命系列2 - 知乎 (zhihu.com)</a></p><ul><li>之前一直不理解为什么叫few-shot prompt：在无监督训练好的 GPT-3，使用时用少量示例（加在prompt里面）就可以得到有较好的输出反馈，这就叫 Few-Shot Prompt。pretrain后的模型通过finetune或者In-context learning完成不同类型的任务，又分zeroshot，oneshot，fewshot，在综述的11.5节有详细区分。</li></ul><h2 id="topk，topp和tempreture"><a href="#topk，topp和tempreture" class="headerlink" title="topk，topp和tempreture"></a>topk，topp和tempreture</h2><ul><li><p>参考这一篇博客<a href="https://zhuanlan.zhihu.com/p/613428710">Top-k &amp; Top-p, Temperature - 知乎 (zhihu.com)</a></p></li><li><p>topk即从前k个高概率的token中随机选，topp是从概率和为topp的前几个token中随机，而tempreture是在softmax中除上了一个系数T，来控制模型的随机性，T越大生成的内容越随机</p></li></ul><h2 id="RoPE编码"><a href="#RoPE编码" class="headerlink" title="RoPE编码"></a>RoPE编码</h2><ul><li>参考1<a href="https://zhuanlan.zhihu.com/p/647832706">超超超超超简单！从结果推导RoPE旋转位置编码 - 知乎 (zhihu.com)</a></li><li>参考2<a href="https://zhuanlan.zhihu.com/p/641274061">【论文阅读】RoPE为何成为大模型中最常见的位置编码？ - 知乎 (zhihu.com)</a></li><li>$\langle f(q, m), f(k, n) \rangle = g(q, k, m - n)$怎么理解？</li><li>f(q,m)和f(k,n)表示查询qk的位置编码，做内积之后，要和g(q, k, m-n)算出来的相等，即结果是和相对位置m-n相关的，要找这样一个映射（位置编码方式）f。</li><li>一开始的疑问是，为什么$e^{m\theta i}$和$e^{n\theta i}$乘积不是$e^{(m+n)\theta i}$而是$e^{(m-n)\theta i}$，因为<strong>两个复向量q，k的内积等于q乘（k的共轭复数），再取实部</strong>即$\langle q,k\rangle = Re(q^*k)$。复数内积参考<a href="https://blog.csdn.net/weixin_43800577/article/details/126869092">复向量的内积（例题详解）-CSDN博客</a></li></ul><p><img src="/img/nlp_study/neiji.png" alt="neiji"></p><ul><li><p>共轭复数的定义是将复数的虚部取负值,对于复数 $re^{\theta i}$，其共轭复数为$re^{-\theta i}$。</p></li><li><p>感觉论文中的2D的例子，可以发现</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法学习笔记 </tag>
            
            <tag> DL </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>李沐d2l课后练习</title>
      <link href="2023/10/09/d2l-practice/"/>
      <url>2023/10/09/d2l-practice/</url>
      
        <content type="html"><![CDATA[<h1 id="李沐D2L-课后practice记录"><a href="#李沐D2L-课后practice记录" class="headerlink" title="李沐D2L-课后practice记录"></a>李沐D2L-课后practice记录</h1><p>习题答案：<a href="https://datawhalechina.github.io/d2l-ai-solutions-manual/#/">动手学深度学习习题解答 (datawhalechina.github.io)</a></p><h2 id="lenet"><a href="#lenet" class="headerlink" title="lenet"></a>lenet</h2><ol><li>将平均汇聚层替换为最大汇聚层，会发生什么？</li></ol><pre class="line-numbers language-none"><code class="language-none">loss收敛更快，acc更高（0.833）<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ol><li><p>尝试构建一个基于LeNet的更复杂的网络，以提高其准确性。</p><ol><li>调整卷积窗口大小。（我从4调整成8，增加感受野，acc=0.826）</li><li>调整输出通道的数量。（增加通道数16=》效果继续变差TAT acc=0.812）</li><li>调整激活函数（如ReLU）。</li><li>调整卷积层的数量。</li><li>调整全连接层的数量。</li><li>调整学习率和其他训练细节（例如，初始化和轮数）。</li></ol><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 手写了一个学习率线性下降</span><span class="token keyword">if</span> epoch <span class="token operator">&gt;=</span> <span class="token number">4</span><span class="token punctuation">:</span>            decay <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">10</span> <span class="token operator">-</span> epoch<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">10</span>            new_lr <span class="token operator">=</span> lr <span class="token operator">*</span> decay            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'in epoch</span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token punctuation">}</span></span><span class="token string">:lr = </span><span class="token interpolation"><span class="token punctuation">{</span>new_lr<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>            optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>new_lr<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>在MNIST数据集上尝试以上改进的网络。</p></li><li><p>显示不同输入（例如毛衣和外套）时，LeNet第一层和第二层的激活值。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法学习笔记 </tag>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>李沐d2lDL部分-深度学习代码</title>
      <link href="2023/09/25/d2l-dl/"/>
      <url>2023/09/25/d2l-dl/</url>
      
        <content type="html"><![CDATA[<h1 id="李沐D2L-DL基础代码"><a href="#李沐D2L-DL基础代码" class="headerlink" title="李沐D2L-DL基础代码"></a>李沐D2L-DL基础代码</h1><h2 id="colab-d2l"><a href="#colab-d2l" class="headerlink" title="colab d2l"></a>colab d2l</h2><ul><li>解决colab没有d2l的问题，使用这个比从github下载更快</li></ul><pre class="line-numbers language-none"><code class="language-none">!pip install d2l==0.14.<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>避免一些无意义的复制粘贴，只记录一些体会和关键的点。</p><h2 id="深度学习计算"><a href="#深度学习计算" class="headerlink" title="深度学习计算"></a>深度学习计算</h2><h3 id="块"><a href="#块" class="headerlink" title="块"></a>块</h3><p>（这段稍微有点记多了）</p><ul><li><p>”当我们使用softmax回归时，一个单层本身就是模型。事实证明，研究讨论“比单个层大”但“比整个模型小”的组件更有价值。 例如，在计算机视觉中广泛流行的ResNet-152架构就有数百层， 这些层是由<em>层组</em>（groups of layers）的重复模式组成。 “</p></li><li><p>使用nn.sequential（torch.nn.modules.container.Sequential,）构造网络，更方便。d2l：<code>nn.Sequential</code>定义了一种特殊的<code>Module</code>)， 即在PyTorch中表示一个块的类， 它维护了一个由<code>Module</code>组成的有序列表。 </p></li><li><p>自己声明nn.Module子类的块（sequential的实现其实也是一样的），同样是自动计算梯度，只需要管前向传播。</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">MLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 多层感知机</span>    <span class="token comment"># 用模型参数声明层。这里，我们声明两个全连接的层</span>    <span class="token comment"># 在init里面声明结构</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 调用MLP的父类Module的构造函数来执行必要的初始化。</span>        <span class="token comment"># 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 调用父类init</span>        self<span class="token punctuation">.</span>hidden <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>  <span class="token comment"># 隐藏层</span>        self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>  <span class="token comment"># 输出层</span>    <span class="token comment"># 定义模型的前向传播，即如何根据输入X返回所需的模型输出</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 使用nn.functional的relu</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>关于为什么使用nn.functional中的relu，gpt：因为它只是一个<strong>函数调用</strong>，而不是一个<strong>层对象</strong>（如nn.ReLU，这样就跟Linear一样要在init中声明）。这使得在模型前向传播中可以直接使用函数调用，而不需要创建额外的层实例。</li><li>d2l中写出了nn.sequential的实现逻辑，即将里面的层作为参数传入，顺便补充一下，<code>type(nn.Linear) = torch.nn.modules.linear.Linear</code></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MySequential</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 定义顺序块</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 通过args传入顺序块中的层</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> idx<span class="token punctuation">,</span> module <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment"># 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员</span>            <span class="token comment"># _module的类型是collections.OrderedDict</span>            self<span class="token punctuation">.</span>_modules<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>idx<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> module    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span>        <span class="token keyword">for</span> block <span class="token keyword">in</span> self<span class="token punctuation">.</span>_modules<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            X <span class="token operator">=</span> block<span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token comment"># 顺序执行_module中的层</span>        <span class="token keyword">return</span> X<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>自己定义块的好处在于可以在自由地进行一些运算，比如d2l中举例的，定义了一组随机的权重参数，并且通过<code>requires_grad=False</code>使这组参数不更新。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">FixedHiddenMLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># 不计算梯度的随机权重参数。因此其在训练期间保持不变</span>        self<span class="token punctuation">.</span>rand_weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token comment"># 随机权重</span>        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        X <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        <span class="token comment"># 使用创建的常量参数以及relu和mm函数</span>        X <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>rand_weight<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 和随机权重相乘</span>        <span class="token comment"># 复用全连接层。这相当于两个全连接层共享参数</span>        X <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        <span class="token comment"># 控制流</span>        <span class="token comment"># 这段定义了一个循环来展示块中在L1范数大于1时一直/2，只是为了演示可以将运算集成到块中</span>        <span class="token keyword">while</span> X<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>             X <span class="token operator">/=</span> <span class="token number">2</span>        <span class="token keyword">return</span> X<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>块也是可以嵌套的，同样，sequential也可以连接nn中封装的层和我们自定义的块</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">NestMLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># 块中嵌套sequential</span>                                 nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span>chimera <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>NestMLP<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span> FixedHiddenMLP<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># sequential再嵌套一层</span>chimera<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="查看参数"><a href="#查看参数" class="headerlink" title="查看参数"></a>查看参数</h3><p>（感觉这几节涉及代码的还是多记点好了）</p><ul><li>几种查看方式<a href="https://zh-v2.d2l.ai/chapter_deep-learning-computation/parameters.html">5.2. 参数管理 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></li><li>python语法：列表推导式</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 一开始看不懂这句</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># *是解压用list用，作为多个元素传给print</span><span class="token comment"># [表达式 for 变量 in 列表] </span><span class="token comment"># 如将i作为列表返回</span>tmp <span class="token operator">=</span> <span class="token punctuation">[</span>i <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>nn.container的add_module，以及d2l没介绍的第二种调用方法</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">block1</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                         nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">block2</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 在这里嵌套</span>        net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'block</span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">,</span> block1<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">## nn.container的add_module(name, module)</span>    <span class="token keyword">return</span> netrgnet <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>block2<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>rgnet<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token comment"># (1)一样可以下标调用嵌套内的</span><span class="token keyword">print</span><span class="token punctuation">(</span>rgnet<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>rgnet<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token comment"># (2)通过name调用的方法</span><span class="token keyword">print</span><span class="token punctuation">(</span>rgnet<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>block1<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>参数初始化方法</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">init_normal</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>      <span class="token comment"># 正态分布初始化</span>        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>        <span class="token comment"># 初始化为常数</span>        <span class="token comment"># nn.init.constant_(m.weight, 1)</span>        <span class="token comment"># xavier初始化</span>        <span class="token comment"># nn.init.xavier_uniform_(m.weight)</span>        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>zeros_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_normal<span class="token punctuation">)</span>net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token comment"># 这种怎么自己传入参数呢？gpt给的方法</span><span class="token keyword">def</span> <span class="token function">init_constant_x</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> w<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> w<span class="token punctuation">)</span>        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> b<span class="token punctuation">)</span>net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> init_constant_x<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>匿名函数lambda <a href="https://www.runoob.com/python/python-functions.html">Python 函数 | 菜鸟教程 (runoob.com)</a></p></li><li><p>最后还介绍了一下共享参数，在一些处理类似任务的情况共享层数可以节省记忆</p></li></ul><h3 id="自定义参数"><a href="#自定义参数" class="headerlink" title="自定义参数"></a>自定义参数</h3><ul><li>d2l用linear距离</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyLinear</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_units<span class="token punctuation">,</span> units<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 调用父类初始化</span>        <span class="token comment"># 随机化参数</span>        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>in_units<span class="token punctuation">,</span> units<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>units<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># 加个,使randn传入的参数是个tuple，不然units会成一个标量，在units=1的时候会报错</span>        <span class="token comment"># 这种情况下传入的参数不是张量而是标量了</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>        linear <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data <span class="token comment"># wx+b</span>        <span class="token keyword">return</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>linear<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 我自己写错的地方</span><span class="token comment"># self.data_weight = torch.randn(input_size,output_size)# 这里错了</span>        <span class="token comment"># self.data_bias = torch.randn(output_size)</span>        self<span class="token punctuation">.</span>data_weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span>output_size<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>data_bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>output_size<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># 如果不用nn.parameter，导致后续参数不能优化</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># save</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'mlp.params'</span><span class="token punctuation">)</span> <span class="token comment"># 保存到同目录下mlp.params</span><span class="token comment"># load</span>clone <span class="token operator">=</span> MLP<span class="token punctuation">(</span><span class="token punctuation">)</span>clone<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'mlp.params'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 这里load_state_dict不需要额外定义，是nn.Module父类的函数，在MLP中依旧只需要定义init和forward即可</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h3><ul><li>张量默认创建在cpu上。(用<code>x.device</code>查看tensor x所在的位置)</li><li>多张gpu时，<code>torch.device(f'cuda:{i}')</code></li><li>指定设备</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 模型位置</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>net <span class="token operator">=</span> net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token operator">=</span>try_gpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 张量位置</span>X <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> device<span class="token operator">=</span>try_gpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 移动</span>z <span class="token operator">=</span> x<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h3><ul><li>lenet一章中提供了一种输出在sequential中，不同层间输入输出大小的方法</li></ul><p><a href="https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/lenet.html">6.6. 卷积神经网络（LeNet） — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">X <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token keyword">for</span> layer <span class="token keyword">in</span> net<span class="token punctuation">:</span>    X <span class="token operator">=</span> layer<span class="token punctuation">(</span>X<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>layer<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">,</span><span class="token string">'output shape: \t'</span><span class="token punctuation">,</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> DL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法学习笔记 </tag>
            
            <tag> DL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>李沐d2l学习笔记ML部分</title>
      <link href="2023/09/19/d2l/"/>
      <url>2023/09/19/d2l/</url>
      
        <content type="html"><![CDATA[<h1 id="李沐D2L-ML部分"><a href="#李沐D2L-ML部分" class="headerlink" title="李沐D2L ML部分"></a>李沐D2L ML部分</h1><h2 id="colab-d2l"><a href="#colab-d2l" class="headerlink" title="colab d2l"></a>colab d2l</h2><ul><li>解决colab没有d2l的问题，使用这个比从github下载更快</li></ul><pre class="line-numbers language-none"><code class="language-none">!pip install d2l==0.14.<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>避免一些无意义的复制粘贴，只记录一些体会和关键的点。</p><h2 id="softmax回归，多层感知机"><a href="#softmax回归，多层感知机" class="headerlink" title="softmax回归，多层感知机"></a>softmax回归，多层感知机</h2><p>softmax为了让概率之和为1所引入$ \frac{exp(x_i)}{\sum{exp(x_k)}}$</p><ul><li><p>和sigmoid搞混了，sigmoid是把输入映射到0-1范围的激活函数，softmax是给一组输入，让所有元素之和=1</p></li><li><p>train函数 这个函数后面的章节也一直通用</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># train_iter是一个dataloader</span>train_iter <span class="token operator">=</span> data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>mnist_train<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>                             num_workers<span class="token operator">=</span>get_dataloader_workers<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>之前不理解为什么backward的时候是l.mean.backward()，因为正常用backward的话l得是个标量。还有一种方法是，如果l是向量的话，可以用l.backward(gradient=grad)实现</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_epoch_ch3</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> updater<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>    <span class="token triple-quoted-string string">"""训练模型一个迭代周期（定义见第3章）"""</span>    <span class="token comment"># 将模型设置为训练模式</span>    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 这里是看net是不是nn.Module类</span>        net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># 训练损失总和、训练准确度总和、样本数</span>    metric <span class="token operator">=</span> Accumulator<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span> <span class="token comment"># 计数器</span>    <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span> <span class="token comment"># train iter是</span>        <span class="token comment"># 计算梯度并更新参数</span>        y_hat <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token comment"># 计算交叉熵</span>        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>updater<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Optimizer<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment"># 使用PyTorch内置的优化器和损失函数</span>            updater<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>            l<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 梯度回传</span>            updater<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 优化器更新参数</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token comment"># 使用定制的优化器和损失函数</span>            l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>            updater<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">(</span>l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> accuracy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 统计loss，准确率用于之后算平均</span>    <span class="token comment"># 返回训练损失和训练精度</span>    <span class="token keyword">return</span> metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_ch3</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> updater<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment">#@save</span>    <span class="token triple-quoted-string string">"""训练模型（定义见第3章）"""</span>    animator <span class="token operator">=</span> Animator<span class="token punctuation">(</span>xlabel<span class="token operator">=</span><span class="token string">'epoch'</span><span class="token punctuation">,</span> xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span> ylim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0.9</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                        legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'train loss'</span><span class="token punctuation">,</span> <span class="token string">'train acc'</span><span class="token punctuation">,</span> <span class="token string">'test acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        train_metrics <span class="token operator">=</span> train_epoch_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> updater<span class="token punctuation">)</span>        test_acc <span class="token operator">=</span> evaluate_accuracy<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">)</span>        animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> train_metrics <span class="token operator">+</span> <span class="token punctuation">(</span>test_acc<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    train_loss<span class="token punctuation">,</span> train_acc <span class="token operator">=</span> train_metrics    <span class="token keyword">assert</span> train_loss <span class="token operator">&lt;</span> <span class="token number">0.5</span><span class="token punctuation">,</span> train_loss    <span class="token keyword">assert</span> train_acc <span class="token operator">&lt;=</span> <span class="token number">1</span> <span class="token keyword">and</span> train_acc <span class="token operator">&gt;</span> <span class="token number">0.7</span><span class="token punctuation">,</span> train_acc    <span class="token keyword">assert</span> test_acc <span class="token operator">&lt;=</span> <span class="token number">1</span> <span class="token keyword">and</span> test_acc <span class="token operator">&gt;</span> <span class="token number">0.7</span><span class="token punctuation">,</span> test_acc<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>可以使用net.apply(f())，来用f对net的参数进行初始化</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>bias可以用zeros初始化，因为主要作用是对网络输出进行平移</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">num_inputs<span class="token punctuation">,</span> num_outputs<span class="token punctuation">,</span> num_hiddens <span class="token operator">=</span> <span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">256</span>W1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>    num_inputs<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span><span class="token punctuation">)</span>b1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_hiddens<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>W2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>    num_hiddens<span class="token punctuation">,</span> num_outputs<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span><span class="token punctuation">)</span>b2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>num_outputs<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>params <span class="token operator">=</span> <span class="token punctuation">[</span>W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>多层感知机中ReLU的引入，是因为如果不引入激活函数，两个Linear先后加在一起和一个Linear的效果是一样的。（线性模型已经能够表示任何仿射函数）</li></ul><h2 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h2><p>$L(\mathbf{w}, b) + \frac{\lambda}{2} |\mathbf{w}|^2$</p><ul><li>计算w的L2范式，这样希望w可以尽量简单</li><li>关于为什么不开根，因为导数好算很多</li><li>对大的wi施加更大的惩罚， 使单个特征的误差影响更小</li><li>避免过拟合</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">l2_penalty</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span><span class="token comment">#使用例</span><span class="token comment"># 广播机制使l2_penalty(w)成为一个长度为batch_size的向量</span>l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token operator">+</span> lambd <span class="token operator">*</span> l2_penalty<span class="token punctuation">(</span>w<span class="token punctuation">)</span>l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>d2l<span class="token punctuation">.</span>sgd<span class="token punctuation">(</span><span class="token punctuation">[</span>w<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 简洁实现</span>trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">"params"</span><span class="token punctuation">:</span>net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">,</span><span class="token string">'weight_decay'</span><span class="token punctuation">:</span> wd<span class="token punctuation">}</span><span class="token punctuation">,</span><span class="token punctuation">{</span><span class="token string">"params"</span><span class="token punctuation">:</span>net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">}</span><span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span><span class="token comment"># 对w设置权重衰减，b不设</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Dropout暂退法"><a href="#Dropout暂退法" class="headerlink" title="Dropout暂退法"></a>Dropout暂退法</h2><ul><li>即使我们有比特征多得多的样本，深度神经网络也有可能过拟合。神经网络过拟合与每一层都依赖于前一层激活值相关，称这种情况为“共适应性”。 作者认为，暂退法会破坏共适应性，就像有性生殖会破坏共适应的基因一样。（好玄学）</li><li>被关闭的神经元不计算loss</li><li>d2l中的活性值，就是经过Linear和激活函数之后的输出</li></ul><h2 id="forward-and-backward"><a href="#forward-and-backward" class="headerlink" title="forward and backward"></a>forward and backward</h2><p><a href="https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/backprop.html">4.7. 前向传播、反向传播和计算图 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p><ul><li>里面的图很精炼</li></ul><p><img src="https://zh-v2.d2l.ai/_images/forward.svg"></p><ul><li>训练比计算要多用GPU显存，因为要存中间值。torch.require_grad需要自动计算梯度，torch.no_grad一般在验证集，测试集使用，省计算。</li></ul><h2 id="数值稳定性和初始化"><a href="#数值稳定性和初始化" class="headerlink" title="数值稳定性和初始化"></a>数值稳定性和初始化</h2><ul><li><p>d2l用了<code>torch.mm</code>来矩阵相乘，但其实这个落后了，应使用<code>torch.matmul</code> 函数支持更广泛的操作。</p></li><li><p>梯度爆炸，对称性，梯度消失概念和原因<a href="https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#id5">4.8. 数值稳定性和模型初始化 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a>，对称性这个概念还是第一次听，就是两个神经元中间计算的梯度之类的都一样，导致两个直接对称了，可以用dropout打破这种对称。</p></li><li><p>上面这些问题，之前都一直使用正态分布初始化参数，这章引入了Xvaier初始化</p></li></ul><h2 id="实战kaggle房价预测"><a href="#实战kaggle房价预测" class="headerlink" title="实战kaggle房价预测"></a>实战kaggle房价预测</h2><h3 id="处理数据集"><a href="#处理数据集" class="headerlink" title="处理数据集"></a>处理数据集</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">n_train <span class="token operator">=</span> train_data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>train_features <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>all_features<span class="token punctuation">[</span><span class="token punctuation">:</span>n_train<span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>test_features <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>all_features<span class="token punctuation">[</span>n_train<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>values<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>train_labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>    train_data<span class="token punctuation">.</span>SalePrice<span class="token punctuation">.</span>values<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>调用df.values取df的value，不然返回的类型是df，报错<code>ValueError: could not determine the shape of object type 'DataFrame'</code></li><li>默认是float64，所以加一句<code>dtype=torch.float32</code>，大多数 GPU 只支持 32 位浮点数计算。</li></ul><h3 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">log_rmse</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 为了在取对数时进一步稳定该值，将小于1的值设置为1</span>    clipped_preds <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>net<span class="token punctuation">(</span>features<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">'inf'</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 截断到1以上</span>    rmse <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>loss<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>clipped_preds<span class="token punctuation">)</span><span class="token punctuation">,</span>                           torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> rmse<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>rmse的type是tensor，所以用item返回一个python的浮点数</li><li>我的疑问是，进行了这种截断后，还能计算梯度吗？gpt回答说可以，而且回传计算梯度的时候，y代入的是截断前的y值。</li><li>clamp这个函数的参数</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">y_clamped <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment"># y截断到1，2之间</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="使用torch的dataloader读数据"><a href="#使用torch的dataloader读数据" class="headerlink" title="使用torch的dataloader读数据"></a>使用torch的dataloader读数据</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 引入dataloader之类的类</span><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils <span class="token keyword">import</span> data<span class="token keyword">def</span> <span class="token function">my_load_array</span><span class="token punctuation">(</span>data_arrays<span class="token punctuation">,</span>batch_size<span class="token punctuation">,</span>is_train <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 代替d2l.load_array</span>    dataset <span class="token operator">=</span> data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span><span class="token operator">*</span>data_arrays<span class="token punctuation">)</span> <span class="token comment"># 加星号表示将列表解开入参！而不是只传入一个张量</span>    <span class="token keyword">return</span> data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span>batch_size<span class="token punctuation">,</span>shuffle<span class="token operator">=</span>is_train<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>关于星号的解释<a href="https://zhuanlan.zhihu.com/p/616180238">手动学深度学习|3.3章：data.TensorDataset(*) - 知乎 (zhihu.com)</a></li></ul><h3 id="可能是沐神的失误？"><a href="#可能是沐神的失误？" class="headerlink" title="可能是沐神的失误？"></a>可能是沐神的失误？</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> X<span class="token punctuation">,</span>y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 梯度清零</span>        <span class="token comment"># 沐神的代码，不知道为什么这里不用相对误差yhat/y，我改了一下这部分代码，使用之前写好的rmseloss</span>        <span class="token comment"># y_pred = net(X)</span>        <span class="token comment"># ls = loss(y_pred,y) </span>        <span class="token comment"># 修改的部分</span>        ls <span class="token operator">=</span> log_rmse<span class="token punctuation">(</span>net<span class="token punctuation">,</span>X<span class="token punctuation">,</span>y<span class="token punctuation">)</span> <span class="token comment"># 使用相对误差</span>        ls<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 反向传播</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 更新参数</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>这段不知道为什么不用说好了的开根的loss来计算误差。</li><li>修改，注意修改后需要改变rmse的返回值，原本是返回item，会报错</li></ul><pre class="line-numbers language-none"><code class="language-none">AttributeError: 'float' object has no attribute 'backward'<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>需要直接返回tensor，才好包含计算梯度所需的，这样以来统计loss的地方再使用item()得数值即可。</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">log_rmse</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span>features<span class="token punctuation">,</span>labels<span class="token punctuation">)</span><span class="token punctuation">:</span>    pred <span class="token operator">=</span> net<span class="token punctuation">(</span>features<span class="token punctuation">)</span>    clamped_pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>pred<span class="token punctuation">,</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    rmse <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>loss<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>clamped_pred<span class="token punctuation">)</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># 整组传到里面去计算mseloss</span>    <span class="token keyword">return</span> rmse<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>不过最后效果完全不如沐神原本的，逆天</li></ul><h3 id="标程的小问题"><a href="#标程的小问题" class="headerlink" title="标程的小问题"></a>标程的小问题</h3><ul><li><p>d2l中的标程也应做相应修改才能正常执行</p></li><li><p>报错：all_features经过处理后类型不符合，告知不能转成tensor，float32</p></li><li><p>答：某些ide独热编码时会将数据处理成boolean类型，需要指定处理类型为数值型，如int。尝试这么处理，pd.get_dummies(all_features, dummy_na=True, dtype=int) </p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">all_features <span class="token operator">=</span> pd<span class="token punctuation">.</span>get_dummies<span class="token punctuation">(</span>all_features<span class="token punctuation">,</span>dummy_na<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="调参过程"><a href="#调参过程" class="headerlink" title="调参过程"></a>调参过程</h3><ul><li>先使用沐神原版的网络结构</li></ul><pre class="line-numbers language-none"><code class="language-none">折1，训练log rmse0.170212, 验证log rmse0.156864折2，训练log rmse0.162003, 验证log rmse0.188812折3，训练log rmse0.163810, 验证log rmse0.168171折4，训练log rmse0.167946, 验证log rmse0.154694折5，训练log rmse0.163320, 验证log rmse0.1829285-折验证: 平均训练log rmse: 0.165458, 平均验证log rmse: 0.170293<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>然后增加了网络层层数，很明显是过拟合的数据</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_net</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span>output_size<span class="token punctuation">)</span>    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-none"><code class="language-none">折1，训练log rmse0.123148, 验证log rmse0.263349折2，训练log rmse0.119781, 验证log rmse0.244233折3，训练log rmse0.120550, 验证log rmse0.208438折4，训练log rmse0.179972, 验证log rmse0.332736折5，训练log rmse0.156284, 验证log rmse0.2003505-折验证: 平均训练log rmse: 0.139947, 平均验证log rmse: 0.249821<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>后面查看曲线，感觉这个可能是lr设置的太大了的原因</li></ul><p><img src="/img/d2l/train1.jpg" alt="train1"></p><ul><li>考虑减少lr到1</li></ul><p><img src="/img/d2l/overfitting.jpg" alt="overfitting"></p><pre class="line-numbers language-none"><code class="language-none">折1，训练log rmse0.080356, 验证log rmse0.175418折2，训练log rmse0.056644, 验证log rmse0.179348折3，训练log rmse0.120459, 验证log rmse0.193579折4，训练log rmse0.101917, 验证log rmse0.149796折5，训练log rmse0.177175, 验证log rmse0.2412515-折验证: 平均训练log rmse: 0.107310, 平均验证log rmse: 0.187878<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>很明显的overfitting，增加一个dropout</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_net</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span>output_size<span class="token punctuation">)</span>    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>发现效果还是不理想，可能因为我增加了dropout，但是却没有设置在valid的时候关dropout，修改代码如下：原来valid的效果差并不是我还overfitting，只是我忘了关dropout</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>      net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 开dropout</span>      <span class="token keyword">for</span> X<span class="token punctuation">,</span>y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>          optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 梯度清零</span>          <span class="token comment"># 沐神的代码，不知道为什么这里不用相对误差yhat/y，我改了一下这部分代码，使用之前写好的rmseloss</span>          y_pred <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>          ls <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span>y<span class="token punctuation">)</span>           <span class="token comment"># 修改的部分</span>          <span class="token comment"># ls = log_rmse(net,X,y) # 使用相对误差</span>          ls<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 反向传播</span>          optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 更新参数</span>            <span class="token comment"># 记录一下loss</span>      net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>      train_ls<span class="token punctuation">.</span>append<span class="token punctuation">(</span>log_rmse<span class="token punctuation">(</span>net<span class="token punctuation">,</span>train_features<span class="token punctuation">,</span>train_labels<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 因为前面修改了函数，所以这里多加一个item</span>      <span class="token keyword">if</span> test_labels <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>          net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 关dropout</span>          test_ls<span class="token punctuation">.</span>append<span class="token punctuation">(</span>log_rmse<span class="token punctuation">(</span>net<span class="token punctuation">,</span>test_features<span class="token punctuation">,</span>test_labels<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token keyword">return</span> train_ls<span class="token punctuation">,</span>test_ls<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>对比一下先后的曲线吧，发现确实是这个原因造成的，在valid上面跑的从0.18到0.15的区别。</li></ul><p><img src="/img/d2l/dropout.jpg" alt="dropout"></p><p><img src="/img/d2l/dropout2.jpg" alt="dropout2"></p><ul><li>后面想想还是不要费时间在调参了</li></ul><h3 id="上传kaggle"><a href="#上传kaggle" class="headerlink" title="上传kaggle"></a>上传kaggle</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 将网络应用于测试集。</span>preds <span class="token operator">=</span> net<span class="token punctuation">(</span>test_features<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>这一部分的detach作用和item类似，只不过detach应用于向量，item应用于标量</li></ul><p><img src="/img/d2l/lb.jpg" alt="lb"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>感觉python还是需要再学一下</li><li>关于如何解决过拟合，使用了ReLU+Dropout的多层感知机也还是不太行</li><li>不过至少知道了如果loss后面不稳定的话，可以适当调低lr就是了</li></ul>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法学习笔记 </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文本QA数据爬取</title>
      <link href="2023/08/11/agri/"/>
      <url>2023/08/11/agri/</url>
      
        <content type="html"><![CDATA[<h1 id="主要任务"><a href="#主要任务" class="headerlink" title="主要任务"></a>主要任务</h1><h2 id="文本对话"><a href="#文本对话" class="headerlink" title="文本对话"></a>文本对话</h2><p>方案A：问答形式数据对chatglm-6B进行微调</p><p>方案B：基于本地知识库</p><h2 id="爬虫收集农业知识库"><a href="#爬虫收集农业知识库" class="headerlink" title="爬虫收集农业知识库"></a>爬虫收集农业知识库</h2><ul><li><p>对应方案A需要问答形式的数据，B的话需要文本形式，可以从论文或者网页爬虫中获取。</p></li><li><p>针对爬取结果设置过滤词，使不要爬到无关词条</p></li></ul><p><img src="/img/agri/filter.png" alt="filter"></p><h1 id="8月10日及之前"><a href="#8月10日及之前" class="headerlink" title="8月10日及之前"></a>8月10日及之前</h1><h2 id="已完成的"><a href="#已完成的" class="headerlink" title="已完成的"></a>已完成的</h2><ul><li><p>部署langchain+chatglm基于本地知识库问答 到autodl云服务器，并且导入少量农业知识数据进行测试</p></li><li><p>导入txt文件时报错，查看github issue发现是因为文档中有无法识别的字符，用程序去除</p></li><li><p>运行开源中文wiki爬虫代码</p></li></ul><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul><li>运行开源的中文维基爬虫bug很多</li></ul><p><a href="https://github.com/wjn1996/scrapy_for_zh_wiki">wjn1996/scrapy_for_zh_wiki: 基于scrapy的层次优先队列方法爬取中文维基百科，并自动抽取结构和半结构数据 (github.com)</a></p><ul><li>scrapy使用代理访问外网</li></ul><h1 id="8月11日"><a href="#8月11日" class="headerlink" title="8月11日"></a>8月11日</h1><h2 id="尝试继续跑通爬虫代码"><a href="#尝试继续跑通爬虫代码" class="headerlink" title="尝试继续跑通爬虫代码"></a>尝试继续跑通爬虫代码</h2><p>云服务器安装了正确版本的库，代码不再报错，但是出现tcp connection timeout<img src="/img/agri/tcp.png" alt="tcp"></p><p>本地端则是能够获取到wiki的category分类页面，但是通过调试发现获取分类页面内容的url似乎有问题，查看list为空。</p><p><img src="/img/agri/emtpy.png" alt="emtpy"></p><p>非常奇怪，考虑了一下可能是下面两种原因：</p><ul><li>怀疑是否是代理的原因？我使用的是clash的tun模式，看到网上一些帖子说要额外写一个scrapy的中间件middleware。</li><li>zh-wiki网页的代码发生变化，需要重新找方法获取分类页面下面的url？</li></ul><h2 id="替代方案：对ecnu爬取好的数据进行处理，使得符合我们需要的格式"><a href="#替代方案：对ecnu爬取好的数据进行处理，使得符合我们需要的格式" class="headerlink" title="替代方案：对ecnu爬取好的数据进行处理，使得符合我们需要的格式"></a>替代方案：对ecnu爬取好的数据进行处理，使得符合我们需要的格式</h2><p>ecnu爬取的数据格式不同，而且存在很多不相关的词条</p><p><img src="/img/agri/ecnu.png" alt="ecnu"></p><p>编写脚本，实现如下功能</p><ul><li>取引号内</li><li>根据设定的列表 filter_words 删去没必要的词条</li><li>重复爬取的词条进行去重</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> re<span class="token keyword">import</span> syssys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"."</span><span class="token punctuation">)</span><span class="token keyword">from</span> filter_words <span class="token keyword">import</span> filters<span class="token keyword">def</span> <span class="token function">contains_filtered_word</span><span class="token punctuation">(</span>line<span class="token punctuation">,</span> filter_list<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> word <span class="token keyword">in</span> filter_list<span class="token punctuation">:</span>        <span class="token keyword">if</span> word <span class="token keyword">in</span> line<span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token boolean">True</span>    <span class="token keyword">return</span> <span class="token boolean">False</span>inpt <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"train_data.txt"</span><span class="token punctuation">,</span><span class="token string">"r"</span><span class="token punctuation">,</span>encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span>outpt <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"outpt.txt"</span><span class="token punctuation">,</span><span class="token string">"w"</span><span class="token punctuation">,</span>encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token comment"># 按行读入</span>cleaned_lines <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>lines <span class="token operator">=</span> inpt<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">baoliu</span><span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 找开头的引号</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#倒着遍历</span>        <span class="token keyword">if</span> line<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'"'</span><span class="token punctuation">:</span>            ed <span class="token operator">=</span> i            <span class="token keyword">break</span>    <span class="token comment"># 找结尾的引号</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>line<span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> line<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'"'</span><span class="token punctuation">:</span>            st <span class="token operator">=</span> i            <span class="token keyword">break</span>    <span class="token comment"># 取st和ed中的子串</span>    <span class="token comment"># return line[st:ed+1]</span>    <span class="token keyword">return</span> line<span class="token punctuation">[</span>st<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">:</span>ed<span class="token punctuation">]</span><span class="token comment"># 清洗</span>cleaned_lines <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> line <span class="token keyword">in</span> lines<span class="token punctuation">:</span>    <span class="token comment"># match = re.search(r'"([^"]+)"', line) # 会有bug，找最外层的双引号</span>    <span class="token comment"># match = re.findall(r'"([^"]+)"', line)</span>    match <span class="token operator">=</span> baoliu<span class="token punctuation">(</span>line<span class="token punctuation">)</span>    <span class="token keyword">if</span> contains_filtered_word<span class="token punctuation">(</span>line<span class="token punctuation">,</span>filters<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 接触到屏蔽词 跳过</span>        <span class="token keyword">continue</span>    <span class="token keyword">if</span> match<span class="token punctuation">:</span>        <span class="token keyword">if</span> match<span class="token operator">+</span><span class="token string">'\n'</span> <span class="token keyword">not</span> <span class="token keyword">in</span> cleaned_lines<span class="token punctuation">:</span><span class="token comment"># 去重</span>            cleaned_lines<span class="token punctuation">.</span>append<span class="token punctuation">(</span>match<span class="token operator">+</span><span class="token string">'\n'</span><span class="token punctuation">)</span>outpt<span class="token punctuation">.</span>writelines<span class="token punctuation">(</span>cleaned_lines<span class="token punctuation">)</span>outpt<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>inpt<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>清洗后剩余六千多条记录，后续考虑导入知识库中</p><p><img src="/img/agri/clean.png" alt="clean"></p><h1 id="8月13日"><a href="#8月13日" class="headerlink" title="8月13日"></a>8月13日</h1><h2 id="处理问答数据"><a href="#处理问答数据" class="headerlink" title="处理问答数据"></a>处理问答数据</h2><p>获得了格式如下的问答数据，编写python程序处理。</p><p>给出的问答同样还有不同格式的也通过类似方法进行处理。</p><p><img src="/img/agri/data.jpg" alt="data"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> re<span class="token comment"># 读入</span><span class="token keyword">def</span> <span class="token function">process_input</span><span class="token punctuation">(</span>input_str<span class="token punctuation">)</span><span class="token punctuation">:</span>    match <span class="token operator">=</span> re<span class="token punctuation">.</span>match<span class="token punctuation">(</span><span class="token string">r"(\d+)\s*、\s*(.*?)\s*答：(.*)"</span><span class="token punctuation">,</span> input_str<span class="token punctuation">)</span>    <span class="token keyword">if</span> match<span class="token punctuation">:</span>        number <span class="token operator">=</span> match<span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>        question <span class="token operator">=</span> match<span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>        answer <span class="token operator">=</span> match<span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">{</span>            <span class="token string">"img"</span><span class="token punctuation">:</span> <span class="token boolean">None</span><span class="token punctuation">,</span>            <span class="token string">"prompt"</span><span class="token punctuation">:</span> question<span class="token punctuation">,</span>            <span class="token string">"label"</span><span class="token punctuation">:</span> answer<span class="token punctuation">,</span>        <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token boolean">None</span><span class="token comment"># 开始处理</span>input_file <span class="token operator">=</span> <span class="token string">"input.txt"</span>processed_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>tmp <span class="token operator">=</span> <span class="token string">""</span><span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>input_file<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> <span class="token builtin">file</span><span class="token punctuation">:</span>    flag <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> line <span class="token keyword">in</span> <span class="token builtin">file</span><span class="token punctuation">:</span>        <span class="token comment"># 调试用</span>        <span class="token comment"># print(line[0])</span>        <span class="token comment"># print(line[0].isdigit)</span>        <span class="token keyword">if</span> line<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">&lt;=</span><span class="token string">'9'</span> <span class="token keyword">and</span> line<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">&gt;=</span><span class="token string">'0'</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> flag<span class="token punctuation">:</span>                processed_entry <span class="token operator">=</span> process_input<span class="token punctuation">(</span>tmp<span class="token punctuation">)</span>                <span class="token keyword">if</span> processed_entry<span class="token punctuation">:</span>                    processed_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>processed_entry<span class="token punctuation">)</span>                tmp <span class="token operator">=</span> <span class="token string">""</span>            tmp <span class="token operator">=</span> tmp <span class="token operator">+</span> line            flag <span class="token operator">=</span> <span class="token number">1</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            linep <span class="token operator">=</span> line<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">)</span>            tmp <span class="token operator">=</span> tmp <span class="token operator">+</span> linep    <span class="token comment">#结束后还有最后一个</span>    processed_entry <span class="token operator">=</span> process_input<span class="token punctuation">(</span>tmp<span class="token punctuation">)</span>    <span class="token keyword">if</span> processed_entry<span class="token punctuation">:</span>        processed_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>processed_entry<span class="token punctuation">)</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>processed_data<span class="token punctuation">)</span>df<span class="token punctuation">.</span>to_json<span class="token punctuation">(</span><span class="token string">"data.json"</span><span class="token punctuation">,</span> orient<span class="token operator">=</span><span class="token string">"records"</span><span class="token punctuation">,</span>force_ascii<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>indent<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"JSON文件已保存"</span><span class="token punctuation">)</span><span class="token comment">#input来源与百度文库</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>将不同问答文件合成后输出到json中去，得到格式如下的json文件</p><pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">{</span>    <span class="token property">"img"</span><span class="token operator">:</span><span class="token null keyword">null</span><span class="token punctuation">,</span>    <span class="token property">"prompt"</span><span class="token operator">:</span><span class="token string">"玉米实行宽窄行种植有哪些优点？"</span><span class="token punctuation">,</span>    <span class="token property">"label"</span><span class="token operator">:</span><span class="token string">"①改善了通风透光条件，发挥边行优势。     ②可以适当增加密度。     ③易于在宽行中间套种矮秆作物，提高经济效益。"</span><span class="token punctuation">}</span><span class="token punctuation">,</span><span class="token punctuation">{</span>    <span class="token property">"img"</span><span class="token operator">:</span><span class="token null keyword">null</span><span class="token punctuation">,</span>    <span class="token property">"prompt"</span><span class="token operator">:</span><span class="token string">"玉米高产栽培中，为什么提倡抽雄前10天左右追肥浇水？"</span><span class="token punctuation">,</span>    <span class="token property">"label"</span><span class="token operator">:</span><span class="token string">"这段时期是玉米雌、雄穗的小穗小花分化期，也是菅养生长和生殖生长并进旺盛期，是玉米一生需肥水最多的关键期。因此，这时追肥浇水可促进穗分化，协调菅养生长和生殖生长，争取穗大、粒多、达、到高产。"</span><span class="token punctuation">}</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Classification</title>
      <link href="2023/05/23/classif/"/>
      <url>2023/05/23/classif/</url>
      
        <content type="html"><![CDATA[<h1 id="ML-Classification"><a href="#ML-Classification" class="headerlink" title="[ML] Classification"></a>[ML] Classification</h1><p>参考了<a href="https://blog.lzlz.site/2019/06/08/hexo-%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87/">hexo+typora 插入图片的简便解决方案 - lzlz000</a>的方法，在markdown的开头添加了</p><pre class="line-numbers language-none"><code class="language-none">typora-root-url: ../<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>可以解决在本地编辑是无法预览图片的问题。</p><h2 id="为什么不能用regression的方法解决分类问题"><a href="#为什么不能用regression的方法解决分类问题" class="headerlink" title="为什么不能用regression的方法解决分类问题"></a>为什么不能用regression的方法解决分类问题</h2><p><img src="/img/classif/reg.png" alt="reg"></p><p>如图，使用regression方法时，接近1记为class1，接近-1记为class2，这种情况下计算损失函数时，如果出现图2中的情况，则远处的那一块蓝色的会有很大的损失，即会惩罚那些output值太大的点。这时候模型会把原本优秀的绿色线调整为这条蓝色的，因为这条线在计算loss时对regression来说是更好的function（loss更小）。</p><ul><li>简而言之：regression对model好坏的定义不适合用在classification</li></ul><p>而且在多种class的时候，如果说把1当作class1，2当作class2，3当作class3，显然是不可以的，因为这样class1和class2之间，class2和class3这些相邻组之间存在一定关系，但实际情况上可能不存在这样的关系。</p><h2 id="应该如何实现"><a href="#应该如何实现" class="headerlink" title="应该如何实现"></a>应该如何实现</h2><p>在fxgx嵌套，使用g得出一个值，然后f判断分类</p><p><img src="/img/classif/qt.png" alt="实现"></p><p>我们知道之前regression中间，优化function的方法是<strong>gradient decent</strong>，但是显然这个loss函数没法微分，所以这里需要用到如：<strong>SVM、Perceptron</strong> （老师说之后讲）的优化方法。</p><h2 id="G"><a href="#G" class="headerlink" title="G"></a>G</h2><h3 id="实现二元分类为例"><a href="#实现二元分类为例" class="headerlink" title="实现二元分类为例"></a>实现二元分类为例</h3><p><img src="/img/classif/c.png" alt="c"></p><p>算x是属于class的概率计算公式如图，要实现概率的计算，需要用trainning data估测出$P(C_1)P(C_2)P(x|C_1)P(x|C_2)$的这四个值。（选C1的概率，选C2的概率，在C1中抽出x的概率，在C2中抽出x的概率。</p><p>这样的模型叫做<strong>生成模型</strong>generative model，即可以自己产生一个x，因为可以计算某个x出现的几率$P(x) = P(x|C_1)\cdot P(C_1) + P(x|C_2)\cdot P(C_2)$即在C1中挑出的几率和C2中挑出的几率。</p><h3 id="Prior"><a href="#Prior" class="headerlink" title="Prior"></a>Prior</h3><p>$P(C_1)andP(C_2)$这两个概率叫prior。</p><p>比如class1有79个，class2有61个，则容易解$P(C_1) = 79/(79+61)$</p><h3 id="Probability-from-Class"><a href="#Probability-from-Class" class="headerlink" title="Probability from Class"></a>Probability from Class</h3><p>计算$P(x|C_1)$即从C1中挑一个样本出来，这个样本正好是x的概率</p><p><img src="/img/classif/hg.png" alt="1"></p><p><img src="/img/classif/hg2.png" alt="2"></p><p>老师举出的例子中海龟是一个不在集合里面的样本，虽然海龟不在水系神奇宝贝的样本中，但是显然海龟属于水系神奇宝贝的概率并不为0，假设水系神奇宝贝的防御和sp服从一个正态分布gaussian distribution，则可以通过这个正态分布计算出海龟属于水系神奇宝贝的概率。</p><p>正态分布由两个参数决定，一个是mean$\mu$和一个矩阵convariance matrix$\sum$。</p><p><img src="/img/classif/gauss.png" alt="gauss"></p>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法学习笔记 </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch tutorial and Regression</title>
      <link href="2023/05/12/20235/"/>
      <url>2023/05/12/20235/</url>
      
        <content type="html"><![CDATA[<h1 id="ML-Pytorch-tutorial-and-Regression"><a href="#ML-Pytorch-tutorial-and-Regression" class="headerlink" title="[ML] Pytorch tutorial and Regression"></a>[ML] Pytorch tutorial and Regression</h1><p>时隔两年重新开始写博客，毕设基本结束，现在先开始跟着李宏毅的课程，系统学习一下。</p><h2 id="Pytorch-Tutorial"><a href="#Pytorch-Tutorial" class="headerlink" title="Pytorch Tutorial"></a>Pytorch Tutorial</h2><h3 id="数据读入"><a href="#数据读入" class="headerlink" title="数据读入"></a>数据读入</h3><p>dataset和dataloder，dataset读入（stores data samles），dataloder使用（groups data in batches）</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token punctuation">,</span>DataLoaderdataset <span class="token operator">=</span> MyDataset<span class="token punctuation">(</span><span class="token builtin">file</span><span class="token punctuation">)</span>dataloder <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span>batch_size<span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment">#train的时候开shuffle，test的时候关</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>dataloader每次按序（shuffle=false的情况）从dataset中取出batch_size个变成一个batch。</p><p>关于为什么要shuffle的说明<a href="https://zhuanlan.zhihu.com/p/57108650">数据集shuffle的重要性 - 知乎 (zhihu.com)</a></p><h3 id="定义dataset"><a href="#定义dataset" class="headerlink" title="定义dataset"></a>定义dataset</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 定义MyDatasetfrom torch.utils.data import Dataset,Dataloader</span><span class="token keyword">class</span> <span class="token class-name">MyDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment">#初始化，对data处理</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span><span class="token builtin">file</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token operator">=</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    <span class="token comment">#返回单个</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>index<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>index<span class="token punctuation">]</span>    <span class="token comment">#获取dataset长度</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h3><p>Tensor是高维度的矩阵，是信息读入后存在Pytorch中的形式。</p><p>dim in Pytorch == axis in NumPy\</p><p>一些常用操作</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 用shape看大小</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># list to tensor</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># numpy to tensor</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># zeros和ones</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># generate a 2*2*2 3dim tensor filled with 0</span><span class="token comment"># rand</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># generate a 2*2*2 random 3dim tensor</span><span class="token comment"># 常见运算+-，sum(),mean(),pow()</span>y <span class="token operator">=</span> x<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token comment"># transpose，指定两个dimension互换，如2*3变3*2</span>y <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># change dim0 and dim1</span><span class="token comment"># squeeze，拿掉长度为1的dimention，如</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment">#generate a 1*2*3 tensor</span>x <span class="token operator">=</span> x<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment">#squeeze to an 2*3 tensor</span>x <span class="token operator">=</span> x<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># unsqueeze to an 2*1*3 tensor</span><span class="token comment"># cat，把多个tensor连接在一起</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>z <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>w <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">,</span>y<span class="token punctuation">,</span>z<span class="token punctuation">]</span><span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token comment"># 沿dim1方向连接</span><span class="token comment"># torch size [2,6,3]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Tensor-Datatype"><a href="#Tensor-Datatype" class="headerlink" title="Tensor - Datatype"></a>Tensor - Datatype</h4><pre class="line-numbers language-none"><code class="language-none"># 32bit floattorch.floattorch.FloatTensor# 64bit integertorch.longtorch.LongTensor#use dtype to check the datatypex.dtype<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Tensor-Device"><a href="#Tensor-Device" class="headerlink" title="Tensor - Device"></a>Tensor - Device</h4><p>Tensor will be computed with CPU by default.</p><p>use to() to move tensor to appropriate device like:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span><span class="token comment"># to gpu</span>x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">)</span><span class="token comment"># multi gpu</span>x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span> <span class="token comment">#cuda:1,cuda:2.....</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Tensor-Gradient"><a href="#Tensor-Gradient" class="headerlink" title="Tensor - Gradient"></a>Tensor - Gradient</h4><p>使用pytorch提供的函数求梯度</p><p>z.backward求z微分，用x.grad看结果</p><p><img src="/img/20235/gradient.png" alt="gradient.png"></p><h3 id="训练前"><a href="#训练前" class="headerlink" title="训练前"></a>训练前</h3><p>进入训练之前三件事情，定好layers，损失函数loss function，optimization algorithm最佳化演算法</p><h4 id="Linear-Layer（fully-connected-layer）"><a href="#Linear-Layer（fully-connected-layer）" class="headerlink" title="Linear Layer（fully connected layer）"></a>Linear Layer（fully connected layer）</h4><p>linear layer包含一组bias和一组weight</p><p><img src="/img/20235/ll.png" alt="gradient.png"></p><p>如图即让(?,32)大小的输入x乘上(32,64)的W，再加上b。先做一次矩阵乘法再做一次加法。$y = W\cdot x+b$</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">layer <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">)</span>layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>shapelayer<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>shape<span class="token comment"># 可以用weight和bias.shape看linear层中w和b参数的大小</span><span class="token comment"># 参数</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_feature<span class="token punctuation">,</span>out_feature<span class="token punctuation">,</span>bias<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>两种定义网络结构的方法</p><p>关于<code>super(Mymodel,self).__init__()</code>这句简言之就是对继承自父类nn.Module的属性进行初始化。super()是用来用于调用父类的一个方法。这里参考了<a href="https://blog.csdn.net/dongjinkun/article/details/114575998">(解惑（一） —– super(XXX, self).<strong>init</strong>()到底是代表什么含义_self.fc3是_底层研究生的博客-CSDN博客</a></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token comment"># 再自己定义网络时，需要继承nn.Module类，并且重新实现构造函数Init和forwawrd两个方法</span><span class="token keyword">class</span> <span class="token class-name">MyModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 第一种方法，使用序列容器Sequential</span><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token builtin">super</span><span class="token punctuation">(</span>Mymodel<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span> <span class="token comment"># sequential是pytorch的一个序列容器</span>    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 前向传播</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>net<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">MyModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 第二种方法，再forward中相继调用</span><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token builtin">super</span><span class="token punctuation">(</span>Mymodel<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>layer1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>layer2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>layer3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 前向传播</span>    out <span class="token operator">=</span> self<span class="token punctuation">.</span>layer1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    out <span class="token operator">=</span> self<span class="token punctuation">.</span>layer2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>    out <span class="token operator">=</span> self<span class="token punctuation">.</span>layer3<span class="token punctuation">(</span>out<span class="token punctuation">)</span>    <span class="token keyword">return</span> out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h4><p>最佳化演算法即选择调整model参数的方法，比如SGD演算法，在torch.optim这个library中。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token punctuation">,</span>momentum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>再定好最佳化演算法后，在每个batch都需要有这三步。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 把上一步的gradient归零</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 计算loss的gradient</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 利用gradient调整model的参数</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="完整训练过程"><a href="#完整训练过程" class="headerlink" title="完整训练过程"></a>完整训练过程</h3><p>关于Training，validation，testing的区别<a href="https://zhuanlan.zhihu.com/p/35394638">训练集(train)验证集(validation)测试集(test)与交叉验证法 - 知乎 (zhihu.com)</a></p><h4 id="Training-Loop训练"><a href="#Training-Loop训练" class="headerlink" title="Training Loop训练"></a>Training Loop训练</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">dataset <span class="token operator">=</span> Mydataset<span class="token punctuation">(</span><span class="token builtin">file</span><span class="token punctuation">)</span> <span class="token comment">#read data</span>tr_set <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span><span class="token number">16</span><span class="token punctuation">,</span>shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># put dataset into dataloader</span>model <span class="token operator">=</span> MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token comment"># device 可以是 cpu/cuda</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#选择损失函数</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token punctuation">,</span>momentum <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment">#选择optimizer</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>  model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 这句是把model设置成training mode</span>  <span class="token keyword">for</span> x<span class="token punctuation">,</span>y <span class="token keyword">in</span> tr_set<span class="token punctuation">:</span>    optimizer<span class="token punctuation">.</span>zerograd<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 1.把上一步的gradient归零</span>    x<span class="token punctuation">,</span>y <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token punctuation">,</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token comment">#放到设备上去</span>    pred <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>pred<span class="token punctuation">,</span>y<span class="token punctuation">)</span> <span class="token comment"># 预测值和期望结果计算loss</span>    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 2.计算loss的gradient</span>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 3.利用gradient调整model的参数</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Validation-Loop验证"><a href="#Validation-Loop验证" class="headerlink" title="Validation Loop验证"></a>Validation Loop验证</h4><p>用于在训练过程中检验模型的状态，收敛情况</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 这句是把model设置成evalidation mode</span>total_loss <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">for</span> x<span class="token punctuation">,</span>y <span class="token keyword">in</span> dv_set<span class="token punctuation">:</span>  x<span class="token punctuation">,</span>y <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span>y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>  <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#关闭梯度</span> pred <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>pred<span class="token punctuation">,</span>y<span class="token punctuation">)</span>  total_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># 收集loss</span>  avg_loss <span class="token operator">=</span> total_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dv_set<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span> <span class="token comment"># 计算平均loss</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Testing-Loop测试"><a href="#Testing-Loop测试" class="headerlink" title="Testing Loop测试"></a>Testing Loop测试</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>preds <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> x <span class="token keyword">in</span> tt_set<span class="token punctuation">:</span>  x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>  <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#和evalidation一样关闭梯度</span>    pred <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    preds<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pred<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 把预测值pred手机到preds中去</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>model的一些layers在testing和training的时候的行为时不一样的，比如dropout，所以需要开evalidation mode。</p><p>在testing和validation的时候我们比希望model学习到这些资料，所以这时候用torch.no_grad把梯度关掉。</p><h4 id="save-load-models"><a href="#save-load-models" class="headerlink" title="save/load models"></a>save/load models</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># save model</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">,</span> path<span class="token punctuation">)</span><span class="token comment"># load model</span>ckpt <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>path<span class="token punctuation">)</span>model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>ckpt<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a>官方文档</h3><p><a href="https://pytorch.org/docs/stable/">PyTorch documentation — PyTorch 2.0 documentation</a></p><p>常用套件</p><p>torch.nn -&gt; neural network</p><p>torch.optim -&gt; optimize function</p><p>Torch.utils.data -&gt; dataset,dataloader</p><h3 id="一些常见error"><a href="#一些常见error" class="headerlink" title="一些常见error"></a>一些常见error</h3><h4 id="Tensor-on-Different-Device-to-model"><a href="#Tensor-on-Different-Device-to-model" class="headerlink" title="Tensor on Different Device to model"></a>Tensor on Different Device to model</h4><p>model和input要放到一个设备上</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 正确写法</span>model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="Mismatched-Dimensions"><a href="#Mismatched-Dimensions" class="headerlink" title="Mismatched Dimensions"></a>Mismatched Dimensions</h4><p>不同大小的矩阵相加，用transpose使大小相同</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token comment"># transpose</span>y <span class="token operator">=</span> y<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># transpose dim1 and dim0</span>z <span class="token operator">=</span> x <span class="token operator">+</span> y<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Cuda-Out-of-Memory"><a href="#Cuda-Out-of-Memory" class="headerlink" title="Cuda Out of Memory"></a>Cuda Out of Memory</h4><p>batch size of data is too larch to fit in GPU，应当减少batch size。</p><p>如果batch已经减少到1了，则可能是model太大，考虑减少model大小。</p><h4 id="Mismatched-Tensor-Type"><a href="#Mismatched-Tensor-Type" class="headerlink" title="Mismatched Tensor Type"></a>Mismatched Tensor Type</h4><p>需要转换tensor的类型，如换成long，用<code>label = label.long()</code></p><h2 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h2><p>老师课上的举例，回归问题就是找function，如最简单的可能就是$y = w\cdot x+b$,其中w为weight，b为bias。</p><p><img src="/img/20235/reg.png" alt="reg.png"></p><p>Liear model线性模型$y=b+\sum w_ix_i$</p><h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><p>Input:a function</p><p>Output:how bad it is</p><p>衡量一组参数的好坏$L(f) = L(w,b)$</p><p>使用估计值-实际值取平方相加并求和，即$L(w,b) = \sum_{n=1}^{10}(\hat{y}^n-(w\cdot x^n+b))^2$</p><p>找loss最小的function，写作$f^* = arg \min_fL(f)$</p><p>$w^*,b^* = arg\min_{w,b}L(w,b)$</p><p>使用gradient descent梯度下降来找这个f</p><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>移动参数 $-\eta \frac{dL}{dw}|_{w=w_0}$，其中$\eta$是learning rate</p><p>如果有两个参数w，b的情况，一样是计算$\frac{\partial L}{\partial w}|<em>{w=w^0}$和$\frac{\partial L}{\partial b}|</em>{b = b^0}$两个偏微分</p><p>在二维上理解，梯度方向就是等高线的方向。</p><p><img src="/img/20235/dgx.png" alt="reg.png"></p><p>在Linear regression问题中不存在local optimal，所以不需要考虑找到的只是局部最优。</p><p><img src="/img/20235/LR.png" alt="reg.png"></p><p>In Linear Regression the loss function L is convex，故不用担心沿着gradient方向找到的只是局部最优解的情况了。即在Linear Regression问题中，Loss function关于w和b的图如上面的图，是形如一个椭圆形的。</p><p><img src="/img/20235/pwf.png" alt="reg.png"></p><p>用求偏导的方式计算Lossfunction对w和b的偏微分，过程如上。</p><p>有时候这种wx+b的模型太简单，可以尝试引入二次项，如$y=b+w_1\cdot x+w_2\cdot x^2$</p><p>效果比单纯一次的模型要更好，同理，可以再增加三次项、四次项$y=b+w_1\cdot x+w_2\cdot x^2 + w_3\cdot x^3$</p><h3 id="over-fitting"><a href="#over-fitting" class="headerlink" title="over fitting"></a>over fitting</h3><p>老师在把模型增加到五次的时候，在trainning data的表现有继续变优，但是testing data反而效果变差了</p><p><img src="/img/20235/overfit.png" alt="overfit"></p><p>A more complex model does not always lead to better performance on testing data!This is called OverFitting.</p><p>所以选择合适的model是一件重要的事情，比如图中的情况显然应当选择3次的model。</p><p><img src="/img/20235/of.png" alt="of.png"></p><h3 id="regularization"><a href="#regularization" class="headerlink" title="regularization"></a>regularization</h3><p>在现有Features不变情况下，降低部分不重要Features的影响力。这个方法有助于有很多Features且每个Features都有贡献的Neural Network避免Overfitting。</p><p><img src="/img/20235/regu.png" alt="regu.png"></p><p>老师的话是，regularization将w平方乘以$\lambda$加入损失函数，有利于让function更加平滑，在实际实验中，regularization能有效的避免overfitting，在训练集中的loss可能会变差，但是在testing中的效果会提升。</p><p><img src="/img/20235/regul.png" alt="在trainning和testing的影响"></p><h2 id="Colab-tutorial"><a href="#Colab-tutorial" class="headerlink" title="Colab tutorial"></a>Colab tutorial</h2><p><a href="https://colab.research.google.com/drive/1Qi4-BRqZ3qI3x_Jtr5ci_oRvHDMQpdiW?usp=sharing&amp;pli=1#scrollTo=bGCELEd1Poea">Google Colab Tutorial 2023 - Colaboratory</a></p><p><a href="https://zhuanlan.zhihu.com/p/386162610">Google Colab 快速上手 - 知乎 (zhihu.com)</a></p><h3 id="很重要！colab防断线"><a href="#很重要！colab防断线" class="headerlink" title="很重要！colab防断线"></a>很重要！colab防断线</h3><p><a href="https://blog.csdn.net/jinniulema/article/details/128994223">(160条消息) colab防止掉线断连及清除方法_colab断开连接_jinniulema的博客-CSDN博客</a></p><pre class="line-numbers language-none"><code class="language-none">function ConnectButton(){    console.log("Connect pushed");     document.querySelector("#top-toolbar &gt; colab-connect-button").shadowRoot.querySelector("#connect").click() }setInterval(ConnectButton,60000);# 结束clearInterval(intervalId) # intervalId换成具体的数字<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="作业1-新冠预测Regression"><a href="#作业1-新冠预测Regression" class="headerlink" title="作业1 : 新冠预测Regression"></a>作业1 : 新冠预测Regression</h2><p>给定1，2，3天的资料，每天的资料是survey和positive cases</p><h3 id="input"><a href="#input" class="headerlink" title="input"></a>input</h3><ul><li>输入包括一个35维度的one-hot vector表示州，后面还跟着一系列feature，其中第三天的test positive case是我们需要预测的</li></ul><p><img src="/img/20235/feature.png" alt="feature"></p><p><img src="/img/20235/feature2.png" alt="feature2"></p><ul><li>one-hot vector就是一个向量里面只有一个元素为1，其余全是0。在作业中，所在州的编码就是一个onehot vector。</li></ul><p><img src="/img/20235/onehot.png" alt="onehot vector">使用MSE（mean squared error)均方误差</p><p>关于feature的大小，打印如下，88个特征我认为是35个州，加上三天的特征(5+5+2+2+3+1)*3 = 54 ，因为最后一天的那个1，感染人数是要预测的，所以-1，即35+54-1 = 88个特征值，一个预测值。</p><p><img src="/img/20235/fsize.png" alt="88个feature"></p><h3 id="sample-code复现"><a href="#sample-code复现" class="headerlink" title="sample code复现"></a>sample code复现</h3><h4 id="固定seed"><a href="#固定seed" class="headerlink" title="固定seed"></a>固定seed</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">same_seed</span><span class="token punctuation">(</span>seed<span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token triple-quoted-string string">'''Fixes random number generator seeds for reproducibility.'''</span>    torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>deterministic <span class="token operator">=</span> <span class="token boolean">True</span>    torch<span class="token punctuation">.</span>backends<span class="token punctuation">.</span>cudnn<span class="token punctuation">.</span>benchmark <span class="token operator">=</span> <span class="token boolean">False</span>    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>    torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>    <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>manual_seed_all<span class="token punctuation">(</span>seed<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>为了固定训练用的seed，让训练结果能够复现，一共四种随机种子，都输入预设好的seed值，在config中可以设置</p><h4 id="train-和-valid-data的划分"><a href="#train-和-valid-data的划分" class="headerlink" title="train 和 valid data的划分"></a>train 和 valid data的划分</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train_valid_split</span><span class="token punctuation">(</span>data_set<span class="token punctuation">,</span> valid_ratio<span class="token punctuation">,</span> seed<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''Split provided training data into training set and validation set'''</span>    valid_set_size <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>valid_ratio <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data_set<span class="token punctuation">)</span><span class="token punctuation">)</span>     train_set_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data_set<span class="token punctuation">)</span> <span class="token operator">-</span> valid_set_size    train_set<span class="token punctuation">,</span> valid_set <span class="token operator">=</span> random_split<span class="token punctuation">(</span>data_set<span class="token punctuation">,</span> <span class="token punctuation">[</span>train_set_size<span class="token punctuation">,</span> valid_set_size<span class="token punctuation">]</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>torch<span class="token punctuation">.</span>Generator<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>train_set<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>valid_set<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>valid_ratio在config中可以设置的一个比例。</p><p>random_split把seed输进去</p><h4 id="predict预测"><a href="#predict预测" class="headerlink" title="predict预测"></a>predict预测</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>test_loader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># Set your model to evaluation mode.</span>    preds <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> x <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>test_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>                                <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                               pred <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                                 preds<span class="token punctuation">.</span>append<span class="token punctuation">(</span>pred<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>       preds <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>preds<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token keyword">return</span> preds<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>比较直观，set成evalidation mode，tqdm是一个用于展示迭代进度条的库</p><p>在preds用append()函数往后面插入结果。</p><p>pred是当前这次预测的结果，这里的pred.detach().cpu()我查阅资料后的理解：</p><p>pred是device:cuda得到的cuda tensor，在官方文档对detach()的说明是返回一个不包含gradient梯度的tensor（detach的含义是分离）</p><pre class="line-numbers language-none"><code class="language-none">Returns a new Tensor, detached from the current graph.The result will never require gradient.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>但是pred.detach()是一个gpu类型的tensor，这时候调用cpu()，转化成cpu tensor类型才可以被list读取。</p><h4 id="dataset定义"><a href="#dataset定义" class="headerlink" title="dataset定义"></a>dataset定义</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">COVID19Dataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''    x: Features.    y: Targets, if none, do prediction.    '''</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> y <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>y <span class="token operator">=</span> y        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>y<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>y <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> self<span class="token punctuation">.</span>x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> self<span class="token punctuation">.</span>x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>y<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>包括初始化init，取单个元素getitem，取长度len</p><h4 id="定义model"><a href="#定义model" class="headerlink" title="定义model"></a>定义model</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">My_Model</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>My_Model<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># TODO: modify model's structure, be aware of dimensions. </span>        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># (B, 1) -&gt; (B)</span>        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里的super之前讲过是调用父类nn.Module的init函数</p><p>然后也有了我们第一个任务，即完成对layers的修改</p><p>这里的layers输入第一层就是input_dim，后来想了一下，其实这么设定的原因是因为有select_feat选择特征函数，所以这里的输入vector维度是不确定的。</p><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p>通过select_feat函数减少特征的维度，即设定传入哪些特征</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">select_feat</span><span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> valid_data<span class="token punctuation">,</span> test_data<span class="token punctuation">,</span> select_all<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''Selects useful features to perform regression'''</span>    y_train<span class="token punctuation">,</span> y_valid <span class="token operator">=</span> train_data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> valid_data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>    raw_x_train<span class="token punctuation">,</span> raw_x_valid<span class="token punctuation">,</span> raw_x_test <span class="token operator">=</span> train_data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> valid_data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> test_data    <span class="token keyword">if</span> select_all<span class="token punctuation">:</span>        feat_idx <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span>raw_x_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        feat_idx <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span> <span class="token comment"># TODO: Select suitable feature columns.</span>            <span class="token keyword">return</span> raw_x_train<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>feat_idx<span class="token punctuation">]</span><span class="token punctuation">,</span> raw_x_valid<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>feat_idx<span class="token punctuation">]</span><span class="token punctuation">,</span> raw_x_test<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>feat_idx<span class="token punctuation">]</span><span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_valid<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里其实诸如train[:,-1]这样的语法不太理解。</p><p>train[:, -1]， 是说对train这个二维的数据，逗号分隔开的前面的”:”是说取全部的行，逗号后面的-1是说取最后一列。 </p><p>如果换成一维数组会容易理解，比如list[:]取全部的列 以及list[-1]取最后一个。</p><p>而后面的b = a[i:j]</p><p>表示复制a[i]到a[j-1]，以生成新的list对象</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span>b <span class="token operator">=</span> a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>  <span class="token comment"># 得到[1,2]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>而如果不填，则默认为0，所以代码中的[:,:-1]其实是[:,0:-1]</p><h4 id="trainning"><a href="#trainning" class="headerlink" title="trainning"></a>trainning</h4><p>torch.optism.SGD参数：第一个是网络的参数，第二个lr是learning rate，第三个momentum是冲量，即$v’ = -dx*lr + v\cdot momentum$即每次的更新量在之前的梯度计算基础上，还要加上上次的更新量乘上一个momentum。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>      model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># Set your model to train mode.</span>      loss_record <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token comment">#记录loss</span>      <span class="token comment"># tqdm is a package to visualize your training progress.</span>      train_pbar <span class="token operator">=</span> tqdm<span class="token punctuation">(</span>train_loader<span class="token punctuation">,</span> position<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> leave<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>      <span class="token keyword">for</span> x<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_pbar<span class="token punctuation">:</span>          optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>               <span class="token comment"># Set gradient to zero.</span>          x<span class="token punctuation">,</span> y <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>   <span class="token comment"># Move your data to device. </span>          pred <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                       loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> y<span class="token punctuation">)</span>          loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>                     <span class="token comment"># Compute gradient(backpropagation).</span>          optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>                    <span class="token comment"># Update parameters.</span>          step <span class="token operator">+=</span> <span class="token number">1</span>          loss_record<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                    <span class="token comment"># Display current epoch number and loss on tqdm progress bar.</span>          train_pbar<span class="token punctuation">.</span>set_description<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch [</span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>n_epochs<span class="token punctuation">}</span></span><span class="token string">]'</span></span><span class="token punctuation">)</span>          train_pbar<span class="token punctuation">.</span>set_postfix<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'loss'</span><span class="token punctuation">:</span> loss<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">)</span>      mean_train_loss <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>loss_record<span class="token punctuation">)</span><span class="token operator">/</span><span class="token builtin">len</span><span class="token punctuation">(</span>loss_record<span class="token punctuation">)</span>      writer<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span><span class="token string">'Loss/train'</span><span class="token punctuation">,</span> mean_train_loss<span class="token punctuation">,</span> step<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>每个epoch中分train和validation两个部分</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># Set your model to evaluation mode.</span>    loss_record <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> x<span class="token punctuation">,</span> y <span class="token keyword">in</span> valid_loader<span class="token punctuation">:</span>        x<span class="token punctuation">,</span> y <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            pred <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>            loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> y<span class="token punctuation">)</span>        loss_record<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            mean_valid_loss <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>loss_record<span class="token punctuation">)</span><span class="token operator">/</span><span class="token builtin">len</span><span class="token punctuation">(</span>loss_record<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch [</span><span class="token interpolation"><span class="token punctuation">{</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>n_epochs<span class="token punctuation">}</span></span><span class="token string">]: Train loss: </span><span class="token interpolation"><span class="token punctuation">{</span>mean_train_loss<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">, Valid loss: </span><span class="token interpolation"><span class="token punctuation">{</span>mean_valid_loss<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>    <span class="token comment"># writer.add_scalar('Loss/valid', mean_valid_loss, step)</span>    <span class="token keyword">if</span> mean_valid_loss <span class="token operator">&lt;</span> best_loss<span class="token punctuation">:</span>        best_loss <span class="token operator">=</span> mean_valid_loss        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> config<span class="token punctuation">[</span><span class="token string">'save_path'</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># Save your best model</span>        <span class="token comment"># 即如果当前的loss比bestloss更好，才会save model</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Saving model with loss {:.3f}...'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>best_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>        early_stop_count <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>         early_stop_count <span class="token operator">+=</span> <span class="token number">1</span>    <span class="token keyword">if</span> early_stop_count <span class="token operator">&gt;=</span> config<span class="token punctuation">[</span><span class="token string">'early_stop'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\nModel is not improving, so we halt the training session.'</span><span class="token punctuation">)</span>        <span class="token keyword">return</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在validation中将mode设置为eval，另外和之前pytorch tutorial中一样，关闭grad，主要目的就是因为validation是不改变参数的。</p><p>这里还添加了一个earlystop，即通过config中设置的参数early_stop（默认600），当模型经过600次都没有改进，则停止训练。</p><h4 id="dataloader"><a href="#dataloader" class="headerlink" title="dataloader"></a>dataloader</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">same_seed<span class="token punctuation">(</span>config<span class="token punctuation">[</span><span class="token string">'seed'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>train_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'./covid_train.csv'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>values<span class="token punctuation">,</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'./covid_test.csv'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>valuestrain_data<span class="token punctuation">,</span> valid_data <span class="token operator">=</span> train_valid_split<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> config<span class="token punctuation">[</span><span class="token string">'valid_ratio'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> config<span class="token punctuation">[</span><span class="token string">'seed'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># Print out the data size.</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"""train_data size: </span><span class="token interpolation"><span class="token punctuation">{</span>train_data<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string"> valid_data size: </span><span class="token interpolation"><span class="token punctuation">{</span>valid_data<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string"> test_data size: </span><span class="token interpolation"><span class="token punctuation">{</span>test_data<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">"""</span></span><span class="token punctuation">)</span><span class="token comment"># Select features</span>x_train<span class="token punctuation">,</span> x_valid<span class="token punctuation">,</span> x_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_valid <span class="token operator">=</span> select_feat<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> valid_data<span class="token punctuation">,</span> test_data<span class="token punctuation">,</span> config<span class="token punctuation">[</span><span class="token string">'select_all'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment">#通过select_feat中的</span><span class="token comment"># Print out the number of features.</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'number of features: </span><span class="token interpolation"><span class="token punctuation">{</span>x_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>train_dataset<span class="token punctuation">,</span> valid_dataset<span class="token punctuation">,</span> test_dataset <span class="token operator">=</span> COVID19Dataset<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> \                                            COVID19Dataset<span class="token punctuation">(</span>x_valid<span class="token punctuation">,</span> y_valid<span class="token punctuation">)</span><span class="token punctuation">,</span> \                                            COVID19Dataset<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span><span class="token comment"># Pytorch data loader loads pytorch dataset into batches.</span>train_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>train_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'batch_size'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> pin_memory<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>valid_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>valid_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'batch_size'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> pin_memory<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>test_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>test_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>config<span class="token punctuation">[</span><span class="token string">'batch_size'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> pin_memory<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出的训练集，valid集和test集的大小如下，由于没做featureselect ，所以还是88维的输入。</p><pre class="line-numbers language-none"><code class="language-none">train_data size: (2408, 89) valid_data size: (601, 89) test_data size: (997, 88)number of features: 88<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="todolist"><a href="#todolist" class="headerlink" title="todolist"></a>todolist</h3><ul><li>modify model’s structure, be aware of dimensions. 更改layers</li><li>Select suitable feature columns.选择重要的feature</li><li>Define your optimization algorithm.选择optimization</li></ul><h3 id="optimization"><a href="#optimization" class="headerlink" title="optimization"></a>optimization</h3><h3 id="我做了什么"><a href="#我做了什么" class="headerlink" title="我做了什么"></a>我做了什么</h3><ul><li><p>在colab上保存了sample code的副本，阅读代码</p></li><li><p>通过google drive导入数据集</p><p><img src="/img/20235/colab.png" alt="colab"></p></li><li><p>运行sample code生成pred.csv，上传到kaggle上，得分（1.68198，185034）</p></li><li><p>尝试修改layers层，增加了一层</p></li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>      nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>      <span class="token comment"># add a layer</span>      <span class="token comment"># 原本是nn.Linear(8, 1)就结束了的</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后上传到kaggle的效果反而变差了（3.90553，3.28428）</p><ul><li>尝试增加instancenorm1d归一化层</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      nn<span class="token punctuation">.</span>InstanceNorm1d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      nn<span class="token punctuation">.</span>InstanceNorm1d<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>      <span class="token comment"># 最后这里加不加激活函数呢？</span>      <span class="token comment"># nn.InstanceNorm1d(1),</span>      <span class="token comment"># nn.ReLU(),</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用归一化后效果还不如最开始什么都不加的。。</p><p>思考了一下会不会是因为之前增加层数导致需要更多的epoch，所以尝试一下增加层数并且增加归一化层。</p><h3 id="代码复现"><a href="#代码复现" class="headerlink" title="代码复现"></a>代码复现</h3><p>torch.optism.SGD参数：第一个是网络的参数，第二个lr是learning rate，第三个momentum是冲量，即$v’ = -dx*lr + v\cdot momentum$即每次的更新量在之前的梯度计算基础上，还要加上上次的更新量乘上一个momentum。</p>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法学习笔记 </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql环境配置和复试sql语句学习</title>
      <link href="2023/02/27/sql/"/>
      <url>2023/02/27/sql/</url>
      
        <content type="html"><![CDATA[<h1 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h1><p>mac arm环境下mysql安装与环境便利那个path的配置</p><p><a href="https://blog.csdn.net/weixin_42148729/article/details/115056646">(146条消息) Mac环境配置MySQL（详细）_互联网小队的博客-CSDN博客_mac mysql</a></p><h1 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h1><p>前面几乎都按照教程中的步骤，之后打开步骤在修改完.bash_profile后</p><p>打开mysql</p><pre class="line-numbers language-none"><code class="language-none">cd /etcmysql -uroot -p//启动<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>创建database</p><pre class="line-numbers language-none"><code class="language-none">create database student//创建数据库mysql&gt; create table students(    -&gt; id INT(11),    -&gt; name VARCHAR(24));//创建students表Query OK, 0 rows affected, 1 warning (0.02 sec)mysql&gt; show tables;//查看一下+-------------------+| Tables_in_student |+-------------------+| students          |+-------------------+1 row in set (0.00 sec)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="考试用内容"><a href="#考试用内容" class="headerlink" title="考试用内容"></a>考试用内容</h1><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h2 id="sql第一小问建表"><a href="#sql第一小问建表" class="headerlink" title="sql第一小问建表"></a>sql第一小问建表</h2><pre class="line-numbers language-none"><code class="language-none">CREAT TABLE 表名( 列名1 数据类型1,  ...  列名n 数据类型n,  表级完整性约束条件1,  ...  表级完整性约束条件n);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 计算机知识 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019秦皇岛 J. MUV LUV EXTRA（KMP找循环）</title>
      <link href="2021/09/22/qhdkmp/"/>
      <url>2021/09/22/qhdkmp/</url>
      
        <content type="html"><![CDATA[<p><a href="https://codeforces.com/gym/102361/problem/J">Problem - J - Codeforces</a></p><h2 id="题意"><a href="#题意" class="headerlink" title="题意"></a>题意</h2><p>给一个小数，如 0.312121，则12为这个小数到末尾的循环节，设p为循环节的长度，l为寻环节出现的长度，则寻环节“12”的长度p为2，出现长度l为5。题目给定权重a，b:问一个小数满足条件的寻环节中，$a\cdot p - b \cdot l$最大值是多少。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><p>用KMP找寻环，但是如果保持小数不做修改，则没法确定模式串的头，故需要反转小数位，这样可以让寻环节都处在开头，只需要考虑不同长度即可。</p><p>如：0.12141可以取开头在位置0的”1214“，也可以是位置3的”14“，如果开始的位置不确定，即模式串的位置不确定。如果把小数位反转，得到”14121“，则发现可以让寻环节都变到开头位置0。</p><p>对这个串处理前缀函数pi。</p><p>如果串12xx12的前缀函数为2，则说明循环的长度是6(总长)-2(前缀函数)。可以发现位置i的循环长度$p=len- \pi(i)$,出现长度$l=len$。在每个位置计算$a\cdot p- b\cdot l$即可。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define int long longconst int MAXN = 1e7+666;string s,ss;int a,b;int pi[MAXN];void getpi(const string &amp;s){ //求s的前缀函数pi[0]=0;int j=0;rep(i,1,s.length()-1){while(j&gt;0&amp;&amp;s[i]!=s[j]) j=pi[j-1];//找到合适且最长的j if(s[i]==s[j])j++;//能成功匹配的情况 pi[i]=j;}}inline void solve(){    cin&gt;&gt;a&gt;&gt;b;    cin&gt;&gt;ss;    int px = 0;    while(ss[px] != '.') px++;        s = ss.substr(px+1);    reverse(s.begin(),s.end());    getpi(s);    //cout&lt;&lt;"s = "&lt;&lt;s&lt;&lt;endl;        int res = a-b;    int n = s.length();    int p,l;    rep(i,0,n-1){        p = i+1;        l = i+1 - pi[i];        res = max(res,a*p-b*l);    }    cout&lt;&lt;res&lt;&lt;endl;}signed main(){    solve();}//KMP找循环节<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 字符串 </tag>
            
            <tag> 题解 </tag>
            
            <tag> VP </tag>
            
            <tag> KMP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>后缀数组SA</title>
      <link href="2021/08/06/sa/"/>
      <url>2021/08/06/sa/</url>
      
        <content type="html"><![CDATA[<h1 id="后缀数组SA"><a href="#后缀数组SA" class="headerlink" title="后缀数组SA"></a>后缀数组SA</h1><h2 id="后缀数组SA-1"><a href="#后缀数组SA-1" class="headerlink" title="后缀数组SA"></a>后缀数组SA</h2><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.luogu.com.cn/blog/xMinh/solution-p3809">题解 P3809 【【模板】后缀排序】 - xMinh 的博客 - 洛谷博客 (luogu.com.cn)</a></p><h3 id="需要维护"><a href="#需要维护" class="headerlink" title="需要维护"></a>需要维护</h3><ul><li>sa[i]：排名为i的后缀的起点（左端点）位置</li><li>rk[i]：起点（左端点）为i的后缀的排名</li></ul><p>理解：rk[sa[i]] = i 和 sa[rk[i]] = i</p><p>即【排名第i的后缀】的排名为i 和 排名第【左端点为i的后缀的排名】的左端点为i</p><h3 id="使用倍增法"><a href="#使用倍增法" class="headerlink" title="使用倍增法"></a>使用倍增法</h3><p>一张特别形象的图：<img src="/img/sasam/sa.jpg" alt="sa"></p><p>这样就能很形象的表示通过倍增进行后缀排序的过程，一直倍增直到排名各不相同为止，因为后缀的长度都不相等，所以结果肯定不会出现排名并列的情况。</p><p><strong>图里面有个“没有则填0”是为了方便理解，在代码中应按照从左到右的顺序，先对这些没有第二关键字的后缀从左到右按顺序进行编号，作为第二关键字最小的那几个。</strong></p><h3 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a>基数排序</h3><pre class="line-numbers language-none"><code class="language-none">一开始的序列是：21 31 32 22 23 13 12从前往后检查低位放到对应的桶里c[1]: 21 31c[2]: 32 22 12c[3]: 23 13按照顺序取出来得到新序列：21 31 32 22 12 23 13从前往后检查高位放入桶中c[1]: 12 13c[2]: 21 22 23c[3]: 31 32得到序列：12 13 21 22 23 31 32<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="实现！SA学习中最大的困难竟然在基数排序"><a href="#实现！SA学习中最大的困难竟然在基数排序" class="headerlink" title="实现！SA学习中最大的困难竟然在基数排序"></a>实现！SA学习中最大的困难竟然在基数排序</h3><pre class="line-numbers language-none"><code class="language-none">代码里，先计算了第二关键字，得到rankrk2: 1  1  2  2  3  3  2序列:21 31 32 22 23 13 12之后根据第一关键字放入桶中rk2:   3  2c[1]: 13 12rk2:   1  2  3c[2]: 21 22 23rk2:   1  2c[3]: 31 32最后根据第二关键字的大小，按顺序从桶中取数即可。c[i]是前缀和，记录第一关键字&lt;=i的一共有几个，对于拥有相同第一关键字的数，根据第二关键字从大到小取出，取出得时候分配排名为c[i]，并且让c[i]--，这样之后从c[i]取数的排名就比先取的考前。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;using namespace std;#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)const int MAXN = 1114514;int n,m;string s;int rk[MAXN],sa[MAXN],c[MAXN],rk2[MAXN];//sa[i]存排名i的原始编号 rk[i]存编号i的排名 第二关键字rk2inline void get_SA(){    rep(i,1,n) ++c[rk[i]=s[i]];//基数排序    rep(i,2,m) c[i] += c[i-1];    //c做前缀和，可以知道每个关键字的排名最低在哪里    repb(i,n,1) sa[c[rk[i]]--] = i;//记录每个排名的原编号    for(int w=1;w&lt;=n;w&lt;&lt;=1){//倍增        int num = 0;        rep(i,n-w+1,n) rk2[++num] = i;//没有第二关键字的排在前面        rep(i,1,n) if(sa[i]&gt;w) rk2[++num] = sa[i]-w;        //编号sa[i]大于w的才能作为编号sa[i]-w的第二关键字        rep(i,1,m) c[i] = 0;        rep(i,1,n) ++c[rk[i]];        rep(i,2,m) c[i]+=c[i-1];        repb(i,n,1) sa[c[rk[rk2[i]]]--]=rk2[i],rk2[i]=0;        //同一个桶中按照第二关键字排序        swap(rk,rk2);        //这时候的rk2时这次排序用到的上一轮的rk,要计算出新的rk给下一轮排序        rk[sa[1]]=1,num=1;        rep(i,2,n)            rk[sa[i]] = (rk2[sa[i]]==rk2[sa[i-1]]&amp;&amp;rk2[sa[i]+w]==rk2[sa[i-1]+w])?num:++num;        //下一次排名的第一关键字,相同的两个元素排名也相同        if(num==n) break;//rk都唯一时，排序结束        m=num;    }}inline void solve(){    cin&gt;&gt;s;    s = ' '+s;    n = s.size()-1,m = 122;//m为字符个数'z'=122    get_SA();    rep(i,1,n) cout&lt;&lt;sa[i]&lt;&lt;' ';    cout&lt;&lt;endl;}int main(){    solve();}//P3809 【模板】后缀排序//https://www.luogu.com.cn/problem/P3809<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="最长公共前缀-LCP"><a href="#最长公共前缀-LCP" class="headerlink" title="最长公共前缀 LCP"></a>最长公共前缀 LCP</h2><p>Longest Common Prefix Array 最长公共前缀，LCP(i,j)表示suff(sa[i])和suff(sa[j])的最长公共前缀。</p><h3 id="LCP的性质"><a href="#LCP的性质" class="headerlink" title="LCP的性质"></a>LCP的性质</h3><ul><li>$LCP(i,j)=LCP(j,i)$</li><li>$LCP(i,i) = length(sa[i]) = n-sa[i]+1$，起点在sa[i]的后缀，长度为n-sa[i]+1</li></ul><h3 id="引理"><a href="#引理" class="headerlink" title="引理"></a>引理</h3><ul><li>对任意的$k\in[i,j]$，有$LCP(i,k)=min{ LCP(i,k),LCP(k,j)}$</li></ul><p>证明：是左右两边两个小于等于号来”夹住”的</p><p>（1）设$suff(sa[i]),suff(sa[j]),suff(sa[k])$三个串串分别是$a,b,c$</p><p>（2）设$p = min(LCP(i,k),LCP(k,j))$，则有$LCP(i,j)\geq p$。（左边夹住了）</p><p>（3）用反证法，若$LCP(i,j)\geq p+1$，因为$p = min(LCP(i,k),LCP(k,j))$，有$a[p+1]\neq b[p+1]$或$b[p+1]\neq c[p+1]$，与$a[p+1]=b[p+1]$矛盾。故征得$LCP(i,j)\leq p$</p><ul><li>补充：上面的步骤（3）是摘抄自参考博客，但是有些许不严谨，可能存在$a[p+1]\neq b[p+1]$且$b[p+1]\neq c[p+1]$的情况，这样的证明漏了$a[p+1]=c[p+1]$的可能性。我的想法是：由于是经过排序后的后缀，故不等号可以修改为$&lt;$号，即$a[p+1]&lt; b[p+1]$或$b[p+1]&lt;c[p+1]$，可推得$a[p+1]&lt;c[p+1]$。</li></ul><p>去博主下面提了一嘴w</p><img src="/img/sasam/comment.png"><p>这么一来，我们要求$LCP(i,j)$,就可以拆成$LCP(i,k),LCP(k,j)$,之后$LCP(k,j)$可以再拆，这么转化，就有点区间DP合并的感觉了</p><h3 id="求LCP"><a href="#求LCP" class="headerlink" title="求LCP"></a>求LCP</h3><p>处理height数组，$height[i]$表示$LCP(i-1,i)$，定义$height[1]=0$，这样求$LCP(i,j)$就变成了求区间最小值$min{height(x)|x\in[i+1,j]}$。但是height怎么来？</p><p>设$h[i]=height[rk[i]]$，即左端点为i的后缀的height</p><p><strong>关键定理</strong>：$h[i]&gt;=h[i-1]-1$</p><p>证明为摘抄，侵删</p><p><a href="https://www.luogu.com.cn/blog/xMinh/solution-p3809">题解 P3809 【【模板】后缀排序】 - xMinh 的博客 - 洛谷博客 (luogu.com.cn)</a></p><pre class="line-numbers language-none"><code class="language-none">首先我们不妨设第i-1个字符串按排名来的前面的那个字符串是第k个字符串，注意k不一定是i-2，因为第k个字符串是按字典序排名来的i-1前面那个，并不是指在原字符串中位置在i-1前面的那个第i-2个字符串。这时，依据height[]的定义，第k个字符串和第i-1个字符串的公共前缀自然是height[rk[i-1]]，现在先讨论一下第k+1个字符串和第i个字符串的关系。第一种情况，第k个字符串和第i-1个字符串的首字符不同，那么第k+1个字符串的排名既可能在i的前面，也可能在i的后面，但没有关系，因为height[rk[i-1]]就是0了呀，那么无论height[rk[i]]是多少都会有height[rk[i]]&gt;=height[rk[i-1]]-1，也就是h[i]&gt;=h[i-1]-1。第二种情况，第k个字符串和第i-1个字符串的首字符相同，那么由于第k+1个字符串就是第k个字符串去掉首字符得到的，第i个字符串也是第i-1个字符串去掉首字符得到的，那么显然第k+1个字符串要排在第i个字符串前面。同时，第k个字符串和第i-1个字符串的最长公共前缀是height[rk[i-1]]，那么自然第k+1个字符串和第i个字符串的最长公共前缀就是height[rk[i-1]]-1。到此为止，第二种情况的证明还没有完，我们可以试想一下，对于比第i个字符串的排名更靠前的那些字符串，谁和第i个字符串的相似度最高（这里说的相似度是指最长公共前缀的长度）？显然是排名紧邻第i个字符串的那个字符串了呀，即sa[rank[i]-1]。但是我们前面求得，有一个排在i前面的字符串k+1，LCP(rk[i],rk[k+1])=height[rk[i-1]]-1;又因为height[rk[i]]=LCP(i,i-1)&gt;=LCP(i,k+1)所以height[rk[i]]&gt;=height[rk[i-1]]-1，也即h[i]&gt;=h[i-1]-1。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>根据上面的定理：h[i]&gt;=h[i-1]-1 ，即height[rk[i]]&gt;=height[rk[i-1]]-1</p><p>用k记录当前的h[i]，因为h[i]&gt;=h[i-1]-1，i每次往后移动时k–。</p><p>i从头扫到尾，k会减少n，因此我们在处理h[i]时完全可以逐字符比较，最多也只比较2n次。</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">int height[MAXN];inline void get_height(){    int k = 0,j;    rep(i,1,n) rk[sa[i]] = i;    rep(i,1,n){        if(rk[i]==1) continue;//第一名往前没有前缀        if(k) k--;//h[i]&gt;=h[i-1]-1 即height[rk[i]]&gt;=height[rk[i-1]]-1        j = sa[rk[i]-1];//找排在rk[i]前面的        while(j+k&lt;=n&amp;&amp;i+k&lt;=n&amp;&amp;s[i+k]==s[j+k]) ++k;//逐字符比较        //因为每次k只会-1，故++k最多只会加2n次        height[rk[i]] = k;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="写题"><a href="#写题" class="headerlink" title="写题"></a>写题</h2><h4 id="第一题"><a href="#第一题" class="headerlink" title="第一题"></a>第一题</h4><p>[P4051 <a href="https://www.luogu.com.cn/problem/P4051">JSOI2007]字符加密 - 洛谷 | 计算机科学教育新生态 (luogu.com.cn)</a></p><p>给一个字符串，如JSOI07，对这个串的所有移动得到的串进行排序，如 JSOI07 SOI07J OI07JS I07JSO 07JSOI 7JSOI0，输出这些串按顺序排列后最后一个字母组成的串。</p><p>是很普通的一题后缀排序，把字符串复制一遍（JSOI07变成JSOI07JSOI07）再对所有的后缀进行排序即可。</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;using namespace std;#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)const int MAXN = 114514&lt;&lt;2;int n,m;string s;int rk[MAXN],sa[MAXN],c[MAXN],rk2[MAXN];inline void get_SA(){    rep(i,1,n) ++c[rk[i]=s[i]];    rep(i,2,m) c[i] += c[i-1];    repb(i,n,1) sa[c[rk[i]]--] = i;    for(int w=1;w&lt;=n;w&lt;&lt;=1){        int num = 0;        rep(i,n-w+1,n) rk2[++num] = i;        rep(i,1,n) if(sa[i]&gt;w) rk2[++num] = sa[i]-w;        rep(i,1,m) c[i]=0;        rep(i,1,n) ++c[rk[i]];        rep(i,2,m) c[i] += c[i-1];        repb(i,n,1) sa[c[rk[rk2[i]]]--]=rk2[i],rk2[i]=0;        swap(rk,rk2);                rk[sa[1]]=1,num=1;        rep(i,2,n)            rk[sa[i]] = (rk2[sa[i]]==rk2[sa[i-1]]&amp;&amp;rk2[sa[i]+w]==rk2[sa[i-1]+w])?num:++num;        if(num==n) break;        m = num;    }}inline void solve(){    cin&gt;&gt;s;    int tmp = s.size();    s = ' '+s+s;    n = s.size()-1;m = 256;    get_SA();    rep(i,1,n)        if(sa[i]&lt;=tmp){            //rep(j,sa[i],sa[i]+tmp-1) cout&lt;&lt;s[j];            //cout&lt;&lt;' ';            cout&lt;&lt;s[sa[i]+tmp-1];        }    cout&lt;&lt;endl;}int main(){    solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="第二题"><a href="#第二题" class="headerlink" title="第二题"></a>第二题</h4><p>[P4248 <a href="https://www.luogu.com.cn/problem/P4248">AHOI2013]差异 - 洛谷 | 计算机科学教育新生态 (luogu.com.cn)</a></p><p>题意：给长n（5e5）的串，问$\sum len(Ti)+len(Tj)-2\cdot LCP(Ti,Tj)|1\leq i&lt;j\leq n$</p><p>解：首先常规的使用sa计算出height数组，之后发现这个式子的前半部分可以直接计算，即$\sum len(Ti)+len(Tj) = \frac{(n-1)n(n+1)}{2}$</p><p>后半部分$\sum LCP(Ti,Tj)$，因为有$lcp(i,j) = min(lcp(i,k),lcp(k,j))$，即$lcp(i,j) = min{height(x)|i\leq x\leq j}$故转变成 <strong>求所有区间最小值之和</strong></p><p>所有区间最小值之和的问题，用<strong>单调栈</strong>实现即可：</p><ul><li><p>维护单调栈，栈底是从$height(2)$到$height(i)$最小的元素，从底到顶单调递增</p></li><li><p>用$dp[i]$记录以$i$为右端点的所有区间的最小值之和。</p></li><li><p>对于一个新的元素$height(i)$，要找到它<strong>左边最近的比他小的元素</strong>p，在访问到$height(i)$时，把栈顶大于$height(i)$的都出栈即可。那么则有$dp[i] = dp[j]+(i-p)*height(i)$</p></li><li><p>当然，如果$height(i)$就是最小的元素，直接$dp[i] = (i-1)*height(i)$</p></li></ul><p>代码：</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;using namespace std;#define rep(i,a,b)  for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)#define ll long longconst int MAXN = 114514&lt;&lt;3;int n,m;string s;int rk[MAXN],sa[MAXN],c[MAXN],rk2[MAXN];inline void get_SA(){    rep(i,1,n) ++c[rk[i]=s[i]];    rep(i,2,m) c[i] += c[i-1];    repb(i,n,1) sa[c[rk[i]]--] = i;    for(int w=1;w&lt;=n;w&lt;&lt;=1){        int num = 0;        rep(i,n-w+1,n) rk2[++num] = i;        rep(i,1,n) if(sa[i]&gt;w) rk2[++num] = sa[i]-w;        rep(i,1,m) c[i] = 0;        rep(i,1,n) ++c[rk[i]];        rep(i,2,m) c[i]+=c[i-1];        repb(i,n,1) sa[c[rk[rk2[i]]]--]=rk2[i],rk2[i]=0;        swap(rk,rk2);        rk[sa[1]]=1,num=1;        rep(i,2,n)            rk[sa[i]] = (rk2[sa[i]]==rk2[sa[i-1]]&amp;&amp;rk2[sa[i]+w]==rk2[sa[i-1]+w])?num:++num;        if(num==n) break;        m = num;    }}int height[MAXN];inline void get_height(){    int k = 0,j;    rep(i,1,n) rk[sa[i]] = i;    rep(i,1,n){        if(rk[i]==1) continue;        if(k) k--;        j = sa[rk[i]-1];        while(j+k&lt;=n&amp;&amp;i+k&lt;=n&amp;&amp;s[i+k]==s[j+k]) ++k;        height[rk[i]] = k;    }}//上面都是板子ll res;ll dp[MAXN];//dp[i]以i为右端点的左侧最小值之和int stk[MAXN],ed;//单调栈inline void solve(){    cin&gt;&gt;s;    n = s.length();    s = ' '+s;    m = 256;    get_SA();    get_height();    //单调栈计算所有区间的最小值之和    ed = 0;    res = (n-1)*n/2*(n+1);    rep(i,2,n){        while(0&lt;ed&amp;&amp;height[stk[ed]]&gt;=height[i]) ed--;        if(0&lt;ed) dp[i] = dp[stk[ed]] + (i-stk[ed])*height[i];        else dp[i] = (i-1)*height[i];        stk[++ed] = i;    }    ll summ = 0;    rep(i,2,n) summ += dp[i];    res -= 2*summ;    cout&lt;&lt;res&lt;&lt;endl;}int main(){    solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 字符串 </tag>
            
            <tag> 算法学习笔记 </tag>
            
            <tag> 后缀数组 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FFT处理字符串匹配问题</title>
      <link href="2021/07/28/fft-match/"/>
      <url>2021/07/28/fft-match/</url>
      
        <content type="html"><![CDATA[<h1 id="FFT处理字符串匹配类问题"><a href="#FFT处理字符串匹配类问题" class="headerlink" title="FFT处理字符串匹配类问题"></a>FFT处理字符串匹配类问题</h1><h2 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h2><p>hdu多校3遇到了一题带通配符*的字符串匹配题，赛中想用KMP写，但是发现是假算法，赛后用Bitset补了一发，优化常数后也T，题解是用FFT写的字符串匹配问题。</p><h2 id="问题1-普通字符串匹配-可用KMP做的那种"><a href="#问题1-普通字符串匹配-可用KMP做的那种" class="headerlink" title="问题1:普通字符串匹配,可用KMP做的那种"></a>问题1:普通字符串匹配,可用KMP做的那种</h2><p>参考：</p><p><a href="https://blog.csdn.net/CCCCTong/article/details/104565258">(27条消息) FFT处理字符串匹配问题_CCCCTong的博客-CSDN博客</a></p><p><a href="https://www.luogu.com.cn/blog/Ebola-Emperor/solution-p4173">题解 P4173 【残缺的字符串】 - Ebola 的博客 - 洛谷博客 (luogu.com.cn)</a></p><h3 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a>数学原理</h3><ul><li><p>定义匹配函数$C(x,y)=A_x-B_y$，如果两个字符相同，则满足$C(x,y)=0$，要推广到字符串上：</p></li><li><p>再定义完全匹配函数$P = \sum^n_{i=1}C(i,i)=\sum^n_{i=1}(A_i-B_i)$，即每一位上的匹配函数求和。</p></li></ul><p>故设A为文本串，B为模式串，取A串的子串A[k,k+m-1]（与B等长），与模式串B进行匹配，定义得到的$P(x) = \sum^{m-1}_{i=0}C(i,k+i)$=$\sum^{m-1}_{i=0} (A\_i-B\_{k+i})$</p><ul><li><strong>如果B的子串B[k~k+m-1]和A完全匹配，则他们的全匹配函数为0</strong></li></ul><p><strong>但其实离上面的还差一步</strong>，考虑串A=ab和B=ba，在第一个位置有$C(1,1)=1-2=-1$;在第二个位置上$C(2,2)=2-1=1$;这样以来，虽然每个位上的匹配函数C不为0，但全匹配函数P为0，所以不行。</p><ul><li>这里很容易就能想到使用<strong>绝对值</strong>来定义匹配函数，但是绝对值需要分类讨论，所以我们使用<strong>平方</strong>，即定义$C(i,i)=(A_i-B_i)^2$。</li></ul><p>这样一来，$P(k) = \sum^{m-1}_{i=0} C(i,k+i)=\sum^{m-1}_{i=0} (A_i-B_{k+i})^2$</p><p>再拆括号：$\sum^{m-1}_{i=0} (A_i-B_{k+i})^2=\sum_{i=0}^{m-1}(A_i^2+B_{k+i}^2-2A_iB_{k+i})$</p><ul><li><p>分开来求和就有$P(x)=\sum_{i=0}^{m-1}A_i^2+\sum_{i=0}^{m-1}B_{k+i}^2-2\sum_{i=0}^{m-1}A_iB_{k+i}$</p></li><li><p>因为是匹配问题，要求出所有k的P(k),第一项与k无关，即对$P(k)$为常数</p></li><li><p>第二项通过前缀和可以$O(n)$得到。</p></li><li><p>第三项则因为要枚举k和i，需要$O(n^2)$,可以怎么样优化？</p></li></ul><p>我们可以观察$A_iB_{k+i}$，可以发现$(k+i)-i=k$。但是FFT在x的次数上是+，这里要-才能满足规律，咋办？</p><ul><li><strong>reverse(A,A+m)反转B串</strong>，记为S串，则有$A_i=S_{m-1-i}$，则第三项$\sum_{i=0}^{m-1}A_iB_{k+i}$变为了$\sum_{i=0}^{m-1}S_{m-1-i}B_{k+i}$。两个下标之和刚好为$m-1+k$</li></ul><p>这里k是左端点，为了符合字符串匹配类问题记右端点的习惯，我们用$x=k+m-1$即<strong>k开头子串的右端点</strong>来代换k：$k=x-m+1$</p><p>我们发现$x=k+m-1$正好是<strong>卷积结果中以k为左端点的结果的位置</strong>。</p><h3 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h3><p>注意右端点x代换左端点k有$k=x-m+1$</p><ul><li><p>设第一项为$T=\sum_{i=0}^{m-1}A_i^2$</p></li><li><p>第二项为$\sum_{i=0}^{m-1}B_{(x-m+1)+i}^2$，我们预处理f[x]数组$f(x)=\sum_{j=0}^{j=x}B_j^2$（f为前缀和），即第二项为f[x]-f[x-m]</p></li><li><p>对于第三项，A和B在进行FFT之后得到G数组，有第三项$\sum_{i=0}^{m-1}A_iB_{k+i}=G_{m-1+k}=G_{x}$刚刚好就是x!</p></li></ul><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;cmath&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define INF 0x3f3f3f3f//FFT板子const int MAXN = 214514;const double Pi=acos(-1.0);struct complex{    double x,y;    complex (double xx=0,double yy=0){x=xx,y=yy;}}a[MAXN],b[MAXN];complex operator + (complex a,complex b){ return complex(a.x+b.x , a.y+b.y);}complex operator - (complex a,complex b){ return complex(a.x-b.x , a.y-b.y);}complex operator * (complex a,complex b){ return complex(a.x*b.x-a.y*b.y , a.x*b.y+a.y*b.x);}//不懂的看复数的运算那部分 int l,r[MAXN];int limit=1;void FFT(complex *A,int type){    for(int i=0;i&lt;limit;i++)         if(i&lt;r[i]) swap(A[i],A[r[i]]);//求出要迭代的序列     for(int mid=1;mid&lt;limit;mid&lt;&lt;=1){//待合并区间的中点        complex Wn( cos(Pi/mid) , type*sin(Pi/mid) ); //单位根         for(int R=mid&lt;&lt;1,j=0;j&lt;limit;j+=R){//R是区间的右端点，j表示前已经到哪个位置了             complex w(1,0);//幂             for(int k=0;k&lt;mid;k++,w=w*Wn){//枚举左半部分                  complex x=A[j+k],y=w*A[j+mid+k];//蝴蝶效应                 A[j+k]=x+y;                A[j+mid+k]=x-y;            }        }    }}//FFT板子string s1,s2;int n,m;complex A[MAXN],B[MAXN];double f[MAXN];double g[MAXN];const double eps = 0.0001;void solve(){    limit = 1,l=0;    cin&gt;&gt;n&gt;&gt;m;    cin&gt;&gt;s1&gt;&gt;s2;    rep(i,0,n-1) B[i].x = s1[i]-'a'+1;    rep(i,0,m-1) A[i].x = s2[i]-'a'+1;    double T = 0;    //T = sigma A[i]^A[i] i=0~m-1    rep(i,0,m-1) T += A[i].x*A[i].x;    //f[x] = sigma B[i]^B[i] i=0~x    f[0] = B[0].x*B[0].x;    rep(i,1,n-1) f[i] = f[i-1]+B[i].x*B[i].x;    //g[x] = S[i]*B[j] i+j==x    reverse(A,A+m);//S = A.reverse    //FFT预处理    while(limit&lt;=n+m-2) limit&lt;&lt;=1,l++;    rep(i,0,limit-1)        r[i]= ( r[i&gt;&gt;1]&gt;&gt;1 )| ( (i&amp;1)&lt;&lt;(l-1) );        FFT(A,1);FFT(B,1);    rep(i,0,limit) A[i]=A[i]*B[i];    FFT(A,-1);    rep(i,0,n-1) g[i] = (int)(A[i].x/limit+0.5);//四舍五入        //T + f(x) - f(x-m) - 2g(x);    double tmp;    rep(x,m-1,n-1){        tmp = T+f[x]-2*g[x];        if(x!=m-1) tmp -= f[x-m];        //cout&lt;&lt;tmp&lt;&lt;' ';        if(fabs(tmp)&lt;eps) cout&lt;&lt;x-(m-1)+1&lt;&lt;endl;//输出匹配上的位置    }    cout&lt;&lt;endl;}int main(){    freopen("in.txt","r",stdin);    int z;    cin&gt;&gt;z;    while(z--) solve();}//洛谷P4173 残缺的字符串（带通配符的字符串匹配问题）//这题的简化问题：没有通配符的情况如何用FFT进行O(nlogn)的字符串匹配//https://www.luogu.com.cn/problem/P4173<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="问题2-带通配符字符串匹配"><a href="#问题2-带通配符字符串匹配" class="headerlink" title="问题2:带通配符字符串匹配"></a>问题2:带通配符字符串匹配</h2><p>题目:在普通字符串匹配的基础上，加上了可以匹配任何字符的通配符’*‘</p><p><a href="https://www.luogu.com.cn/problem/P4173">P4173 残缺的字符串 - 洛谷 | 计算机科学教育新生态 (luogu.com.cn)</a></p><p>参考:</p><p><a href="https://www.luogu.com.cn/blog/Ebola-Emperor/solution-p4173">题解 P4173 【残缺的字符串】 - Ebola 的博客 - 洛谷博客 (luogu.com.cn)</a></p><h3 id="数学原理-1"><a href="#数学原理-1" class="headerlink" title="数学原理"></a>数学原理</h3><p>在普通的字符串匹配中，我们设字母a到z为1-26。</p><ul><li>我们设定通配符为0（原因之后会讲</li></ul><p>这样一来，先前的匹配函数$C(i,i)=(A_i-B_i)^2$在出现通配符时，不能满足$C(i,i)=0$</p><ul><li><p>故构造$C(i,i)=(A_i-B_i)^2A_iB_i$，<strong>这样只要Ai或者Bi中有一个通配符(值为0)，匹配函数的值都为0！非常奇妙！</strong></p></li><li><p>对应这个新的匹配函数$C$,有$P(k) = \sum^{m-1}_{i=0} C(i,k+i)=\sum^{m-1}_{i=0} (A_i-B_{k+i})^2A_iB_{k+i}$</p></li></ul><p>考虑拆平方化简为:$\sum^{m-1}_{i=0} (A_i^3B_{k+i}+A_iB_{k+i}^3-2A_i^2B_{k+i}^2)$</p><p>其实和没有通配符的情况非常类似，依旧可以<strong>拆分为三个项</strong>：</p><p>$\sum^{m-1}_{i=0} (A_i^3B_{k+i})+\sum^{m-1}_{i=0}(A_iB_{k+i}^3)-2\sum^{m-1}_{i=0}(A_i^2B_{k+i}^2)$</p><p>同样把A给reverse$\sum^{m-1}_{i=0} (S_{m-1-i}^3B_{k+i})+\sum^{m-1}_{i=0}(S_{m-1-i}B_{k+i}^3)-2\sum^{m-1}_{i=0}(S_{m-1-i}^2B_{k+i}^2)$</p><p>在上一题中，前两项还可以用比较简单的方法去处理，第三项才需要FFT，这里三项都需要FFT来处理了<span class="github-emoji"><span>😅</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><span class="github-emoji"><span>😅</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><span class="github-emoji"><span>😅</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></p><p>可以进行6次FFT，把三项都加到P上，最后对P进行一次逆变换。</p><h3 id="踩的坑"><a href="#踩的坑" class="headerlink" title="踩的坑"></a>踩的坑</h3><ul><li>在进行完一次A和B的卷积之后，记得要把A和B<strong>全部清空</strong>，包括n和m之后的数据，因为在多项式转化为点值的时候，n和m往后的位置也会被填上。</li></ul><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;#include&lt;cmath&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define INF 0x3f3f3f3f//FFT板子const int MAXN = 114514&lt;&lt;4;const double Pi=acos(-1.0);struct complex{    double x,y;    complex (double xx=0,double yy=0){x=xx,y=yy;}};complex operator + (complex a,complex b){ return complex(a.x+b.x , a.y+b.y);}complex operator - (complex a,complex b){ return complex(a.x-b.x , a.y-b.y);}complex operator * (complex a,complex b){ return complex(a.x*b.x-a.y*b.y , a.x*b.y+a.y*b.x);}//不懂的看复数的运算那部分 int l,r[MAXN];int limit=1;void FFT(complex *A,int type){    for(int i=0;i&lt;limit;i++)         if(i&lt;r[i]) swap(A[i],A[r[i]]);//求出要迭代的序列     for(int mid=1;mid&lt;limit;mid&lt;&lt;=1){//待合并区间的中点        complex Wn( cos(Pi/mid) , type*sin(Pi/mid) ); //单位根         for(int R=mid&lt;&lt;1,j=0;j&lt;limit;j+=R){//R是区间的右端点，j表示前已经到哪个位置了             complex w(1,0);//幂             for(int k=0;k&lt;mid;k++,w=w*Wn){//枚举左半部分                  complex x=A[j+k],y=w*A[j+mid+k];//蝴蝶效应                 A[j+k]=x+y;                A[j+mid+k]=x-y;            }        }    }}//FFT板子string s1,s2;int n,m;int Aa[MAXN],Bb[MAXN];complex A[MAXN],B[MAXN],P[MAXN];double f[MAXN];double g[MAXN];vector&lt;int&gt; vec;const double eps = 0.0001;inline int idx(char c){    if(c=='*') return 0;    else return c-'a'+1;}void solve(){    limit = 1,l=0;    cin&gt;&gt;m&gt;&gt;n;//注意读入顺序，这里是先读入模式串B    cin&gt;&gt;s2&gt;&gt;s1;    rep(i,0,n-1) Bb[i] = idx(s1[i]);    rep(i,0,m-1) Aa[i] = idx(s2[i]);    reverse(Aa,Aa+m);    //S = A.reverse,这里因为三项都要reverse,我直接把原数组给翻了    //FFT预处理    while(limit&lt;=n+m-2) limit&lt;&lt;=1,l++;    rep(i,0,limit-1)        r[i]= ( r[i&gt;&gt;1]&gt;&gt;1 )| ( (i&amp;1)&lt;&lt;(l-1) );    //第一项g1[x] = sigma S[i]^3*B[j] i+j=x    rep(i,0,limit-1) A[i]=B[i]=complex();//n和m之后的数据也要清空,之前wa这里    rep(i,0,m-1) A[i] = complex(Aa[i]*Aa[i]*Aa[i],0);    rep(i,0,n-1) B[i] = complex(Bb[i],0);    FFT(A,1);FFT(B,1);    rep(i,0,limit) P[i]=A[i]*B[i];    //第二项g2[x] = sigma S[i]*B[j]^3 i+j=x    rep(i,0,limit-1) A[i]=B[i]=complex();    rep(i,0,m-1) A[i] = complex(Aa[i],0);    rep(i,0,n-1) B[i] = complex(Bb[i]*Bb[i]*Bb[i],0);    FFT(A,1);FFT(B,1);    rep(i,0,limit) P[i]=P[i]+A[i]*B[i];    //第三项g3[x] = sigma S[i]^2*B[j]^2 i+j=x    rep(i,0,limit-1) A[i]=B[i]=complex();    rep(i,0,m-1) A[i] = complex(Aa[i]*Aa[i],0);    rep(i,0,n-1) B[i] = complex(Bb[i]*Bb[i],0);    FFT(A,1);FFT(B,1);    rep(i,0,limit) P[i]=P[i]-complex(2,0)*A[i]*B[i];        FFT(P,-1);    vec.clear();    double tmp;    rep(x,m-1,n-1){        tmp = (int)(P[x].x/limit+0.5);        if(fabs(tmp)&lt;eps) vec.push_back(x-(m-1)+1);//存左端点位置    }    cout&lt;&lt;vec.size()&lt;&lt;endl;    for(auto x:vec) cout&lt;&lt;x&lt;&lt;' ';    cout&lt;&lt;endl;}int main(){    //freopen("in.txt","r",stdin);    solve();}//洛谷P4173 残缺的字符串（带通配符的字符串匹配问题）//https://www.luogu.com.cn/problem/P4173//洛谷开O2才能过<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="杭电多校2021（3）-1003-Forgiving-Matching"><a href="#杭电多校2021（3）-1003-Forgiving-Matching" class="headerlink" title="杭电多校2021（3） 1003.Forgiving Matching"></a>杭电多校2021（3） 1003.Forgiving Matching</h2><p><a href="https://acm.dingbacode.com/showproblem.php?pid=6975">Problem - 6975 (dingbacode.com)</a></p><h3 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h3><p>时间给8s，n和m的总和1e6，每组n和m为1e5。给长为n的字符串B和长为m的字符串A。串中包含通配符*，可以匹配任何符号。</p><p>定义一个原谅数k，即允许有k位失配，问对[0,m]的不同k，A有多少个连续子串与B匹配。</p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>这题其实比上面的题都简单。它对时间复杂度的要求很低。故可以分成0-9共10次FFT。</p><ul><li>分析题目，可以转化为：求位置x的子串$B[x-m-2,x]$与A的不匹配的数量$tmp$，加到差分数组$cf[m-tmp]$上去即可。</li></ul><p>我们维护10个数组$A[i][j]$，代表A串的j位置是否为数字i（*存在A[10]中）。</p><ul><li>我们定义匹配函数$C(i,j)=A_i\cdot B_i$，故全匹配函数为$P(x)=\sum C(i,x+i)$,同样通过反转后卷积来维护，卷积后的数组$G[i][j]$代表$A$和$B[j-m-2,j]$有多少个相同的位置(都是数字i)。</li><li>通配符另外讨论，有<strong>匹配字符数 = 0-9匹配字符数 + A通配符数 + B子串通配符数 - AB位置重合通配符数量</strong></li></ul><p>A通配符数扫一遍可以得到，B子串通配符数通过前缀和得，AB位置重合的通配符数量，则和之前的其他字母一样，可以通过FFT维护后查询。</p><h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;cmath&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)const int MAXN = 114514&lt;&lt;1;const double Pi=acos(-1.0);//FFT板子struct complex{    double x,y;    complex (double xx=0,double yy=0){x=xx,y=yy;}};complex operator + (complex a,complex b){ return complex(a.x+b.x , a.y+b.y);}complex operator - (complex a,complex b){ return complex(a.x-b.x , a.y-b.y);}complex operator * (complex a,complex b){ return complex(a.x*b.x-a.y*b.y , a.x*b.y+a.y*b.x);}//不懂的看复数的运算那部分 int l,r[MAXN&lt;&lt;2];int limit=1;void FFT(complex *A,int type){    for(int i=0;i&lt;limit;i++)         if(i&lt;r[i]) swap(A[i],A[r[i]]);//求出要迭代的序列     for(int mid=1;mid&lt;limit;mid&lt;&lt;=1){//待合并区间的中点        complex Wn( cos(Pi/mid) , type*sin(Pi/mid) ); //单位根         for(int R=mid&lt;&lt;1,j=0;j&lt;limit;j+=R){//R是区间的右端点，j表示前已经到哪个位置了             complex w(1,0);//幂             for(int k=0;k&lt;mid;k++,w=w*Wn){//枚举左半部分                  complex x=A[j+k],y=w*A[j+mid+k];//蝴蝶效应                 A[j+k]=x+y;                A[j+mid+k]=x-y;            }        }    }}//FFT板子string s1,s2;int n,m;inline int idx(char c){if(c=='*')return 10;return c-'0';}complex A[12][MAXN&lt;&lt;2],B[12][MAXN&lt;&lt;2];int cnt[MAXN];//记录数字相同位置数量int cnttp[MAXN];//通配符位置相同int sum1[MAXN],sum2;int cf[MAXN];//差分数组void solve(){    l=0;limit=1;    cin&gt;&gt;n&gt;&gt;m;    cin&gt;&gt;s1&gt;&gt;s2;    rep(i,0,m) cf[i]=0;//差分数组    while(limit&lt;=n+m-2) limit&lt;&lt;=1,l++;    rep(i,0,limit-1)        r[i]= ( r[i&gt;&gt;1]&gt;&gt;1 )| ( (i&amp;1)&lt;&lt;(l-1) );    rep(k,0,10){        rep(i,0,limit-1) A[k][i] = complex();        rep(i,0,limit-1) B[k][i] = complex();    }    rep(i,0,n-1){        B[idx(s1[i])][i]=complex(1);        cnt[i]=cnttp[i]=0;        if(i==0) sum1[i] = (s1[i]=='*');        else sum1[i]=sum1[i-1]+(s1[i]=='*');    }    sum2=0;    rep(i,0,m-1){        A[idx(s2[i])][i]=complex(1);        sum2 += (s2[i]=='*');    }    //cnt[x] = sigma A[i]*B[j] i+j=x    rep(i,0,10){//普通字符匹配的位置        reverse(A[i],A[i]+m);        FFT(A[i],1);FFT(B[i],1);        rep(j,0,limit-1) A[i][j] = A[i][j]*B[i][j];        FFT(A[i],-1);        if(i!=10) rep(j,m-1,n-1) cnt[j] += (int)(A[i][j].x/limit+0.5);        else rep(j,m-1,n-1) cnttp[j] += (int)(A[i][j].x/limit+0.5);    }    //匹配字符数 = cnt[j] + A通配 + B通配 - AB重合通配(FFT)    int tmp;    rep(i,m-1,n-1){        tmp = cnt[i] + sum2 + sum1[i] - cnttp[i];        if(i!=m-1) tmp -= sum1[i-m];//其实就是sum1[i]-sum1[i-m]，防止越界用        cf[m-tmp]++;//更新差分数组    }    int now = 0;    rep(i,0,m){        now += cf[i];        cout&lt;&lt;now&lt;&lt;endl;    }}  int main(){    int z;    cin&gt;&gt;z;    while(z--) solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FFT </tag>
            
            <tag> 字符串 </tag>
            
            <tag> 算法学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于反悔贪心</title>
      <link href="2021/07/21/fanhui-greedy/"/>
      <url>2021/07/21/fanhui-greedy/</url>
      
        <content type="html"><![CDATA[<h1 id="反悔贪心"><a href="#反悔贪心" class="headerlink" title="反悔贪心"></a>反悔贪心</h1><h2 id="学习的帖子"><a href="#学习的帖子" class="headerlink" title="学习的帖子"></a>学习的帖子</h2><p><a href="https://www.cnblogs.com/amagaisite/p/13537905.html">反悔贪心 学习笔记 - SyadouHayami - 博客园 (cnblogs.com)</a></p><p>[题解 P3620 【<a href="https://www.luogu.com.cn/blog/zhouzhuo/solution-p3620">APIO/CTSC 2007]数据备份】 - 野心qwq 的博客 - 洛谷博客 (luogu.com.cn)</a></p><h2 id="写的第一个例题："><a href="#写的第一个例题：" class="headerlink" title="写的第一个例题："></a>写的第一个例题：</h2><p>[P3620 <a href="https://www.luogu.com.cn/problem/P3620">APIO/CTSC 2007]数据备份 - 洛谷 | 计算机科学教育新生态 (luogu.com.cn)</a></p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>转换后的题目是：给长度为n-1的序列，不能选择相邻的两个元素，问选择k个元素的最小和。</p><p>普通的贪心是，把所有的元素从大到小排列，每次选择最小的元素，并且把它相邻的两个元素打上删除标记表示不能选择。但是显然这样不带反悔的贪心是不行的。</p><p>比如对于n=4,k=2的序列2 1 2 9，按照这个贪心来做，会先选到1，并且删除了两个2，之后就只能选择9了，这样显然不是最优解。</p><p><strong>”考虑一个反悔的过程，我们需要构造一个方法去消掉之前的贡献转而加为正确的更优的贡献。”</strong></p><p>也就是说，我们需要把一个能够消除掉之前选择的影响的点推入这个排序中，和其他普通的贪心选项一起比较，每次选择其中最优的。(由于要动态维护这个顺序，所以用堆/优先队列实现)</p><p>在这题里，当选择了位置i的元素$da_i$,则$da_{i-1}$和$da_{i+1}$是不能选择的，但是最优解依旧有可能包含着两个元素，故我们构造一个新的点加入排序，这个点的点权就是：$da_{i-1}+da_{i+1}-da_i$,仔细想想，就是消去了之前选择$da_i$的贡献，并且加上了这个新的选择的贡献。</p><ul><li>关于为什么是$da_{i-1}$和$da_{i+1}$一起，而不可能是单独选择其中的一个？</li></ul><p>因为显然对于这种情况，在优先队列中$da_i$比$da_{i-1}$和$da_{i+1}$更早出现，故$da_{i-1}$和$da_{i+1}$都要比$da_i$小，所以显然着两个要选也是一起选，而不会只选其中一个就把之前的选择$da_i$反悔掉。</p><p>在选择了这个代表$da_{i-1}$和$da_{i+1}$的点后，同理也要推入代表$da_{i-1}$和$da_{i+1}$的点。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>通过双向链表实现了对点的删去，在选择了$da_i$之后，我们把$da_{i-1}+da_{i+1}-da_i$的值存在了原先$da_i$的位置，并且删去了$da_{i-1}$和$da_{i+1}$两个点，并且把反悔新点推入优先队列。</p><pre class="line-numbers language-none"><code class="language-none">2 1 2 9<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>两个2被删去，并且把反悔点的贡献存在了原先选择的1的位置：</p><pre class="line-numbers language-none"><code class="language-none">(2) 3 (2) 9<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这时候$da_i$左右分别和$da_{i-2}$和$da_{i+2}$相连，如果我们之后发现$da_{i-2}$和$da_{i+2}$要比$da_{i-1}$和$da_{i+1}$更优，同样是删去$da_i$左右两侧的两个点，并且把这两个点减去$da_i$的值存入$da_i$，以此类推。</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;queue&gt;using namespace std;const int MAXN = 114514;#define INF 0x3f3f3f3f#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)int da[MAXN],l[MAXN],r[MAXN];//双向连表bool del[MAXN];inline void del_lr(int pos){//删除其左右的元素    del[l[pos]] = del[r[pos]] = 1;//打上删除    da[pos] = da[l[pos]]+da[r[pos]]-da[pos];    //双向链表删点    l[pos]=l[l[pos]],r[pos]=r[r[pos]];    l[r[pos]]=pos,r[l[pos]]=pos;//一开始忘了这句即另一边也要接到pos上来}//关于为什么是双向链表：//如果是4 2 1 2 4，选择了1之后,2 2无法选择从链表中删除变成4 3 4,3代表1左右两边的2//这时候选择3就代表选择了两个2而放弃了1，而选了这两个2就不能选4,3和两个4在链表上也是相邻的int a[MAXN];int n,k;struct Node{    int val,pos;//数值和编号    bool operator &lt; (Node rhs)const{//优先队列要加const        return val&gt;rhs.val;    }};priority_queue&lt;Node&gt; q;void solve(){    cin&gt;&gt;n&gt;&gt;k;    rep(i,1,n) cin&gt;&gt;a[i];    rep(i,1,n-1){        da[i] = a[i+1]-a[i];//相邻做差存入双向链表        l[i] = i-1,r[i] = i+1;        q.push(Node{da[i],i});    }    da[0]=da[n]=INF;//保证了链表两端不会被修改(数值很大一定在堆的尾部)    Node now;    int res = 0;//记录结果    rep(i,1,k){//选择k条线缆        while(del[q.top().pos]) q.pop();//已经被删掉的点还在优先队列里        now = q.top();        q.pop();        res += now.val;//res += da[now.pos]也行        del_lr(now.pos);//删掉左右两边        q.push(Node{da[now.pos],now.pos});//这时候的da[pos]已经是修改过的了    }    cout&lt;&lt;res&lt;&lt;endl;}int main(){    //freopen("P3620_2.in","r",stdin);    solve();}//反悔贪心例题//P3620 [APIO/CTSC 2007]数据备份//https://www.luogu.com.cn/problem/P3620<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="写的第二个例题："><a href="#写的第二个例题：" class="headerlink" title="写的第二个例题："></a>写的第二个例题：</h2><p>[P4053 <a href="https://www.luogu.com.cn/problem/P4053">JSOI2007]建筑抢修 - 洛谷 | 计算机科学教育新生态 (luogu.com.cn)</a></p><p>这题只是蓝题，比第一题要简单点</p><h3 id="分析-1"><a href="#分析-1" class="headerlink" title="分析"></a>分析</h3><p>题目意思是，每个待修理的建筑有两个参数：修理需要的时间和结束时间，必须在结束时间之前修完，问最多有可以修多少个建筑。</p><p>先考虑一般的贪心：把建筑根据结束时间进行排序，从小到大考虑若能修$(now+a[i].cost\leq a[i].endt)$就直接修，但是这样显然会有不行的情况，比如</p><pre class="line-numbers language-none"><code class="language-none">3100 10050 20050 200//目测最优解为2，但上述贪心为1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>考虑怎么反悔贪心：</p><ul><li>可以直接加入的情况$now+a[i].cost\leq a[i].endt$</li><li>不可以加入，需要考虑反悔的情况$now+a[i].cost&gt;a[i].endt$</li></ul><p>考虑每次把点$a[i]$加入修理的集合中时，把$a[i].cost$推入堆中维护最大堆，之后一旦出现不可加入的情况，考虑：</p><ul><li><p>(1)去除堆顶的点后这个点能不能加入,$now-q.top+a[i].cost\leq a[i].endt$</p></li><li><p>(2)用这个点替换堆顶的点是不是更优(用的时间更少),$a[i].cost&lt;q.top$</p></li></ul><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;queue&gt;using namespace std;#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define ll long long#define int ll//题目居然不标范围!typedef pair&lt;int,int&gt; pii;#define ft first#define sd second#define mkp make_pairconst int MAXN = 114514&lt;&lt;1;int n;pii a[MAXN];priority_queue&lt;int&gt; q; //维护当前选择的点中时间最长的bool cmp(pii a,pii b){    return a.sd&lt;b.sd;}void solve(){    cin&gt;&gt;n;    rep(i,1,n) cin&gt;&gt;a[i].ft&gt;&gt;a[i].sd;//按照结束时间排序    //按照时间排序    sort(a+1,a+1+n,cmp);    //rep(i,1,n) cout&lt;&lt;'('&lt;&lt;a[i].ft&lt;&lt;','&lt;&lt;a[i].sd&lt;&lt;')'&lt;&lt;endl;    int nowt = 0;//当前时间    int tmp,cnt;    rep(i,1,n){        if(nowt+a[i].ft&gt;a[i].sd){//时间不够修了            if(!q.empty()){                tmp = q.top();//取需要时间最大的                if(tmp&gt;a[i].ft&amp;&amp;nowt+a[i].ft-tmp&lt;=a[i].sd){//去掉头可以让时间足够                    nowt = nowt - tmp + a[i].ft;                    q.pop();                    q.push(a[i].ft);                }            }        }        else{//时间足够            nowt += a[i].ft;            q.push(a[i].ft);            cnt++;        }    }    cout&lt;&lt;cnt&lt;&lt;endl;}signed main(){    solve();}// 反悔贪心// P4053 [JSOI2007]建筑抢修// https://www.luogu.com.cn/problem/P4053<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="写的第三个例题："><a href="#写的第三个例题：" class="headerlink" title="写的第三个例题："></a>写的第三个例题：</h2><p><a href="https://www.luogu.com.cn/problem/CF436E">CF436E Cardboard Box - 洛谷 | 计算机科学教育新生态 (luogu.com.cn)</a></p><h3 id="分析-2"><a href="#分析-2" class="headerlink" title="分析"></a>分析</h3><p>题意：给n个关卡，要获得w颗星，每个关卡可以支付$a_i$获得1颗星，支付$b_i$获得两颗星，问得到w颗星的最小费用。要求输出选择。</p><h4 id="一个假贪心"><a href="#一个假贪心" class="headerlink" title="一个假贪心"></a>一个假贪心</h4><p>我先写了一个假的后悔贪心：即选择了$a_i$之后$a_i-b_i$推入堆中，代表反悔第i关选择1个的操作，改成了选2个。</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">//假的反悔贪心const int MAXN = 114514&lt;&lt;2;int n,w;priority_queue&lt;pii,vector&lt;pii&gt;,greater&lt;pii&gt; &gt; q;int a[MAXN],b[MAXN],sel[MAXN];void solve(){    cin&gt;&gt;n&gt;&gt;w;    int cost = 0;    rep(i,1,n){        cin&gt;&gt;a[i]&gt;&gt;b[i];        q.push(mkp(a[i],i));    }    pii tmp;    rep(i,1,w){        tmp = q.top();        q.pop();        if(sel[tmp.sd]==0){            cost += tmp.ft;            q.push(mkp(b[tmp.sd]-tmp.ft,tmp.sd));            sel[tmp.sd] = 1;        }        else{//sel[tmp.sd]==1            cost += tmp.ft;            sel[tmp.sd] = 2;        }    }    cout&lt;&lt;cost&lt;&lt;endl;    rep(i,1,n) cout&lt;&lt;sel[i];    cout&lt;&lt;endl;}int main(){    solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>这个贪心的问题在于：有些关是一个点非常不划算，两个点非常划算的情况，导致$a_i$根本没机会被选，这样$b_i-a_i$根本没机会加入队列。</strong></p><h4 id="正解"><a href="#正解" class="headerlink" title="正解"></a>正解</h4><p>首先，根据之前的假算法，我们需要考虑从0星直接变成2星的情况，但是这种情况，选择的星星数量会直接+2，而普通的情况是+1，这样不能进行比较。因此，我们在<strong>0星变成2星的时候，同时也要让一个已经做的选择-1</strong>，即”i从0星变2星，j从1星反悔成0星“和”i从0星变2星，j从2星反悔成1星“这两种情况。</p><p><strong>讨论贪心情况对答案的贡献：</strong></p><ul><li>i从0星变1星：$a_i$</li><li>i从1星变2星：$b_i-a_i$</li></ul><p>本来可以只用一条队列维护，但由于后面的反悔选项，因此也需要两条队列，原因一会儿会讲。</p><p><strong>讨论两种反悔情况对答案的贡献：</strong></p><ul><li>i从0星变2星，j从1星反悔成0星：$b_j-a_i$</li><li>i从0星变2星，j从2星反悔成1星：$b_j-(b_i-a_i)$</li></ul><p>要求得以上两种情况的最优解，需要维护：最小的$b_j$，最大的$a_i$和最大的$b_i-a_i$。需要用到三条队列。</p><p><strong>为什么需要五条队列？：</strong></p><p>有些关卡星值的修改会<strong>牵扯到两个堆</strong>，即一个关卡会被推入多个堆中，因此我们在取出了这个关卡的其中一个，并对它进行处理后，另一个堆中的这个关卡的点会变为无效点。我们用sel[i]记录第i关的选项，每次取堆顶之前，先与sel进行比较，把堆顶的无效点都去除（有点像单调队列）。如下：</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">while(!q1.empty()&amp;&amp;sel[q1.top().sd]!=0) q1.pop();<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h4><ul><li>我们在实现的时候，从5个堆中取出相应的堆顶元素，求出四种方案的解的贡献存进$op1,op2,op3,op4$，再进行比较，选择贡献最小的一个，并把对应的堆顶出栈，再推入更改后的元素。</li></ul><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">int minp = 1;rep(i,2,4)if(op[i]&lt;op[minp])minp=i;//找到4个op中的最优解res += op[minp];<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>更新关卡星数的操作会被重复用，因此可以用函数封装让其更直观，比如，贪心操作1和反悔操作2都会产生新的1星关，故可以用函数add1封装。</li></ul><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">inline void add0(int pos){    sel[pos] = 0;    q1.push(mkp(a[pos],pos));//0变1    q3.push(mkp(b[pos],pos));//0变2}inline void add1(int pos){    sel[pos] = 1;    q2.push(mkp(b[pos]-a[pos],pos));//1变2    q4.push(mkp(a[pos],pos));//1变0}inline void add2(int pos){    sel[pos] = 2;    q5.push(mkp(b[pos]-a[pos],pos));//2变1}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;queue&gt;#include&lt;algorithm&gt;using namespace std;#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)typedef pair&lt;int,int&gt; pii;#define ft first#define sd second#define mkp make_pair#define int long long#define INF 0x7fffffffconst int MAXN = 114514&lt;&lt;2;int n,w;int a[MAXN],b[MAXN],sel[MAXN];//====贪心操作====//1.把0星变1星  2.把1星变2星//   ai         bi-ai(q2)//q1维护ai最小值，q2维护bi-ai最小值//====反悔操作====//3.把j的0星变2星,再把i的1星变0星  4.把j的0星变2星,再把i的2星变1星//    bj - ai                       bj - (bi-ai)//q3维护bj最小值，q4维护ai最大值，q5维护bi-ai最大值//这样一来便可以处理"1星特别不划算,2星特划算"的情况priority_queue&lt;pii,vector&lt;pii&gt;,greater&lt;pii&gt; &gt; q1,q2,q3;priority_queue&lt;pii&gt; q4,q5;inline void add0(int pos){    sel[pos] = 0;    q1.push(mkp(a[pos],pos));//0变1    q3.push(mkp(b[pos],pos));//0变2}inline void add1(int pos){    sel[pos] = 1;    q2.push(mkp(b[pos]-a[pos],pos));//1变2    q4.push(mkp(a[pos],pos));//1变0}inline void add2(int pos){    sel[pos] = 2;    q5.push(mkp(b[pos]-a[pos],pos));//2变1}inline void solve(){    cin&gt;&gt;n&gt;&gt;w;    int cost = 0;    rep(i,1,n){        cin&gt;&gt;a[i]&gt;&gt;b[i];        q1.push(mkp(a[i],i));        q3.push(mkp(b[i],i));    }    int op[6];    int res=0;    rep(i,1,w){//进行w次选择        op[1]=op[2]=op[3]=op[4]=INF;//四种操作的贡献,找最值        int pos1,pos2,pos3,pos4,pos5;//五个堆的下标暂存        while(!q1.empty()&amp;&amp;sel[q1.top().sd]!=0) q1.pop();        if(!q1.empty()){//0变1            pos1 = q1.top().sd;            op[1] = a[pos1];        }        while(!q2.empty()&amp;&amp;sel[q2.top().sd]!=1) q2.pop();        if(!q2.empty()){//1变2            pos2 = q2.top().sd;            op[2] = b[pos2]-a[pos2];        }        while(!q3.empty()&amp;&amp;sel[q3.top().sd]!=0) q3.pop();        while(!q4.empty()&amp;&amp;sel[q4.top().sd]!=1) q4.pop();        if(!q3.empty()&amp;&amp;!q4.empty()){//0变2,1变0            pos3 = q3.top().sd;pos4 = q4.top().sd;            op[3] = b[pos3]-a[pos4];        }        while(!q5.empty()&amp;&amp;sel[q5.top().sd]!=2) q5.pop();        if(!q3.empty()&amp;&amp;!q5.empty()){//0变2,2变1            pos3 = q3.top().sd;pos5 = q5.top().sd;            op[4] = b[pos3]-(b[pos5]-a[pos5]);        }        int minp = 1;        rep(i,2,4)if(op[i]&lt;op[minp])minp=i;//找到4个op中的最优解        res += op[minp];        if(minp==1){//1.把0星变1星            q1.pop();            add1(pos1);        }        else if(minp==2){//2.把1星变2星            q2.pop();            add2(pos2);        }        else if(minp==3){//3.把j的0星变2星,再把i的1星变0星            q3.pop(),q4.pop();            add2(pos3);            add0(pos4);        }        else if(minp==4){//4.把j的0星变2星,再把i的2星变1星            q3.pop(),q5.pop();            add2(pos3);            add1(pos5);        }    }    cout&lt;&lt;res&lt;&lt;endl;    rep(i,1,n) cout&lt;&lt;sel[i];    cout&lt;&lt;endl;}signed main(){    solve();}//反悔贪心例题//CF436E Cardboard Box//https://www.luogu.com.cn/problem/CF436E<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法学习笔记 </tag>
            
            <tag> 贪心 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多校补题记录</title>
      <link href="2021/07/20/duoxiao/"/>
      <url>2021/07/20/duoxiao/</url>
      
        <content type="html"><![CDATA[<h1 id="多校补题记录"><a href="#多校补题记录" class="headerlink" title="多校补题记录"></a>多校补题记录</h1><h2 id="n个区间分k组-使每组区间的并集长度之和最大"><a href="#n个区间分k组-使每组区间的并集长度之和最大" class="headerlink" title="n个区间分k组,使每组区间的并集长度之和最大"></a>n个区间分k组,使每组区间的并集长度之和最大</h2><p>牛客2-G-League of Legends 【单调队列优化DP】</p><p><a href="https://ac.nowcoder.com/acm/contest/11253/G">G-League of Legends_2021牛客暑期多校训练营2 (nowcoder.com)</a></p><p>题意：给n,k(5000)，有n个成员要被分成k组，每组对答案的贡献为：所有组员空闲时间[ai,bi)的并集的长度。求最大的答案。</p><p>解：把所有的线段分成两个集合，“大边”和“小边”，如果一个线段能够完整包含另一条线段，那这个边是“大边”，剩余的边为“小边”。</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">//O(n)处理大边小边int minr = INF;    sort(a+1,a+1+n,cmp);//这里第一关键字是ft小，第二关键字是sd大    repb(i,n,1){//分成大小区间        if(a[i].sd&gt;=minr) da[++cd] = a[i].sd-a[i].ft;//大区间只记录长度        else minr = a[i].sd,xi[++cx] = a[i];    }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>一条大边只有两种去处：一种是单独成组，贡献=长度。另一种是跟随某条能被它完全包含的“小边”进入同一组，则对答案不造成影响。</p></li><li><p>对小边，在对pair排序后，i位置一定满足$L_i \geq L_{i-1}$且$R_i \geq R_{i-1}$。</p></li></ul><p>因此我们只需要讨论小边的答案，最后枚举使用大边的数量即可。一段连续的小边$(i,j)$如果要放在同一组，一定满足$L_j&gt;R_i $，而且这么成一组的贡献就是$L_j-R_i$。</p><p><img src="/img/duoxiao/nk2g.jpg" alt="nk2g"></p><p>设$DP_{i,j}$为前i组用了j个人的最优解。显然有公式$DP_{i,j} = DP_{i-1,k} + (R_{k+1}-L{j})$，这么以来就有了$n^3$的做法。之后考虑使用优先队列优化，把$DP_{i,j} = DP_{i-1,k} + (R_{k+1}-L{j})$中的$DP_{i-1,k} + R_{k+1}$推入队列即可。</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);template&lt;class T&gt;inline void read(T &amp;x){x=0;char o,f=1;while(o=getchar(),o&lt;48)if(o==45)f=-f;do x=(x&lt;&lt;3)+(x&lt;&lt;1)+(o^48);while(o=getchar(),o&gt;47);x*=f;}#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)#define INF 0x3f3f3f3ftypedef pair&lt;int,int&gt; pii;#define ft first#define sd second#define mkp make_pairconst int MAXN = 5050;pii a[MAXN],xi[MAXN];//小边int da[MAXN];//大边int cd,cx;//记数int n,m;//n个人分m组int dp[MAXN][MAXN];//分成i组，用前j个pii q[MAXN];//对于每个1&lt;=i&lt;=m分配一条优先队列//优先队列,ft存dp[i][k] + xi[k].sd,sd存 xi[k].sdint st,ed;bool cmp(pii a,pii b){    if(a.ft==b.ft) return a.sd&gt;b.sd;    return a.ft&lt;b.ft;}void solve(){    cin&gt;&gt;n&gt;&gt;m;//分成m组    rep(i,1,n)        cin&gt;&gt;a[i].ft&gt;&gt;a[i].sd;    int minr = INF;    sort(a+1,a+1+n,cmp);    repb(i,n,1){//分成大小区间        if(a[i].sd&gt;=minr) da[++cd] = a[i].sd-a[i].ft;//大区间只记录长度        else minr = a[i].sd,xi[++cx] = a[i];    }    sort(xi+1,xi+1+cx);    dp[0][0] = 0;//要特殊处理一下    rep(i,1,m){        st=1,ed=0;        rep(j,i,cx){            if((i==1&amp;&amp;j==1)||dp[i-1][j-1]!=0){                while(st&lt;=ed&amp;&amp;q[ed].ft&lt;=dp[i-1][j-1]+xi[j].sd) ed--;//pop back;                 q[++ed] = mkp(dp[i-1][j-1]+xi[j].sd,xi[j].sd);            }            while(st&lt;=ed&amp;&amp;q[st].sd&lt;=xi[j].ft) st++;//pop front            if(st&lt;=ed)                dp[i][j] = q[st].ft - xi[j].ft;//dp[i][k]+xi[k].sd - xi[j].ft        }    }    //接下来枚举前缀和    sort(da+1,da+1+cd,greater&lt;int&gt;());    cd = min(cd,m);    int res = 0;    int sum = 0;    rep(j,0,m){//前j个大区间单独成组        sum += da[j];//做前缀和        if(dp[m-j][cx]==0) continue;//无效值        res = max(res,dp[m-j][cx]+sum);    }    cout&lt;&lt;res&lt;&lt;endl;}int main(){    //freopen("in.txt","r",stdin);    solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意单调队列实现的细节。一开始一直wa，因为是要满足$k&lt;j$的，所以只有在计算到$DP_{i,j}$时，才能把$DP_{i-1,j-1}+R_j$推进来。</p><h2 id="把字符串分成两段，问有多少失配数-lt-k的子串"><a href="#把字符串分成两段，问有多少失配数-lt-k的子串" class="headerlink" title="把字符串分成两段，问有多少失配数<=k的子串"></a>把字符串分成两段，问有多少失配数&lt;=k的子串</h2><p>杭电多校5 Another String【滑动窗口，差分】</p><p><a href="https://acm.hdu.edu.cn/showproblem.php?pid=7015">Problem - 7015 (hdu.edu.cn)</a></p><p>题意：给一个长3000的串n，和允许最大失配数（距离）k，问对于每个分界点i，左右两侧有多少对距离小于等于k的子串。</p><p>这个分界点$i$，题目定义的是左半边的最后一个字符，即左半边为$s[1,i]$右半边为$s[i+1,n]$。</p><p>我写的时候定$i$为右半边的第一个字符，即左半边为$s[1,i-1]$,右半边为$s[i,n]$。</p><p>解：第一层枚举两个滑动窗口的间距d，然后在里面用for循环枚举$l$，对于每个$l$，使$r$一直增加得到最大的且距离不超过$k$的子串。</p><p><img src="/img/duoxiao/hd5.jpg" alt="hd5"></p><p>如图，可以发现对于这一对匹配串，可以选择的分界点$i$从$4$到$6$，在结果上，对这一段的$res[4]$到$res[6]$，都要$+1$，可以用差分数组实现。</p><p>但是这样只是讨论到“最长”的情况，对于每个l而言，不仅仅是这个最大的r可以使得串s[l,r]与串$s[d+l-1,d+r-1]$匹配，对于其他的$l\leq x\leq r$，显然对于串$s[l,x]$和$s[l+d-1,x+d-1]$也满足条件，故我们在图上标出这些满足条件的子串：</p><p><img src="/img/duoxiao/hd5.2.jpg" alt="hd5.2"></p><ul><li>可以发现，这对串的结束点x从后往前移动，分别有长度为1，2，3的三对匹配串。长度为1的串’a’有符合条件的分界点$i\in [2,6]$，长度为2的串’ab’有符合条件的分界点$i\in [3,6]$，长度为3的串’abc’有符合条件的分界点$i\in[4,6]$。可以发现，对答案的贡献为一个<strong>梯型</strong>。</li><li>我们用差分数组实现，要构造这样的梯形，需要在差分数组的cf[2,3,4]处+1，cf[7]处-3。但是对于这个梯形比较长的时候，修改差分数组的复杂度也变成了$O(n)$，故需要再造一个<strong>差分数组的差分数组</strong>cf2，原本的cf[2,3,4]+=1，可以变成cf[2]+=1，cf[5]-=1。</li></ul><p>赛中这题其实已经写的七七八八了，最后发现，对于第一层枚举的间距d，我并没有把这个理解成间距，而是理解成另外一个分界点x，想着滑窗的时候，不能让左边的l和r超过这个分界点x，实际上完全是可以的，不然会漏掉很多情况。</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);template&lt;class T&gt;inline void read(T &amp;x){x=0;char o,f=1;while(o=getchar(),o&lt;48)if(o==45)f=-f;do x=(x&lt;&lt;3)+(x&lt;&lt;1)+(o^48);while(o=getchar(),o&gt;47);x*=f;}#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)#define INF 0x3f3f3f3fll gcd(ll a,ll b){ while(b^=a^=b^=a%=b); return a; }int n,k;string s;const int MAXN = 3090;ll cf1[MAXN],cf2[MAXN];void solve(){    cin&gt;&gt;n&gt;&gt;k;    rep(i,1,n) cf1[i]=cf2[i]=0;    cin&gt;&gt;s;    s = ' '+s;    rep(fj,2,n){        int nowdis = 0;        int rb = 0;        rep(lb,1,n){            if(fj+lb-1&gt;n) break;            while(rb+1&lt;fj+lb-1&amp;&amp;fj+rb&lt;=n&amp;&amp;nowdis+(s[rb+1]!=s[fj+rb])&lt;=k){                rb++;                nowdis += s[rb]!=s[fj+rb-1];            }            nowdis -= (s[lb]!=s[fj+lb-1]);            cf1[lb+1]++;cf1[rb+2]--;            cf2[fj+lb]-=rb-lb+1;        }    }    ll now = 0;    rep(i,1,n){        now += cf1[i];        cf2[i] += now;    }    now = cf2[1];    rep(i,2,n){        now += cf2[i];        cout&lt;&lt;now&lt;&lt;endl;    }}int main(){    int z;    cin&gt;&gt;z;    while(z--) solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="图上选边，使两端点权-1，问最少步数"><a href="#图上选边，使两端点权-1，问最少步数" class="headerlink" title="图上选边，使两端点权-1，问最少步数"></a>图上选边，使两端点权-1，问最少步数</h2><p>牛客多校9 J-Jam</p><p><a href="https://ac.nowcoder.com/acm/contest/11260/J">J-Jam_2021牛客暑期多校训练营9 (nowcoder.com)</a></p><p>题意：有十字路口，每个路口有三个方向，每个时间点的安排不能使两条路线交叉，问最少需要多少个时间可以让十字路口的车都走完。（每个路口每个方向的车Ci,j&lt;=100)</p><p><img src="/img/duoxiao/shizi.png" alt="shizi"></p><p>解：这题可以用带花树写，对每个车建点，之后跑图上的最大匹配。但是我们赛中写的是网络流。</p><p>首先可以发现，每个路口向右走的车辆可以独立出来，因为向右走是不与其他方向冲突点。</p><p>这样一来，可以把每方位的两种方向都拉出来，转变成了：”给定一张图，每次可以选择一条边，使得两端点权值各-1，问最少操作多少次使点权全部清空？”</p><p><img src="/img/duoxiao/shizitu.jpg" alt="shizitu"></p><p>即便建出来这样一张图，还是无法想到怎么做（可以把每个车子拆出来建图用带花树，这里先不讨论这种）。这张图也不是二分图，<strong>但是我们可以通过删去两条边使之变成一张二分图，并且着两条边可以100*100地取枚举</strong>，总时间复杂度就是O(Cx100x100)，C是八个点的网络流的复杂度，应该不会很大。如下图删去这两条边：</p><p><img src="/img/duoxiao/erfentu.jpg" alt="erfentu"></p><p>代码：（用了hjt的网络流板子）</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;#include&lt;queue&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);template&lt;class T&gt;inline void read(T &amp;x){x=0;char o,f=1;while(o=getchar(),o&lt;48)if(o==45)f=-f;do x=(x&lt;&lt;3)+(x&lt;&lt;1)+(o^48);while(o=getchar(),o&gt;47);x*=f;}#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)#define INF 0x3f3f3f3fconst int N = 114;struct FLOW{struct edge{int to,w,nxt;};vector&lt;edge&gt; a; int head[N],cur[N];int n,s,t;queue&lt;int&gt; q; bool inque[N];int dep[N];void ae(int x,int y,int w){ // add edgea.push_back({y,w,head[x]});head[x]=a.size()-1;}bool bfs(){ // get dep[]fill(dep,dep+n,INF); dep[s]=0;copy(head,head+n,cur);q=queue&lt;int&gt;(); q.push(s);while(!q.empty()){int x=q.front(); q.pop(); inque[x]=0;for(int i=head[x];i!=-1;i=a[i].nxt){int p=a[i].to;if(dep[p]&gt;dep[x]+1 &amp;&amp; a[i].w){dep[p]=dep[x]+1;if(inque[p]==0){inque[p]=1;q.push(p);}}}}return dep[t]!=INF;}int dfs(int x,int flow){ // extendint now,ans=0;if(x==t)return flow;for(int &amp;i=cur[x];i!=-1;i=a[i].nxt){int p=a[i].to;if(a[i].w &amp;&amp; dep[p]==dep[x]+1)if((now=dfs(p,min(flow,a[i].w)))){a[i].w-=now;a[i^1].w+=now;ans+=now,flow-=now;if(flow==0)break;}}return ans;}void init(int _n){n=_n+1; a.clear();fill(head,head+n,-1);fill(inque,inque+n,0);}int solve(int _s,int _t){ // return max flows=_s,t=_t;int ans=0;while(bfs())ans+=dfs(s,INF);return ans;}}flow;void add_edge(int x,int y,int w){flow.ae(x,y,w),flow.ae(y,x,0);}inline int mo(int x){return (x+4-1)%4+1;}int cnt[10][6];int c[10][6];int l[10],r[10],al[10],ar[10];void solve(){    rep(i,1,4)        rep(j,1,4) cin&gt;&gt;c[i][j];    rep(i,1,4){        cnt[i][1] = c[i][mo(i+1)];//左走        cnt[i][2] = c[i][mo(i+2)];//前走        cnt[i][3] = c[i][mo(i-1)];//右走    }    int res = 0;    rep(i,1,4) res = max(res,cnt[i][3]);    l[1] = cnt[1][1],l[2] = cnt[2][1];    l[3] = cnt[3][2],l[4] = cnt[4][2];    r[1] = cnt[1][2],r[2] = cnt[2][2];    r[3] = cnt[3][1],r[4] = cnt[4][1];    int s=99,t=100;    int fres = INF;    int eg1 = min(l[1],l[4]),eg2 = min(r[2],r[3]);    int tot = 0;//总点权    rep(i,1,4) ar[i]=r[i],al[i]=l[i],tot+=r[i]+l[i];    //cout&lt;&lt;"tot"&lt;&lt;tot&lt;&lt;endl;    rep(i,0,eg1){        al[1]=l[1]-i;        al[4]=l[4]-i;        rep(j,0,eg2){            flow.init(101);//s=1,t=100;            ar[2]=r[2]-j;            ar[3]=r[3]-j;            rep(i,1,4){                add_edge(s,i,al[i]);//s to l                add_edge(4+i,t,ar[i]);//r to t                add_edge(i,4+i,INF);//同侧                add_edge(i,4+mo(i+2),INF);//对面            }            add_edge(2,5,INF);            add_edge(3,8,INF);            int tmp;            fres = min(fres,tot-flow.solve(s,t)-i-j);        }    }    cout&lt;&lt;max(res,fres)&lt;&lt;endl;}int main(){    int z;    cin&gt;&gt;z;    while(z--) solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 题解 </tag>
            
            <tag> 做题记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于欧拉降幂</title>
      <link href="2021/07/19/euler/"/>
      <url>2021/07/19/euler/</url>
      
        <content type="html"><![CDATA[<h1 id="欧拉降幂"><a href="#欧拉降幂" class="headerlink" title="欧拉降幂"></a>欧拉降幂</h1><p>起因是因为牛客多校的一个数论题，需要计算组合数作为次数，即$a^{C_x^t}\mod p$,我在计算C的时候用p作为模数，导致了结果的错误。</p><p>洛谷欧拉降幂模板题：</p><p><a href="https://www.luogu.com.cn/problem/P5091">P5091 【模板】扩展欧拉定理 - 洛谷 | 计算机科学教育新生态 (luogu.com.cn)</a></p><h2 id="欧拉定理：要求a和p互质"><a href="#欧拉定理：要求a和p互质" class="headerlink" title="欧拉定理：要求a和p互质"></a>欧拉定理：要求a和p互质</h2><p>证明：<a href="https://oi-wiki.org/math/euler/">欧拉函数 - OI Wiki (oi-wiki.org)</a></p><p>$a^{\phi(p)} \mod p = 1$</p><p>则有$a^b \mod p=a^{b\mod\phi(p) }$</p><h2 id="扩展欧拉降幂：不要求a和p互质，按照b和-phi-p-比较分两类"><a href="#扩展欧拉降幂：不要求a和p互质，按照b和-phi-p-比较分两类" class="headerlink" title="扩展欧拉降幂：不要求a和p互质，按照b和$\phi(p)$比较分两类"></a>扩展欧拉降幂：不要求a和p互质，按照b和$\phi(p)$比较分两类</h2><p>$a^b \mod p = a^{b}(b&lt;\phi(p))$</p><p>$a^b \mod p = a^{(b\mod\phi(p))+\phi(p)}(b&lt;\phi(p))$</p><p>故我用一个flag来记录b是否有超过phi(p)的情况，来判断最后是否在次数上加上phi(p)</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;cmath&gt;using namespace std;#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)ll a,b,med;#define int llll qpow(ll d,ll c){//快速幂    ll res = 1;    while(c){        if(c&amp;1) res=res*d%med;        d=d*d%med;c&gt;&gt;=1;    }return res;}int euler_phi(int n){    int sqr = sqrt(n+0.5);    int res = n;     for(int i=2;i&lt;=sqr;i++){        if(n%i==0){//找到一个质因子             res = res/i*(i-1);//先除后乘，防止越界             while(n%i==0) n/=i;//把这个因子从n中消除掉         }    }    if(n&gt;1) res = res/n*(n-1);//大于sqrt的因子最多只有一个     return res; }string s;signed main(){    cin&gt;&gt;a&gt;&gt;med;    ll phi = euler_phi(med);    cin&gt;&gt;s;    int siz = s.size();    b = 0;    int flag = 0;    rep(i,0,siz-1){        b = b*10;        b = (b+s[i]-'0');        if(b&gt;=phi) flag = 1;        b%=phi;    }    cout&lt;&lt;qpow(a,b+flag*phi)&lt;&lt;endl;}//P5091 【模板】扩展欧拉定理//https://www.luogu.com.cn/problem/P5091<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>题解里给的数据加强版，a和m的范围变成了1e12,需要用龟速乘</p><p><a href="https://www.luogu.com.cn/problem/U55950">U55950 【模板】扩展欧拉定理 - 洛谷 | 计算机科学教育新生态 (luogu.com.cn)</a></p><h2 id="龟速乘"><a href="#龟速乘" class="headerlink" title="龟速乘"></a>龟速乘</h2><p>如果x和y直接相乘会爆ll，则可以使用龟速乘。</p><p>看代码就很容易理解，即把y按二进制拆分，从低位往高位移动，每次把x乘上2再取模。</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">ll gui_mul(ll x,ll y){//龟速乘    ll res = 0;    while(y!=0){        if(y&amp;1)res+=x,res%=med;        x+=x,x%=med;//y的位数每次往上,对应的权值*2        y&gt;&gt;=1;    }    return res;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结合龟速乘后的完整代码：</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;cmath&gt;using namespace std;#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)ll a,b,med;#define int llll gui_mul(ll x,ll y){//龟速乘    ll res = 0;    while(y!=0){        if(y&amp;1)res+=x,res%=med;        x+=x,x%=med;        y&gt;&gt;=1;    }    return res;}ll qpow(ll d,ll c){//快速幂    ll res = 1;    while(c){        if(c&amp;1) res=gui_mul(res,d);        d=gui_mul(d,d);c&gt;&gt;=1;    }return res;}int euler_phi(int n){    int sqr = sqrt(n+0.5);    int res = n;     for(int i=2;i&lt;=sqr;i++){        if(n%i==0){//找到一个质因子             res = res/i*(i-1);//先除后乘，防止越界             while(n%i==0) n/=i;//把这个因子从n中消除掉         }    }    if(n&gt;1) res = res/n*(n-1);//大于sqrt的因子最多只有一个     return res; }string s;signed main(){    cin&gt;&gt;a&gt;&gt;med;    ll phi = euler_phi(med);    cin&gt;&gt;s;    int siz = s.size();    b = 0;    int flag = 0;    rep(i,0,siz-1){        b = b*10;        b = (b+s[i]-'0');        if(b&gt;=phi) flag = 1;        b%=phi;    }    cout&lt;&lt;qpow(a,b+flag*phi)&lt;&lt;endl;}//P5091 【模板】扩展欧拉定理//https://www.luogu.com.cn/problem/P5091<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法学习笔记 </tag>
            
            <tag> 数论 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>整了个NAS和DDNS</title>
      <link href="2021/06/12/nas/"/>
      <url>2021/06/12/nas/</url>
      
        <content type="html"><![CDATA[<h2 id="大致步骤"><a href="#大致步骤" class="headerlink" title="大致步骤"></a>大致步骤</h2><p>注意：下面参考了很多个帖子，但是知识用到了说明说到的部分，其他部分不要和他们搞一样就行了。</p><p>我的openwrt已经装好ddns插件和luci-app-ddns了。一开始我用的aliddns，但是似乎有点问题，不能正常运行，而且日志文件也没有任何输出，之后就换了普通的ddns插件。</p><p>1.购买了阿里的域名，并且根据这个视频的方法创建了用户，添加了域名解析权限，并且声称accesskey。</p><p><a href="https://www.bilibili.com/video/BV1n741147yj">OpenWrt软路由设置阿里云DDNS动态域名解析教程_哔哩哔哩_bilibili</a></p><p>2.设置端口转发，就是把内网中某个地址下的某个端口，映射到公网ip的某个端口上，在openwrt的防火墙设置里可以设置端口转发。</p><p><a href="https://www.bilibili.com/video/BV1sb41157Lk">【视频教程】手把手教你用OpenWRT路由器进行端口转发_哔哩哔哩_bilibili</a></p><p>3.在openwrt设置ddns，在概况里添加ipv4的动态dns，看公网ip在哪个接口里，我的电信闪讯是在wan口，把域名设置一下即可，记得设置一下子域名，形如nas.example.com中的nas，这个子域名到时候是要填在阿里云控制台的域名解析里面的。</p><p><a href="https://post.smzdm.com/p/a6lrn3l0/">我的软路由折腾之旅 篇四：Openwrt by Lean搞定花生壳+3322 DDNS，洗白黑群晖远程/定时唤醒_NAS存储_什么值得买 (smzdm.com)</a></p><p>4.之后在阿里云控制台的域名解析中创建一个A记录，这里的公网ip可以乱设置，因为之后路由器那边会把正确的ip更新上来。很遗憾阿里的频率最快也只能设置10分钟。</p><p>5.这里有一个挺好看的视频，讲了由于阿里云更新比较慢，有时候可能需要几个小时，所以在每天晚上定一个时间来重启路由器，可以尽量把这个公网ip更换的时间放在晚上睡觉后。</p><p><a href="https://www.zhihu.com/zvideo/1254589422701576192">(1 条消息) 软路由玩6年 OPENWRT设置 插件哪坑哪好及DDNS设置 - 知乎 (zhihu.com)</a></p>]]></content>
      
      
      <categories>
          
          <category> 计算机知识 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 捡垃圾 </tag>
            
            <tag> Openwrt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客新主题</title>
      <link href="2021/05/18/new-theme/"/>
      <url>2021/05/18/new-theme/</url>
      
        <content type="html"><![CDATA[<h1 id="新主题"><a href="#新主题" class="headerlink" title="新主题"></a>新主题</h1><h3 id="github"><a href="#github" class="headerlink" title="github"></a>github</h3><p><a href="https://github.com/blinkfox/hexo-theme-matery">https://github.com/blinkfox/hexo-theme-matery</a></p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>近期CF做题记录</title>
      <link href="2021/04/26/zuoti/"/>
      <url>2021/04/26/zuoti/</url>
      
        <content type="html"><![CDATA[<h1 id="近期CF做题记录："><a href="#近期CF做题记录：" class="headerlink" title="近期CF做题记录："></a>近期CF做题记录：</h1><h2 id="梗概"><a href="#梗概" class="headerlink" title="梗概"></a>梗概</h2><p>主要包括一些近期写的CDE题，大致记录一下，以便之后翻阅复习，也作记录激励自己写题。</p><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><h3 id="【物品冲突】"><a href="#【物品冲突】" class="headerlink" title="【物品冲突】"></a>【物品冲突】</h3><ul><li>【暴力】Edu104E 很有cf风格的一题，四种类型物品，每种类型物品各选一个，有的物品u和上一种的v间有冲突，问四种物品各选一个的最小花费。</li></ul><p><a href="https://codeforces.com/contest/1487/problem/E">https://codeforces.com/contest/1487/problem/E</a></p><p>冲突数量有限，所以排序后直接暴力即可，本以为会T，其实因为冲突只有2e6，复杂度并不会炸。</p><h3 id="【网格挖槽联通】"><a href="#【网格挖槽联通】" class="headerlink" title="【网格挖槽联通】"></a>【网格挖槽联通】</h3><ul><li>【构造,思维】CF706D 构造，在n*m的网格中有几个位置被挖空，这些位置不会有同边或者同角，问怎么再挖几个空，让所有空槽联通，并且只有一条路，即成为树形</li></ul><p><a href="https://codeforces.com/contest/1496/problem/E">https://codeforces.com/contest/1496/problem/E</a></p><p>在j=3k+2的列上全部挖空，每个这种“走廊”中间刚好隔2，这时候只需要每两个“走廊”挖一条小路即可，如果m%3==1，那么最后空出来的两列在左边有走廊，右边没有，需要特殊讨论！</p><h3 id="【网格涂色连线最优解】"><a href="#【网格涂色连线最优解】" class="headerlink" title="【网格涂色连线最优解】"></a>【网格涂色连线最优解】</h3><ul><li>【差分】CF578D n*n网格填充黑白两种颜色，可以选择把一个k的正方形全部填成白色，问怎么样选这个正方形使得网格中的白色行/列数量最多</li></ul><p><a href="https://codeforces.com/contest/1200/problem/D">https://codeforces.com/contest/1200/problem/D</a></p><p>对每行找出最先出现黑色空缺minn和最后出现黑色空缺maxx，然后想象用一个k*k的正方形去“套住”这条黑线时，哪些是可以选择的左上角，在可以的位置上通过二维差分数组全部加上1，最后看所有位置上最大的数值，就是白色k方块的最佳位置</p><h3 id="【已涂色矩阵求满足要求子矩阵数量】"><a href="#【已涂色矩阵求满足要求子矩阵数量】" class="headerlink" title="【已涂色矩阵求满足要求子矩阵数量】"></a>【已涂色矩阵求满足要求子矩阵数量】</h3><ul><li>【DP】CF567C (这题不看图有点难懂) 有n*m的网格，每个cell有一种颜色(a to z表示26种不同颜色)，问图中有几个子矩形可以组成“国旗”？有三条高度相同的横着的色带的矩形被定义为国旗</li></ul><p><a href="https://codeforces.com/contest/118C1/problem/C">https://codeforces.com/contest/118C1/problem/C</a></p><p>用dp写：维护两个数组,$dp_{i,j}$表示以i,j为左下角可以组成几个国旗，$top_{i,j}$表示(i,j)格的颜色往上可以延伸到多长。</p><p>先O(nm)预处理出top数组，然后dp的时候，每到一个格子(i,j)，先往上走检查是否有三条高度相同的色带。（注意一开始在这里wa了，因为第三段颜色的长度其实不需要等于第一种颜色的长度，只需要大于等于即可！）如果满足这个条件，那么这里可以形成宽度为1的国旗即$dp_{i,j}$可以等于1，之后再检查和(i,j-1)位置是否可以拼接（三段颜色完全相同),如果可以，那么$dp_{i,j}=dp_{i,j}+dp_{i,j-1}$。答案就是所有dp的和。</p><h3 id="【重排序列使极小值最多】"><a href="#【重排序列使极小值最多】" class="headerlink" title="【重排序列使极小值最多】"></a>【重排序列使极小值最多】</h3><ul><li>【构造,思维】CF671D2 给n个冰球(1e5)，分别有ai的价格（可能相同），要求找到一种排列，使得满足a[i-1]&gt;a[i]&lt;a[i+1]的点尽可能多，输出这种最优的排列。</li></ul><p><a href="https://codeforces.com/contest/1419/problem/D2">https://codeforces.com/contest/1419/problem/D2</a></p><p>如果是D1的条件，即ai价格各不相同，那么就是一大一小地输出。但是这题D2是可能相同的，所以我们想：让接近的数字距离尽可能远。所以我们先把ai排序，然后依次把ai填入数组，先填偶数再填奇数。</p><h3 id="【n的除数排环使相邻不互质】"><a href="#【n的除数排环使相邻不互质】" class="headerlink" title="【n的除数排环使相邻不互质】"></a>【n的除数排环使相邻不互质】</h3><ul><li>【构造,数论】CF671E 给一个数字n，列出n的所有大于1的除数，之后把这些除数按一定顺序排成环，要求两个相邻的数字都不互质，还提供一种操作，就是在相邻的两个数字$a_i,a_j$中插入他们$lcm(a_i,a_j)$，问最少进行多少次操作可以使得环满足条件？要求输出初始的环排列及操作次数。除数的数量题目保证不超过2e5</li></ul><p><a href="https://codeforces.com/contest/1419/problem/E">https://codeforces.com/contest/1419/problem/E</a></p><p>首先有一个结论：只有n是由两个不同质因子组成时，需要一次操作。先$O(n\sqrt{n})$找出n所有的质因子及出现次数，并编号，另外的情况，依照类似如下的排列搭出“骨架”：$1,12,2,23,3,34,4,41$，并把这些数字存进map中防止之后重复，我们发现,可以把所有含有prim[1]的除数都放在$prim_1,prim_1\cdot prim_2$之间，同理可以把所有含有prim[2]的除数放在$prim_2,prim_2\cdot prim_3$之间。我们知道除数的数量不超过2e5,因此直接通过dfs找到所有除数(似乎也可以不用dfs,但是我的方法写dfs比较方便），并且在搜索过程中随便记录它的一个因子i，推入对应的vector[i]中，最后结果就是$1,vec[1],12,2,vec[2],23,3,vec[3],34,4,vec[4],41$</p><h3 id="【排列的逆序数】"><a href="#【排列的逆序数】" class="headerlink" title="【排列的逆序数】"></a>【排列的逆序数】</h3><ul><li>【数学】DBPK2-1004 给数字n，为长度为n的排列初始为$1,2,3,4….n$即字典序最小的排列，m次查询，每次查询让n的排列按字典序后移$x_i$次，并输出移动后的逆序数。</li></ul><p><a href="https://www.dingbacode.com/contest/242/problem/1004">https://www.dingbacode.com/contest/242/problem/1004</a></p><p>研究规律后发现，用一个特殊的进制来表示逆序数的一种排列，比如$1,2,3,4$为0000，$1,2,4,3$为0010，$1,3,2,4$为0100，这个是怎么来的呢？我的这种进制是，第i位上逢i进1！这样一来排列的逆序数即为这个进制数每个位上的数字之和：比如$1,3,4,2$位0110，则逆序数为$1+1=2$。非常神奇</p><ul><li>【数学,DP】DBPK5-1001 求长度为n的排列的所有逆序对之和对998244353取模的值.</li></ul><p><a href="https://www.dingbacode.com/contest/289/problem/1001">https://www.dingbacode.com/contest/289/problem/1001</a></p><p>结合之前对于逆序数的结论，我推得公式：$dp[i]=i\cdot dp[i-1]+\prod_{i=1}^{i-1}i\cdot\sum_{i=1}^{i-1}i$</p><h3 id="【快速计数二进制中的1】【序列多少对ai-aj异或和二进制中1为偶数】"><a href="#【快速计数二进制中的1】【序列多少对ai-aj异或和二进制中1为偶数】" class="headerlink" title="【快速计数二进制中的1】【序列多少对ai,aj异或和二进制中1为偶数】"></a>【快速计数二进制中的1】【序列多少对ai,aj异或和二进制中1为偶数】</h3><ul><li>【数学】 DPPK5 给序列a，问有多少对ai和aj使得$a_i\oplus a_j$二进制上有偶数个1</li></ul><p><a href="https://www.dingbacode.com/contest/289/problem/1002">https://www.dingbacode.com/contest/289/problem/1002</a></p><p>对于第二个问题，只要求ai二进制中的1和aj二进制中的1数量和为偶数即可。但是如果用除二模二的方法会被卡，(1000ms/1500ms)所以有一种更快的求1数量的方法：这样每次必定能去掉二进制中的一个1！</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">while(hc){cnt+=1;    hc&amp;=(hc-1);}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="【map-lt-vector-xxx-gt-代替字典树】【分割序列使每段任两个积不是平方数】"><a href="#【map-lt-vector-xxx-gt-代替字典树】【分割序列使每段任两个积不是平方数】" class="headerlink" title="【map<vector,xxx>代替字典树】【分割序列使每段任两个积不是平方数】"></a>【map&lt;vector,xxx&gt;代替字典树】【分割序列使每段任两个积不是平方数】</h3><ul><li>【数学】 CF708E1 给序列a，不改变顺序分割成几段，要求每段中不能存在ai和aj使得$a_i\cdot a_j=x^2$即不能是</li></ul><p>一个平方数。问最少分割成几段。ai范围1e7。</p><p><a href="https://codeforces.com/contest/1497/problem/E1">https://codeforces.com/contest/1497/problem/E1</a></p><p>其实这题思路使好想的，在2-sqrt(1e7)之中只有500以内的质因数，ai为最多8个质因数相乘。所以我们拆分每个ai（唯一分解定理）并记录在哪些质数上的次数为奇数。之后用字典树或者这个map统计都行，遇到冲突后清空map并让res++.</p><h3 id="【按规则取完石子】"><a href="#【按规则取完石子】" class="headerlink" title="【按规则取完石子】"></a>【按规则取完石子】</h3><ul><li>【贪心】CF696D 有n堆石子(2e5)，每次操作可以从相邻的两堆石头里各取一个，还支持一种超级操作，在比赛开始前选择相邻的两堆石子进行交换，超级操作只能进行一次，问是否能把石头取完。</li></ul><p><a href="https://codeforces.com/contest/1474/problem/D">https://codeforces.com/contest/1474/problem/D</a></p><p>考虑如果没有超级操作怎么写：贪心地从1到n扫一遍，每次用前面的石头减在后面的石头上，如果前面的多于后面的，则失败。</p><p>核心：如果交换了i和i+1，那么1到i-1以及i+2到n是不受影响的，每次交换只要模拟这种两边凑过来只剩下中间的i和i+1的情况。</p><p>首先正着反着都扫一遍，处理出b和c两个数组(即从开头到当前节点贪心下来的情况)，如果扫到一个位置发现b[i]&gt;b[i+1],这时说明正着最多只能走到这里了，反过来同理。我们考虑所有可行的位置，交换a[i]和a[i+1],并且让a[i]-=b[i-1],a[i+1]-=c[i+2]（保证b中i-1和c中i+1是有效值），然后看a[i]和a[i+1]是否相等且大于0即可！</p><h3 id="【找坐标xy使离所有点距离-xi-xj-yi-yj-和最小】"><a href="#【找坐标xy使离所有点距离-xi-xj-yi-yj-和最小】" class="headerlink" title="【找坐标xy使离所有点距离|xi-xj|+|yi-yj|和最小】"></a>【找坐标xy使离所有点距离|xi-xj|+|yi-yj|和最小】</h3><ul><li>【思维】CF703B 给n(1e3)个点的xy坐标(1e9),找到合适的位置建立“博物馆”使得所有点到博物馆的距离（|xi-xj|+|yi-yj|)之和最小，问有多少个这样的点.</li></ul><p><a href="https://codeforces.com/contest/1486/problem/B">https://codeforces.com/contest/1486/problem/B</a></p><p>没错这是个b题，但是我确实不会写qaq。把所有点投射到x轴和y轴上的投影记录，排序后找中位数，如果有两个中位数ai和ai+1,那么在这[ai,ai+1]这个区间内的所有坐标都是可行的。同理找到y轴上的中位数，两个区间长度相乘即结果。</p><p>这题之所以做不出来，我觉得是总想用算法的角度去写，但是其实这是简单的从二维往一维上的转化的思想。我们把总距离拆分成x轴上的距离和y轴上的距离，这样便转化成了一维直线上的距离之和问题，对于这类问题我们就知道该用中位数了。</p><h3 id="【按规则构造树-最多能给多少点涂色】"><a href="#【按规则构造树-最多能给多少点涂色】" class="headerlink" title="【按规则构造树,最多能给多少点涂色】"></a>【按规则构造树,最多能给多少点涂色】</h3><ul><li>【dp,树】CF652D 给一种树叫RDB，1级的RDB只有一个点，RDB的等级每次往上升级一次时:</li></ul><p>如果点u没有孩子，则给它创造一个孩子；如果点u有一个孩子，则给它增加俩孩子；如果点u有三个孩子，则不动它。</p><img src="\img\zuoti\capture_20210324164919694.bmp" alt="capture_20210324164919694" style="zoom:50%;"><p>每组输入给一个n级的RDB，所有点都是绿色，每次可以选择一个“鸡爪”即一个点连着三个儿子且全部为绿色的子图，把整个“鸡爪”染黄，问最多能染黄多少个点。</p><p><a href="https://codeforces.com/contest/1369/problem/D">https://codeforces.com/contest/1369/problem/D</a></p><p>答：i级的树其实就是两颗i-2级树和一颗i-1级树组成。</p><p><img src="\img\zuoti\capture_20210324171647672.bmp" alt="capture_20210324171647672" style="zoom:33%;"><img src="\img\zuoti\capture_20210324172049435.bmp" alt="capture_20210324172049435" style="zoom:50%;"> </p><p>因此初步可得到递推式$cnt_i=2\cdot cnt_{i-2}+cnt_{i-1}$，另外还要维护一个top数组，表示i级树的根是否被使用，如果$top_{i-1}$和$top_{i-1}$都为0，则i级树的根和这三个点可以再构成一个鸡爪（最后一张图这样），且$top_i=1$。只需要一开始预处理即可。</p><h3 id="【三种木棍选两种构造矩形-使用完木棍总面积最大】"><a href="#【三种木棍选两种构造矩形-使用完木棍总面积最大】" class="headerlink" title="【三种木棍选两种构造矩形,使用完木棍总面积最大】"></a>【三种木棍选两种构造矩形,使用完木棍总面积最大】</h3><ul><li>【DP】EDU93D 给r,g,b三个数字表示三种类木棍的数量(&lt;=200)之后读入每种木棍的所有木棍。每次可以取两种不同颜色的木棍，以之为长宽构造一个矩形，重复直到木棍用尽，问最大的总面积。</li></ul><p><a href="https://codeforces.com/contest/1398/problem/D">https://codeforces.com/contest/1398/problem/D</a></p><p>用$O(n^3)$的动态规划写，每种木棍从大到小(仔细想一下从小到大也行)排序（因为后面总会有一些用不完的木棍，我们希望能优先使用长的木棍），方法有点类似背包的状态转移。我们用$dp_{i,j,k}$表示用前i个r木棍（全都使用），前j个g木棍，前k个b木棍构造的最大总长度。</p><p>我们知道这个dp中有很多状态都是达不到的，比如$dp_{1,0,0}$显然是达不到的状态</p><p>并且如果要求$dp_{i,j,k}$,我们不知道他是从$dp{i-1,j-1,k},dp_{i-1,j,k-1},dp_{i,j-1,k-1}$中的哪个拼接上来的。</p><p>因此我们用$dp{i,j,k}$往上去推$dp_{i+1,j+1,k}$等等。所以有转移方程：</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">if(i+1&lt;=c[1]&amp;&amp;j+1&lt;=c[2])    dp[i+1][j+1][k]=max(dp[i+1][j+1][k],dp[i][j][k]+(ll)a[1][i+1]*a[2][j+1]); if(i+1&lt;=c[1]&amp;&amp;k+1&lt;=c[3])    dp[i+1][j][k+1]=max(dp[i+1][j][k+1],dp[i][j][k]+(ll)a[1][i+1]*a[3][k+1]);if(j+1&lt;=c[2]&amp;&amp;k+1&lt;=c[3])    dp[i][j+1][k+1]=max(dp[i][j+1][k+1],dp[i][j][k]+(ll)a[2][j+1]*a[3][k+1]);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>想了一下为什么从小到大排序也是可以的：三种类型的木棍，一定有两种木棍是要全部使用的，也就是只有一种木棍可以不用完，而这些不用完的都是最小的那几个。</p><p>因为上面的状态转移方程，$dp_{0,0,123}$之类的都是0，从这个点往上dp推出的结果，也就是b类木棍前123个都不使用得到的结果。所以从小到大排序也是可以的。</p><h3 id="【旅行商问题-边费用为max-a-j-a-i-c-i-】"><a href="#【旅行商问题-边费用为max-a-j-a-i-c-i-】" class="headerlink" title="【旅行商问题,边费用为max(a[j]-a[i],c[i])】"></a>【旅行商问题,边费用为max(a[j]-a[i],c[i])】</h3><ul><li>【DP】CF712E 有n(1e5)个城市，每个城市有ai和ci两个属性，从城市i到城市j的费用为$max(c_i,a_j-c_i)$,问从任一城市开始经过所有城市再回到该城市所需要的最小费用。</li></ul><p><a href="https://codeforces.com/contest/1504/problem/E">https://codeforces.com/contest/1504/problem/E</a></p><p>我们观察这条式子：$cost(i,j)=max(c_i,a_j-c_i)$,可以知道$c_i$是逃不掉的，因此我们把c全部都先加到结果$res$上，这样每条边的费用就变成了$cost(i,j)=max(0,a_j-(a_i+c_i))$。</p><p>我们可以形象地把ai理解成楼房的高度，ci理解成这个楼房的梯子高度，我们在楼房i上时，如果楼房高度和梯子高度之和高于楼房j的高度，则可以免费从i爬到j，否则则需要补上这些高度的差。即从楼高+梯子较高的楼房爬到低的是不需要费用的。</p><p>以a为第一关键字，c为第二关键字对房子排序。</p><p>因此我们分两部分：</p><p>第一部分先从最低的楼层爬到最高的楼层（中间使费用尽量小）。</p><p>第二部分只需要把没爬过的楼房从高往低排列，最后走回最低的楼房即可（这部分不需要任何费用）。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int now = ac[1].ft+ac[1].sd;//now是当前房子+梯子的高度   //之后每步的消耗看作max(0,aj-(ai+ci))   rep(i,2,n){       if(ac[i].ft&gt;=now)//通过梯子上去的情况           res += ac[i].ft-now;       //不加的这些是达到最高之后访问的       //如果这个楼可以使now更高，也应该在中间免费经过       now = max(now,ac[i].ft+ac[i].sd);   }   //第二部分经过的楼全部免费<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="【k个字母构造字符串使-a-i-a-j-amp-amp-a-i-1-a-j-1-的ij尽可能少】"><a href="#【k个字母构造字符串使-a-i-a-j-amp-amp-a-i-1-a-j-1-的ij尽可能少】" class="headerlink" title="【k个字母构造字符串使(a[i]==a[j]&amp;&amp;a[i+1]==a[j+1])的ij尽可能少】"></a>【k个字母构造字符串使(a[i]==a[j]&amp;&amp;a[i+1]==a[j+1])的ij尽可能少】</h3><ul><li>【构造】EDU107D 给两个数字n和k，用字母表的前k个字母构造一个长度为n的字符串，每有一对i!=j使得$a_i=a_j,a_{i+1}=a_{j+1}$，则该串的cost+1,问最小cost的构造为？</li></ul><p><a href="https://codeforces.com/contest/1511/problem/D">https://codeforces.com/contest/1511/problem/D</a></p><p>先考虑如何构造完全无费用地字符串？所有字母地两两组合都能够出现，一共有k*k种组合，这些组合都出现则可以构成长度为$k^2+1$的串（考虑三个字母包含两种组合，四个字母包含3种组合以此类推）.</p><p>考虑这种构造:$aabacad,bbcbd,ccd,d$，即对于每个字母，先输出一个它自己，再加上它和排后面的各种字母的组合。</p><h2 id="数论"><a href="#数论" class="headerlink" title="数论"></a>数论</h2><h3 id="【统计x的所有除数】【求c·lcm-a-b-d·gcd-a-b-x​的ab对数】"><a href="#【统计x的所有除数】【求c·lcm-a-b-d·gcd-a-b-x​的ab对数】" class="headerlink" title="【统计x的所有除数】【求c·lcm(a,b)-d·gcd(a,b)=x​的ab对数】"></a>【统计x的所有除数】【求c·lcm(a,b)-d·gcd(a,b)=x​的ab对数】</h3><ul><li>【数论】EDU106D 给$c,d,x$三个1e7内的数，求有多少对$(a,b)$满足$c\cdot lcm(a,b)-d\cdot gcd(a,b)=x$</li></ul><p><a href="https://codeforces.com/contest/1499/problem/D">https://codeforces.com/contest/1499/problem/D</a></p><p>把式子变形，设$a=L\cdot gcd(a,b),b=R\cdot gcd(a,b)$则将式子变形得到$L\cdot R=\frac{\frac{x}{gcd(a,b)}+d\cdot gcd(a,b)}{c}$，观察式子可以发现，x必须是gcd的倍数，因此我们$O(\sqrt{x})$枚举x的所有除数(比赛的时候以为这个是o(x)的),之后右边的值就确定了.</p><p>有了$L\cdot R$的值，然后设$zyz[L\cdot R]$为y的所有质因子数量,先用类似埃式筛的方法筛出每个数字的有多少种不同的质因子。那么这种情况下$(L,R)$有$2^(zyz[L\cdot R])$对。</p><p>哦对，关于统计x所有除数的方法：非常基础，但是我忘掉了</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">rep(i,1,sqr){    if(x%i==0){        vec.push_back(i);        if(i*i!=x) vec.push_back(x/i);    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="【给一序列-可插入2-ai-aj-问能否得到k】"><a href="#【给一序列-可插入2-ai-aj-问能否得到k】" class="headerlink" title="【给一序列,可插入2*ai-aj,问能否得到k】"></a>【给一序列,可插入2*ai-aj,问能否得到k】</h3><ul><li>【数论,裴蜀定理】CF698D 给一个序列a，长度2e5，每次操作可以选定序列中的任意两个数字x,y.可以把2x-y插入序列中，问能不能通过这种操作拼凑出k（1e18）。</li></ul><p><a href="https://codeforces.com/problemset/problem/1477/A">https://codeforces.com/problemset/problem/1477/A</a></p><p>裴蜀定理：有非0数a和b，则一定有ax+by=gcd(a,b)一定有整数解.(也是exgcd的前置)</p><p>我们把$new = 2a-b$拆分为$new = a+(a-b)$ ，设$a=c+(c-d)$，那么递归可以得到$new = c+(c-d)+(a-b)$,后面也可以再来一堆(x-y),所以我们把问题转化成了：</p><p>是否能用$\sum a_i-a_j=k-a_x$。（i,j随便取）。再结合之前的裴蜀定理，我们先算出所有$a_i-a_{i-1}$的gcd，再枚举$a_x$，看$k-a_x$能否被gcd整除即可，复杂度$O(n)$.</p><h3 id="【查询l-r区间最少要拆分成几段使子串各元素互质】"><a href="#【查询l-r区间最少要拆分成几段使子串各元素互质】" class="headerlink" title="【查询l,r区间最少要拆分成几段使子串各元素互质】"></a>【查询l,r区间最少要拆分成几段使子串各元素互质】</h3><ul><li>【数论,倍增】CF717D 给1e5长的序列(ai也是1e5范围)进行1e5次询问，每次给l和r，问最少需要把l到r这一区间的序列拆分成几段子串，才能使每段的LCM=这段元素的积.</li></ul><p><a href="https://codeforces.com/contest/1516/problem/D">https://codeforces.com/contest/1516/problem/D</a></p><p>很久没有写倍增了，因此这题记录的详细一点。</p><p>首先可以确定，要满足LCM=这段元素的积，即这段元素的所有数字必须两两互质。我们用质数筛筛出1到1e5的质数推入质数表中，然后对每个数字进行质因数分解，我们需要维护两个数组：</p><ul><li>pre[x]表示质数x上一次出现的位置(因为是倒着找的，所以pre[x]一定是比当前位置往后的)</li><li>rb[i]表示i位置往右边数，最近是哪个数字和他不互质</li></ul><p>我们知道如果要找到当前位置i的下一段，即直接跳转到rb[i]即可。我们倒着做一次循环：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">if(a[i]%prm[j]==0){    while(a[i]%prm[j]==0) a[i]/=prm[j];    rb[i] = min(rb[i],pre[prm[j]]);    pre[prm[j]] = i;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>即一个数字有多个质因子组成时，需要找到这些质因子x中pre[x]最小的赋值给rb[i];其次因为在当前位置i上找到了新的包含质数prm[j]的位置，所以也需要更新pre[prm[j]]。</p><p>我们知道rb[i]是i位置经过一次跳转后到达的位置，使用倍增法，我们需要对i位置找出跳转2,4,8,16….次的位置，放在bz[i],[j]中，表示位置i经过$2^j$次跳转后到达的位置。</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">rep(i,1,n) bz[i][0] = rb[i];bz[n+1][0] = n+1;rep(j,1,20){    bz[n+1][j] = n+1;    rep(i,1,n){        bz[i][j] = bz[bz[i][j-1]][j-1];    }}   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样一来，我们读入l时，从大往小检测2的倍数，弱没有超过r则执行这次跳跃，并且统计结果。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cin&gt;&gt;l&gt;&gt;r;int cnt = 1;repb(j,20,0){//从大往小找    if(bz[l][j]&lt;=r){//如果执行2^j次跳跃不会越界,则执行        cnt+=(1&lt;&lt;j);        l = bz[l][j];    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这题还有一些边界需要赋初值，比如rb[n+1]=n+1,所有pre[x]=n+1,以及bz[n+1],[0]=n+1。</p>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 题解 </tag>
            
            <tag> 做题记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多元线性回归——波士顿房价问题 学习笔记</title>
      <link href="2021/04/09/linear/"/>
      <url>2021/04/09/linear/</url>
      
        <content type="html"><![CDATA[<h1 id="多元线性回归——波士顿房价问题学习笔记"><a href="#多元线性回归——波士顿房价问题学习笔记" class="headerlink" title="多元线性回归——波士顿房价问题学习笔记"></a>多元线性回归——波士顿房价问题学习笔记</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>影响波士顿房价的一共有12个参数，我们假设房价$Y=x_1\cdot w_1+x_2\cdot w_2\dots +x_{12}\cdot w_{12}+b$,要通过数据求这十二个参数的权重和$b$。</p><h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><p>损失函数$loss$越大表示预测的越不准确，$loss = (y-pred)^2$即预测值与实际值的均方误差。</p><h2 id="对矩阵求导"><a href="#对矩阵求导" class="headerlink" title="对矩阵求导"></a>对矩阵求导</h2><p>这一部分参考<a href="https://zhuanlan.zhihu.com/p/137713040">https://zhuanlan.zhihu.com/p/137713040</a></p><p>类比之前的线性回归问题，我们设房价为$Y=X\beta$其中X是每个参数$x_i$组成的矩阵,$\beta$是$w_i$的矩阵</p><p>我们要求残差平方和$Q=(X\beta-Y)(X\beta-Y)^T$对$\beta$的导数，</p><p>有$\frac{\partial X^TY\beta}{\partial \beta}=Y^TX$(不会用Latex打矩阵，证明部分在知乎有，这里先不记录了)</p><p>因此$(X\beta-Y)(X\beta-Y)^T=(\beta^TX^T-Y^T)(X\beta-Y)=\beta^TX^TX\beta-\beta^TX^TY-Y^TX\beta+Y^TY$</p><p>求导后得到$2X^TX\beta-2X^TY$</p><h2 id="参数求解"><a href="#参数求解" class="headerlink" title="参数求解"></a>参数求解</h2><p>我们要找导数为0的极值点，因此设$\beta$满足$2X^TX\beta-2X^TY$</p><p>变形得到$\beta=(X^TX)^{-1}X^TY$即可得到$\beta$</p><p>但是方法有两个问题：</p><ul><li>X可能没有逆矩阵，结果不够精确</li><li>唯独多，矩阵求逆，矩阵相乘耗费大量时间</li></ul><h2 id="梯度下降法求解"><a href="#梯度下降法求解" class="headerlink" title="梯度下降法求解"></a>梯度下降法求解</h2><p>用矩阵求导的方法得梯度$\frac{\partial Q}{\partial \beta}=2X^TX\beta-2X^TY$</p><p>因此带入梯度下降公式：$\theta_i+1=\theta_i-\alpha\cdot2X^T(X\beta-Y)$得到下一步的点坐标</p><h2 id="一些概念记录"><a href="#一些概念记录" class="headerlink" title="一些概念记录"></a>一些概念记录</h2><p>**epoch(时期):**一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一个epoch。</p><p>**batch(批):**将训练样本分成多个batch，一次只选一个部分。</p><p>**随机梯度下降:**批量梯度下降的每一步都把整个数据集载入进来运算，比较小号时间和内存，随机梯度下降每次随机采样一批样本来估计当前梯度。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>代码部分看的这个视频 <a href="https://www.bilibili.com/video/BV1iJ41127cw">https://www.bilibili.com/video/BV1iJ41127cw</a></p><h3 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_bostonBoston <span class="token operator">=</span> load_boston<span class="token punctuation">(</span><span class="token punctuation">)</span>X <span class="token operator">=</span> Boston<span class="token punctuation">.</span>dataY <span class="token operator">=</span> Boston<span class="token punctuation">.</span>target<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>从sklearn里面导入波士顿房价的数据集。</p><h3 id="划分样本数据"><a href="#划分样本数据" class="headerlink" title="划分样本数据"></a>划分样本数据</h3><h4 id="train-test-split函数"><a href="#train-test-split函数" class="headerlink" title="train_test_split函数"></a>train_test_split函数</h4><p>参考这个文章<a href="https://zhuanlan.zhihu.com/p/248634166">https://zhuanlan.zhihu.com/p/248634166</a></p><p><strong>完整模板：</strong></p><p>train_X,test_X,train_y,test_y = train_test_split(train_data,train_target,test_size=0.3,random_state=5)</p><p><strong>参数解释：</strong></p><p>train_data：样本特征集</p><p>train_target：样本标签</p><p>test_size：0.1之间的浮点数，表示测试集占总样本的占比，如果是整数就是样本数量</p><p>random_state：随机数种子</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LinearRegressionX_train<span class="token punctuation">,</span>X_test<span class="token punctuation">,</span>Y_train<span class="token punctuation">,</span>Y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>X<span class="token punctuation">,</span>Y<span class="token punctuation">,</span>test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>random_state<span class="token operator">=</span><span class="token number">865</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>参考<a href="https://zhuanlan.zhihu.com/p/71196742">https://zhuanlan.zhihu.com/p/71196742</a></p><p>我其实比较震惊，线性回归居然直接调用库函数就可以用了。</p><h4 id="LinearRegression函数"><a href="#LinearRegression函数" class="headerlink" title="LinearRegression函数"></a>LinearRegression函数</h4><p><strong>完整模板:</strong></p><p>LinearRegression(fit_intercept=True,normalize=False,copy_X=True,n_jobs=1)</p><p><strong>参数解释：</strong></p><p>fit_intercept：是否对训练数据进行中心化，默认为true(中心化：使得平均值为0)</p><p>normalize：是否对数据进行标准化处理，默认为true(标准化：将数据按比例缩放，使之落入一个小的特定区间)</p><p>copy_X：中心化，标准化后，是否把新数据覆盖到原数据上，默认为true</p><p>n_jobs：整型，默认为1</p><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">lin_reg <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>lin_reg<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>Y_train<span class="token punctuation">)</span><span class="token comment">#训练过程</span>lin_reg<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>deviation <span class="token operator">=</span> lin_reg<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span> <span class="token operator">-</span> Y_testdeviation<span class="token comment">#打印预测结果和真实值的差</span>RMSE <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>deviation<span class="token operator">*</span>deviation<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">102</span><span class="token comment">#均方根误差，越小表示预测结果越准确</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>以前没接触过python，之前还看了另外一个没用库的教程，里面还涉及标准化之类的好多细节，因为语言之类的都不熟悉故放弃了。跟着视频的讲解写完代码之后感觉到python有好多的库，似乎都不用了解内部的原理就可以拿过来用，而且很多语法都比较简短。</p>]]></content>
      
      
      <categories>
          
          <category> CNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Codeforces Round /#712 (Div. 2) E. Travelling Salesman Problem</title>
      <link href="2021/04/06/cf-712/"/>
      <url>2021/04/06/cf-712/</url>
      
        <content type="html"><![CDATA[<h1 id="题目地址"><a href="#题目地址" class="headerlink" title="题目地址"></a>题目地址</h1><p><a href="https://codeforces.com/contest/1504/problem/E">https://codeforces.com/contest/1504/problem/E</a></p><h1 id="题目大意"><a href="#题目大意" class="headerlink" title="题目大意"></a>题目大意</h1><p>有n(1e5)个城市，每个城市有ai和ci两个属性，从城市i到城市j的费用为$max(c_i,a_j-c_i)$,问从任一城市开始经过所有城市再回到该城市所需要的最小费用。</p><h1 id="解法"><a href="#解法" class="headerlink" title="解法"></a>解法</h1><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>我们观察这条式子：$cost(i,j)=max(c_i,a_j-c_i)$,可以知道$c_i$是逃不掉的，因此我们把c全部都先加到结果$res$上，这样每条边的费用就变成了$cost(i,j)=max(0,a_j-(a_i+c_i))$。</p><p>我们可以形象地把ai理解成楼房的高度，ci理解成这个楼房的梯子高度，我们在楼房i上时，如果楼房高度和梯子高度之和高于楼房j的高度，则可以免费从i爬到j，否则则需要补上这些高度的差。即从楼高+梯子较高的楼房爬到低的是不需要费用的。</p><h2 id="解"><a href="#解" class="headerlink" title="解"></a>解</h2><p>以a为第一关键字，c为第二关键字对房子排序。</p><p>因此我们分两部分：</p><p>第一部分先从最低的楼层爬到最高的楼层（中间使费用尽量小）。</p><p>第二部分只需要把没爬过的楼房从高往低排列，最后走回最低的楼房即可（这部分不需要任何费用）。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int now = ac[1].ft+ac[1].sd;//now是当前房子+梯子的高度   //之后每步的消耗看作max(0,aj-(ai+ci))   rep(i,2,n){       if(ac[i].ft&gt;=now)//通过梯子上去的情况           res += ac[i].ft-now;       //不加的这些是达到最高之后访问的       //如果这个楼可以使now更高，也应该在中间免费经过       now = max(now,ac[i].ft+ac[i].sd);   }   //第二部分经过的楼全部免费<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)#define INF 0x3f3f3f3fll gcd(ll a,ll b){ while(b^=a^=b^=a%=b); return a; }int n;typedef pair&lt;int,int&gt; pii;#define mkp make_pair#define ft first#define sd secondconst int MAXN = 114514;pii ac[MAXN];ll res;void solve(){    res = 0;    cin&gt;&gt;n;    rep(i,1,n) cin&gt;&gt;ac[i].ft&gt;&gt;ac[i].sd,res+=ac[i].sd;    sort(ac+1,ac+1+n);    int now = ac[1].ft+ac[1].sd;//now是当前房子+梯子的高度    //之后每步的消耗看作max(0,aj-(ai+ci))    rep(i,2,n){        if(ac[i].ft&gt;=now)//通过梯子上去的情况            res += ac[i].ft-now;        //不加的这些是达到最高之后访问的        //如果这个楼可以使now更高，也应该在中间免费经过        now = max(now,ac[i].ft+ac[i].sd);    }    //第二部分经过的楼全部免费    cout&lt;&lt;res&lt;&lt;endl;}int main(){    solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 题解 </tag>
            
            <tag> Codeforces </tag>
            
            <tag> DP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Codeforces Round /#700 D1. Painting the Array I</title>
      <link href="2021/02/16/cf-700d/"/>
      <url>2021/02/16/cf-700d/</url>
      
        <content type="html"><![CDATA[<h1 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h1><p>心碎的一场，一开始40分钟秒掉三题，结果后来fst掉两题（一题是忘了赋INF，另一题是&gt;0写成&gt;=0，这次pretest数据真的比以往弱太多！），掉了一百二十分差点下蓝，一朝回到解放前，心态都崩了。哭哭，近期都没心情打了</p><h1 id="题目地址"><a href="#题目地址" class="headerlink" title="题目地址"></a>题目地址</h1><p><a href="https://codeforces.com/contest/1480/problem/D1">https://codeforces.com/contest/1480/problem/D1</a></p><h1 id="题目大意"><a href="#题目大意" class="headerlink" title="题目大意"></a>题目大意</h1><p>给一个长n的序列，可以按顺序分成两组（要保留先后顺序的），求怎么样使两个序列的段数和最大？</p><p>说明：连续的同种数字算同一段，比如$[1,2,3]$有3段，$[1,1,2,2,3,3,3,3]$同样也是3段。</p><h1 id="解法"><a href="#解法" class="headerlink" title="解法"></a>解法</h1><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>我一开始也想到了用贪心，但是其实可以发现这题有很多需要讨论的地方。</p><ul><li>首先，当前数字放在特定的一组中，可以<strong>隔开</strong>之后来的数字使得段数增加，比如对序列$[1,2,3,2,2]$</li></ul><p>当前$A:[1]$和$B:[2]$，下一个来的为$3$。这个$3$一定要放在第二组才能使结果最大化即$A:[1,2]$和$B:[2,3,2]$</p><p>这里的重点在于：之后出现了连续的一段$[2,2]$对于连续的一段我们<strong>没得选</strong>,如果A或者B的尾部同样也有2，那么就<strong>会使答案损失1</strong>。</p><ul><li>因此，如果现在有一个数字要选择插入A还是B，我们需要依据A和B尾端的数字以及之后<strong>“连着的两个相同数字”</strong>进行讨论</li></ul><h2 id="解"><a href="#解" class="headerlink" title="解"></a>解</h2><p>预处理一个nxt数组,表示离当前位置i最近的**“连续两个相同数字”**是什么。</p><ul><li><p>如果两个序列其中一个的尾部和a[i]相同，那么肯定是要放到另一个序列中去的。</p></li><li><p>如果两个序列的尾部都不同于当前a[i]，那么就要依据nxt[i]和两个尾部进行讨论，如果序列A的尾部和nxt[i]相同，那么就放在序列A上，反之则放在B上。</p></li></ul><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>有些细节上还需要讨论以下，比如a中一段长段的末尾和单个数字是一样讨论的等等。</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)#define INF 0x3f3f3f3fint n;const int MAXN = 114514;int a[MAXN],nxt[MAXN];void solve(){    cin&gt;&gt;n;    rep(i,1,n) cin&gt;&gt;a[i];    nxt[n]=-1;//记录之后最近一次出现两次以上的数字    repb(i,n-1,1){        if(a[i]==a[i+1]) nxt[i]=a[i];        else nxt[i]=nxt[i+1];    }    int top[2];//两个栈的栈顶元素    top[0]=top[1]=-1;    int res=0;    rep(i,1,n){        if(a[i]==a[i+1]){//重复长段,放哪儿都一样            if(top[0]!=a[i]) res++;            top[0]=a[i];        }        else{//重复长段末尾或单个字符            if(top[0]==a[i]) res+=(top[1]!=a[i]),top[1]=a[i];            else if(top[1]==a[i]) res+=(top[0]!=a[i]),top[0]=a[i];            else{                if(top[0]==nxt[i]) res++,top[0]=a[i];                else res++,top[1]=a[i];            }        }    }    cout&lt;&lt;res&lt;&lt;endl;}int main(){    solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 题解 </tag>
            
            <tag> Codeforces </tag>
            
            <tag> 贪心 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Codeforces Round /#580 (Div. 2) D. Shortest Cycle(Floyd求最小环)</title>
      <link href="2021/02/09/cf-580/"/>
      <url>2021/02/09/cf-580/</url>
      
        <content type="html"><![CDATA[<h1 id="题目地址"><a href="#题目地址" class="headerlink" title="题目地址"></a>题目地址</h1><p><a href="https://codeforces.com/contest/1206/problem/D">https://codeforces.com/contest/1206/problem/D</a></p><h1 id="题目大意"><a href="#题目大意" class="headerlink" title="题目大意"></a>题目大意</h1><p>给n个数字(n是1e5级),如果a[i]&amp;a[j]!=0，则i点到j点有连边，问最短的环长度是多少。</p><h1 id="解法"><a href="#解法" class="headerlink" title="解法"></a>解法</h1><p>先按位扫数组，如果一个位在数字上出现的次数超过了三次，那么这三个数字可以直接成环，输出答案3.</p><p>接下来讨论每个位最多两个数字的情况，这种情况最多只有2*64个数字，因此算法的复杂度可以比较大。但要记得筛除a[i]=0的情况。</p><p>那么floyd是怎么求最小环的呢？</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">rep(i,1,n)    rep(j,1,n)        mp[i][j]=e[i][j];//把一开始的e复制一遍给mprep(k,1,n){    //当循环到k的时候,可以保证之前的e[i][j]路径中都不会包含k    rep(i,1,k-1){        rep(j,i+1,k-1){            minn = min(minn,e[i][j]+mp[i][k]+mp[j][k]);//i-&gt;j-&gt;k-&gt;i这个环长度            //这句后面两个一定要是mp！一开始错在这里            //想明白了是因为圈上会有边被经过两次,类似丫字型,原本不能成环,但如果不是mp会被误判成环        }    }    rep(i,1,n){        rep(j,1,n){            e[i][j]=min(e[i][j],e[i][k]+e[k][j]);//这样处理之后,路径可以包含k        }    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以比较直观地看到，第一层枚举k，内层的i和j都不会大于k，这样就保证了新加入的点k不在任何的eij路径中。那么这样一来，i-&gt;j是i到j的一条路径,i-&gt;k-&gt;j是第二条路径，这样一来就是一个环。在这些解里面找最小值即可。</p><p>！之前比较疑惑的点就是，为什么要保留最初的邻接矩阵e在mp中呢？</p><p>因为如果都使用数组e，即<code>minn = min(minn,e[i][j]+mp[i][k]+mp[j][k])</code>，在这种情况下会出错：</p><p><img src="%5Cimg%5Ccf-580%5Ctu1.png" alt="tu1"></p><p>显然原本是不能成环的，但是$e_{i,k}$和$e_{j,k}$都有值，这样在<code>minn = min(minn,e[i][j]+mp[i][k]+mp[j][k])</code>下是有解的，实际上这条路径用了重复的路，所以没环，就出错了。</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)#define INF 0x3f3f3f3fconst int MAXN = 114514;ll a[MAXN];int e[666][666],mp[666][666];int n;vector&lt;ll&gt; vec;void solve(){    cin&gt;&gt;n;    rep(i,1,n) cin&gt;&gt;a[i];    ll tmp;    rep(i,0,63){        tmp = 1ll&lt;&lt;i;        int cnt = 0;        rep(i,1,n)            if(a[i]&amp;tmp) cnt++;        if(cnt&gt;=3){            cout&lt;&lt;3&lt;&lt;endl;            return;        }    }    vec.push_back(-1);//填个空    rep(i,1,n)        if(a[i]!=0) vec.push_back(a[i]);//防止0的情况    n = vec.size()-1;    //最多2*64个数，floyd找最小环    int minn = INF;    rep(i,1,n){        rep(j,1,n){            e[i][j] = INF/3;//INF的话会越界            if(i==j) e[i][j]=0;            else if(vec[i]&amp;vec[j]) e[i][j]=1;        }    }    //floyd    rep(i,1,n)        rep(j,1,n)            mp[i][j]=e[i][j];//把一开始的e复制一遍给mp    rep(k,1,n){        //当循环到k的时候,可以保证之前的e[i][j]路径中都不会包含k        rep(i,1,k-1){            rep(j,i+1,k-1){                minn = min(minn,e[i][j]+mp[i][k]+mp[j][k]);//i-&gt;j-&gt;k-&gt;i这个环长度                //这句后面两个一定要是mp！一开始错在这里                //想明白了是因为圈上会有边被经过两次,类似丫字型,原本不能成环,但如果不是mp会被误判成环            }        }        rep(i,1,n){            rep(j,1,n){                e[i][j]=min(e[i][j],e[i][k]+e[k][j]);//这样处理之后,路径可以包含k            }        }    }    if(minn&gt;n) cout&lt;&lt;-1&lt;&lt;endl;    else cout&lt;&lt;minn&lt;&lt;endl;}int main(){    solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 题解 </tag>
            
            <tag> Codeforces </tag>
            
            <tag> 图论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Codeforces Round /#697(Div-3)全题解</title>
      <link href="2021/01/26/cf-697/"/>
      <url>2021/01/26/cf-697/</url>
      
        <content type="html"><![CDATA[<h1 id="A-Odd-Divisor"><a href="#A-Odd-Divisor" class="headerlink" title="A. Odd Divisor"></a>A. Odd Divisor</h1><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>读题可知，只有这个数字可用$2^x$表示时不存在奇数除数。</p><p>这题比赛的时候没怎么想就开始写了，赛后才想到可以用lowbit判断是否是形如$2^x$的数，如果lowbit(x)==x，则这个数字可以用$2^x$表示。</p><p>我的方法是预处理出$2^x(x\in[0,60])$这样的一个数组，之后遇到一个数字，使用lowerbound进行二分，找到对应的位置之后，比较两个数字是否相同即可。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)#define INF 0x3f3f3f3fll p[114514];void init(){    p[0] = 1;    rep(i,1,60){        p[i] = p[i-1]*2;    }}void solve(){    ll n;      cin&gt;&gt;n;    if(*lower_bound(p+1,p+1+60,n)==n||n==1){        cout&lt;&lt;"NO"&lt;&lt;endl;    }    else cout&lt;&lt;"YES"&lt;&lt;endl;}int main(){    init();    int z;    cin&gt;&gt;z;    while(z--) solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="B-New-Year’s-Number"><a href="#B-New-Year’s-Number" class="headerlink" title="B. New Year’s Number"></a>B. New Year’s Number</h1><h2 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h2><p>已知2021比2020多1，如果一共有n个数供我们选择求和，全选2020会比全选2021少n。</p><p>因此，我们如果要判断一个数能不能被2020和2021表示，直接先把这个数整除2020得到t,知道这个数最多可以被t个数求和来表示。再把这个数对2020取余得到y，得知我们至少需要把y个2020改成2021来弥补这些差距。</p><p>之后只要检查t是否比y小，即是否有至少y个2020可以被改成2021的。</p><h2 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h2><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)#define INF 0x3f3f3f3fint n;void solve(){    cin&gt;&gt;n;    int t = n/2020;    int y = n%2020;    //cout&lt;&lt;t&lt;&lt;' '&lt;&lt;y&lt;&lt;endl;    if(y&lt;=t) cout&lt;&lt;"YES"&lt;&lt;endl;    else cout&lt;&lt;"NO"&lt;&lt;endl;}int main(){    int z;    cin&gt;&gt;z;    while(z--) solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="C-Ball-in-Berland"><a href="#C-Ball-in-Berland" class="headerlink" title="C. Ball in Berland"></a>C. Ball in Berland</h1><h2 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a>思路</h2><p>首先从最暴力的方法想，我们枚举第一个组合$(x,y)$,那么组合$(x,i)，i\in[1,a]$是不能被选择的，组合$(x,j)，j\in[1,b]$也是不能被选择的。</p><p>我们如果把$(x,y)$放在坐标轴上看，可以发现上面不能被选择的那些组合，就是x行和j列的这些数字。</p><p>如这张图中，我们选中了(4,6)则直接从总数k上减去第四行的总和以及第六列的总和，又因为(4,6)被减去了两次，所以要+1.</p><p><img src="%5Cimg%5Ccf-697%5Cbg.png" alt="bg"></p><h2 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h2><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)#define INF 0x3f3f3f3fint a,b,k;const int MAXN = 114514*2;int aa[MAXN],bb[MAXN];//读入a,bint cnta[MAXN],cntb[MAXN];//记录行列的和ll res;void solve(){    res = 0;    cin&gt;&gt;a&gt;&gt;b&gt;&gt;k;    rep(i,1,a) cnta[i] = 0;//计数的清空    rep(i,1,b) cntb[i] = 0;    rep(i,1,k){        cin&gt;&gt;aa[i];        cnta[aa[i]]++;//aa[i]行的总和增加    }    rep(i,1,k){        cin&gt;&gt;bb[i];        cntb[bb[i]]++;//bb[i]列的总和增加    }    rep(i,1,k){        res += k-cnta[aa[i]]-cntb[bb[i]]+1;    }    cout&lt;&lt;res/2&lt;&lt;endl;//因为两个组合前后顺序无关,所以要除2}int main(){    int z;    cin&gt;&gt;z;    while(z--) solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="D-Cleaning-the-Phone"><a href="#D-Cleaning-the-Phone" class="headerlink" title="D. Cleaning the Phone"></a>D. Cleaning the Phone</h1><h2 id="思路-3"><a href="#思路-3" class="headerlink" title="思路"></a>思路</h2><p>先按照”性价比“(即 $\frac{a[i]}{b[i]}$ )对所有app进行排序.</p><p>从开头往后遍历数组，优先取性价比高的放入要删掉的集合中。</p><p>我们维护pre1(上一个取得消耗为1的app)和pre2(上一个取得消耗为1的app)有两种情况需要讨论。</p><ul><li>如果此时删掉的集合的内存总和$nowm&lt;m$,且$b[i]=2$,把当前app加进来能满足$nowm\geq m$，这时候pre1可能是不必要的，我们要讨论pre1移除后是否能满足条件。</li><li>如果此时删掉的集合内存总和$nowm\geq m$,且$b[i]=1$,这时候pre2可能是不必要的，即用当前第i个app代替之前的pre2可能仍旧能满足$nowm\geq m$的条件。</li></ul><p>我们只需要贪心地先把nowm补足，并且用上面两条判断特殊情况即可。</p><p>另一种方法是，也可以用两个数组分别存放消耗为1和消耗为2的app，用两个指针表示每边的进度。</p><h2 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h2><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)#define INF 0x3f3f3f3ftypedef pair&lt;int,int&gt; pii;//用pair来存app的信息,first存a,second存b#define ft first#define sd secondconst int MAXN = 114514*2;pii a[MAXN];bool cmp(pii a,pii b){//自定义地比较函数，性价比高地排在前面    if((double)a.ft/a.sd&gt;(double)b.ft/b.sd) return 1;    else if((double)a.ft/a.sd==(double)b.ft/b.sd)        return a.sd&lt;b.sd;    return 0;}int n,m;void solve(){    cin&gt;&gt;n&gt;&gt;m;    rep(i,1,n)        cin&gt;&gt;a[i].ft;    rep(i,1,n)        cin&gt;&gt;a[i].sd;    sort(a+1,a+1+n,cmp);    int nowm = 0;    int cost = 0;    int pre1=0,pre2=0;//上次选择的    rep(i,1,n){        if(nowm&gt;=m&amp;&amp;pre2&amp;&amp;nowm-a[pre2].ft+a[i].ft&gt;=m&amp;&amp;a[i].sd==1){            cost--;            nowm=INF;            break;//处理完这种特殊情况后可以直接退出了,因为消耗不可能更小了        }//第二种特殊情况        else if(nowm&lt;m&amp;&amp;pre1&amp;&amp;nowm-a[pre1].ft+a[i].ft&gt;=m){            cost++;            nowm=INF;            break;        }//第一种特殊情况        else if(nowm&lt;m){            if(a[i].sd==1)pre1 = i;            else if(a[i].sd==2) pre2 = i;            cost+=a[i].sd;            nowm+=a[i].ft;        }    }    if(nowm&lt;m) cout&lt;&lt;-1&lt;&lt;endl;    else cout&lt;&lt;cost&lt;&lt;endl;}int main(){    int z;    cin&gt;&gt;z;    while(z--) solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="E-Advertising-Agency"><a href="#E-Advertising-Agency" class="headerlink" title="E. Advertising Agency"></a>E. Advertising Agency</h1><h2 id="思路-4"><a href="#思路-4" class="headerlink" title="思路"></a>思路</h2><p>这题其实比较简单。</p><p>举一组例子来说明：假如$k=7,n=12,$主播数组有$[5,5,4,4,4,3,3,3,3,3,2,1]$</p><p>如果要使粉丝数总和最大，选出7人，那么5和4粉丝的必须全部选择，不然不可能达到最大，且1和2的也必不可能取。只需要讨论3有几种情况即可。</p><p>选完5和4后还有$7-2-3=2$个名额，一共有5个3粉丝的主播，所以方数为$C^2_5$.</p><p>所以我们用一个cnt[1001]的数组统计每个数字一共出现几次。再从1000往1循环，找到需要在哪个数字上讨论(例子中是对3进行讨论)，now来统计当前消耗了几个名额，当$k-now\leq cnt[x]$时，说明需要对当前数字进行讨论。</p><h2 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h2><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)#define INF 0x3f3f3f3f//组合数板子const int MAXN = 3e5+5;const int med = 1e9+7;ll jc[MAXN];ll qpow(ll d,ll c){//快速幂    ll res = 1;    while(c){        if(c&amp;1) res=res*d%med;        d=d*d%med;c&gt;&gt;=1;    }return res;}inline ll niyuan(ll x){return qpow(x,med-2);}void initjc(){//记得要初始化阶乘    jc[0] = 1;    rep(i,1,MAXN-1) jc[i] = jc[i-1]*i%med;}inline int C(int n,int m){//n是下面的    if(n&lt;m) return 0;    return jc[n]*niyuan(jc[n-m])%med*niyuan(jc[m])%med;}//组合数板子const int MAXX = 114514;int a[MAXX],cnt[MAXX];int n,k;//选k个人void solve(){    cin&gt;&gt;n&gt;&gt;k;    rep(i,1,1000) cnt[i]=0;    int hc;    rep(i,1,n){        cin&gt;&gt;hc;        cnt[hc]++;//统计每个数字出现几次    }    int now=0;    repb(i,1000,1){//找到需要对哪个数字进行讨论        if(now+cnt[i]&gt;=k){            cout&lt;&lt;C(cnt[i],cnt[i]+now-k)&lt;&lt;endl;//计算排列组合            return;        }        now+=cnt[i];    }}int main(){    initjc();    int z;    cin&gt;&gt;z;    while(z--) solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="F-Unusual-Matrix"><a href="#F-Unusual-Matrix" class="headerlink" title="F. Unusual Matrix"></a>F. Unusual Matrix</h1><h2 id="思路-5"><a href="#思路-5" class="headerlink" title="思路"></a>思路</h2><p>每次操作影响了一整行/一整列，我们只需要设每行和每列操作了几次(只需要考虑是奇数次还是偶数次)，之后检验每个格子操作了几次(也只需要考虑奇数次还是偶数次即可)是否和之前所设的行和列的操作数匹配即可。</p><p>$jiou[i][j]$即$(i,j)$上的操作数即$(a[i][j])xor(b[i][j])$,从$a[i][j]$变成$b[i][j]$进行了奇数次还是偶数次操作。</p><p>$x[i]$表示第i行进行了几次操作，$y[i]$表示第y列进行了几次操作。</p><ul><li><p>我们先设$x[1]=0$即第一行进行偶数次操作，那么就可以通过$x[1]$和$jiou[1][i]$推得第i列($i\in[1,n]$)的操作数.</p></li><li><p>然后通过$y[1]$和$jiou[i][1]$，可以推得第i行($i\in[2,n]$)的操作数.</p></li></ul><p>当得到了每行每列的操作数后，我们通过judge()函数对每个格子进行检查，是否符合<strong>“每个格子的操作数是所处行和所处列的操作数总和”</strong>这一条件，即<code>jiou[i][j] = b[i][j]^a[i][j]</code>;。</p><p>之后再设$x[1]=1$，重复上述操作即可。</p><h2 id="代码-5"><a href="#代码-5" class="headerlink" title="代码"></a>代码</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)#define INF 0x3f3f3f3fconst int MAXN = 1145;int a[MAXN][MAXN],b[MAXN][MAXN];int jiou[MAXN][MAXN];//记录每个格子上操作了几次int x[MAXN],y[MAXN];//分别记录行和列上操作了几次int n;bool judge(){//判断行列操作数和格子操作数是否匹配    rep(i,1,n)        rep(j,1,n)            if(jiou[i][j]!=(x[i]^y[j])) return 0;    return 1;}void solve(){    cin&gt;&gt;n;    char hc;    rep(i,1,n){        rep(j,1,n){            cin&gt;&gt;hc;            a[i][j]=hc-'0';        }    }    rep(i,1,n){        rep(j,1,n){            cin&gt;&gt;hc;            b[i][j]=hc-'0';            jiou[i][j] = b[i][j]^a[i][j];        }    }    bool flag = 0;    x[1] = 0;//先假设第一行是偶数    rep(j,1,n) y[j] = jiou[1][j];    rep(i,2,n) x[i] = jiou[i][1]^y[1];    if(judge()) flag = 1;    x[1] = 1;//再设第一行是奇数    rep(j,1,n) y[j] = jiou[1][j]^1;    rep(i,2,n) x[i] = jiou[i][1]^y[1];    if(judge()) flag = 1;    if(flag) cout&lt;&lt;"YES"&lt;&lt;endl;    else cout&lt;&lt;"NO"&lt;&lt;endl;}int main(){    int z;    cin&gt;&gt;z;    while(z--) solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="G-Strange-Beauty"><a href="#G-Strange-Beauty" class="headerlink" title="G. Strange Beauty"></a>G. Strange Beauty</h1><h2 id="思路-6"><a href="#思路-6" class="headerlink" title="思路"></a>思路</h2><p>已知如果要满足$a[i]\mod a[j]=0$即aj为ai的除数，那么肯定满足$a[i]\leq a[j]$。所以如果我们对数组从小到大排序，则一定满足$a[i-1]\leq a[i]$,只需要考虑前面的数字是否是后面的数字即可。</p><p>用动态规划写，如果是最暴力的方法：</p><ul><li>dp[i]统计到第i个数字最多可以保留几个。</li><li>统计到第i个数字时，j从头1到i-1扫一遍，如果a[i]%a[j]==0,那么dp[i]就可以是dp[j]+1</li><li>在这些dp[j]+1中取最大值赋给dp[i]即可。</li></ul><p>这样的复杂度是$O(n^2)$的</p><p>之后考虑怎么优化：</p><p>因为要求a[i]%a[j]==0，所以只需要考虑a[j]为a[i]因子的情况，我们枚举a[i]的因子，再检查之前是否有符合条件的a[j]即可。</p><ul><li>用一个map[x]来存当前等于x的最大下标，即上一个x出现在哪一位置。</li><li>处理a[i]时，只需要枚举a[i]的所有因子y，检查map[y]，那么dp[i]就可以是dp[map[y]]+1</li><li>这些dp[map[y]]+1中取最大值赋给dp[i]即可。</li></ul><p>因为数字x范围比较小，这个map可以用数组实现，而且会比stl的map更快,复杂度为$O(n\cdot sqrt(n))$。我图方便用stl的map写的，因此复杂度为$O(n\cdot sqrt(n)\cdot logn)$</p><h2 id="代码-6"><a href="#代码-6" class="headerlink" title="代码"></a>代码</h2><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;cmath&gt;#include&lt;map&gt;#include&lt;algorithm&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)#define INF 0x3f3f3f3fconst int MAXN = 114514*2;int a[MAXN];int dp[MAXN];//存最大长度map&lt;int,int&gt; mp;//从数字到下标的映射int n;void solve(){    mp.clear();    cin&gt;&gt;n;    rep(i,1,n) cin&gt;&gt;a[i];    sort(a+1,a+1+n);//小的排在前面    dp[0]=0;    int res=0;    rep(i,1,n){        dp[i] = 0;        int sqr = sqrt(a[i]);        rep(k,1,sqr){//细节是1和a[i]也需要去map找,因为可能有重复数字            if(a[i]%k==0){                dp[i]=max(dp[i],dp[mp[k]]+1);//dp[i]要取最大值！                dp[i]=max(dp[i],dp[mp[a[i]/k]]+1);            }        }        mp[a[i]]=i;//更新map        res = max(res,dp[i]);    }    cout&lt;&lt;n-res&lt;&lt;endl;}int main(){    int z;    cin&gt;&gt;z;    while(z--) solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="PS"><a href="#PS" class="headerlink" title="PS"></a>PS</h1><p>以后想尽量每次都写个题解出来，我水平比较菜，所以主要还是给零基础学弟学妹看的QAQ</p>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 题解 </tag>
            
            <tag> Codeforces </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Codeforces Round /#691 (Div. 2) D. Glass Half Spilled(DP,背包) 含两种写法</title>
      <link href="2020/12/21/cf-691-d/"/>
      <url>2020/12/21/cf-691-d/</url>
      
        <content type="html"><![CDATA[<h1 id="题目地址"><a href="#题目地址" class="headerlink" title="题目地址"></a>题目地址</h1><p><a href="https://codeforces.com/contest/1459/problem/D">https://codeforces.com/contest/1459/problem/D</a></p><h1 id="题目大意"><a href="#题目大意" class="headerlink" title="题目大意"></a>题目大意</h1><p>$n\leq 100$</p><p>有$n$个玻璃杯，有$a[i]$和$b[i]$两个属性，分别代表容量和里面已经有的水，转移水的时候会损失一半的水被撒到地上。</p><p>问把水集中到$k$个杯子里的时候，最多总共可以有多少水？$k=1,2,3,4\dots n$</p><h1 id="解法"><a href="#解法" class="headerlink" title="解法"></a>解法</h1><p>这题居然是一个几乎是裸的<strong>01背包</strong>！</p><p>有两种写法，先讲我一开始写的一种：</p><p>是两个限制条件的01背包$dp[i][j][k]$,第一维是“当只考虑前i个杯子时”，第二维是“取了几个杯子”，第三维是这些杯子里的初始水量$b[i]$之和。</p><p>这时候就可以开始按照裸01背包写了，$dp[i][j][k]$维护这个条件下最大的容量！为什么是最大的容量呢？因为选中的$k$个杯子中初始水量和相等的情况下，肯定是容量越大最后的结果也会越大(或者相等，但肯定不会小)。</p><p>考虑到空间复杂度，$dp$数组需要把第一维变成滚动的来节省空间。</p><p>在处理完$dp[j][k]$数组之后，只需要按照题意列出实际值，比较后得出结果即可</p><p>实际值也就是 除去k个杯子外的总水量$\sum_{i=1}^{n} b[i]-k$除以2之后再加上k，即$k+(sum-k)/2$,这个值还要跟$dp[j][k]$取$min$,因为杯子的容量有限，最后就是$max(min(dp[j][k],k+(sum-k)/2))$。</p><p>——————————分割线———————————-</p><p>第二种方法就是k改成k个杯子的总容量</p><p>这个时候的结果就是$max(res,min(k,dp[j][k]+(sum-dp[j][k])/2))$</p><h1 id="两个值得注意的点"><a href="#两个值得注意的点" class="headerlink" title="两个值得注意的点"></a>两个值得注意的点</h1><p>1.因为有的杯子的初始水量为0，所以一开始开始$O(n^3)$求$dp[j][k]$的时候$k$也要从0开始。</p><p>2.因为有的杯子的初始水量为0，所以最后结果的循环要从0开始。</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><h2 id="解法1"><a href="#解法1" class="headerlink" title="解法1"></a>解法1</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;using namespace std;#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)const int MAXN = 114;#define INF 0x3f3f3f3fint cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);int dp[MAXN][10010];int a[MAXN],b[MAXN];int n,tot;void solve(){    cin&gt;&gt;n;    tot = 0;    rep(i,1,n){        cin&gt;&gt;a[i]&gt;&gt;b[i];        tot+=b[i];    }    memset(dp,-INF,sizeof(dp));    dp[0][0]=0;    rep(i,1,n){        repb(j,i,1){            repb(k,tot,0){//错在这里，因为可能有bi=0的情况                if(k&gt;=b[i]&amp;&amp;j) dp[j][k] = max(dp[j][k],dp[j-1][k-b[i]]+a[i]);            }        }    }    rep(j,1,n){        double res = 0;        rep(k,0,tot){//第二处错在这里，应该从0开始，因为k是水量不是容量，所以也可能为0            res = max(res,min((double)dp[j][k],k+(double)(tot-k)/2));        }        cout&lt;&lt;res&lt;&lt;' ';    }    cout&lt;&lt;endl;}int main(){    solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="解法2"><a href="#解法2" class="headerlink" title="解法2"></a>解法2</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;algorithm&gt;using namespace std;#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)const int MAXN = 114;#define INF 0x3f3f3f3fint cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);int dp[MAXN][10010];int a[MAXN],b[MAXN];int n,tota,totb;void solve(){    cin&gt;&gt;n;    tota = totb = 0;    rep(i,1,n){        cin&gt;&gt;a[i]&gt;&gt;b[i];        tota+=a[i];        totb+=b[i];    }    memset(dp,-INF,sizeof(dp));    dp[0][0]=0;    rep(i,1,n){        repb(j,i,1){            repb(k,tota,0){                if(k&gt;=a[i]&amp;&amp;j) dp[j][k] = max(dp[j][k],dp[j-1][k-a[i]]+b[i]);            }        }    }    rep(j,1,n){        double res = 0;        rep(k,0,tota){            if(dp[j][k]&gt;=0){                res = max(res,min((double)k,(double)dp[j][k]+(double)(totb-dp[j][k])/2));            }        }        cout&lt;&lt;res&lt;&lt;' ';    }    cout&lt;&lt;endl;}int main(){    solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 题解 </tag>
            
            <tag> Codeforces </tag>
            
            <tag> DP </tag>
            
            <tag> 背包 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Codeforces Round /#688 (Div. 2) D. Checkpoints(期望,概率,贪心)</title>
      <link href="2020/12/14/cf-688-d/"/>
      <url>2020/12/14/cf-688-d/</url>
      
        <content type="html"><![CDATA[<h1 id="题目地址"><a href="#题目地址" class="headerlink" title="题目地址"></a>题目地址</h1><p><a href="https://codeforces.com/contest/1453/problem/D">https://codeforces.com/contest/1453/problem/D</a></p><h1 id="题目大意"><a href="#题目大意" class="headerlink" title="题目大意"></a>题目大意</h1><p>游戏有若干个关卡，有些关卡前面有存档点，有些关卡没有。</p><p>通过每个关卡的概率都是$\frac{1}{2}$，如果失败后从上一次存档的存档点处开始。</p><p>输入一个$k$。</p><p>输出一个序列，0代表关卡开头没有存档点，1则有。第一个关卡前面肯定是有存档点的。</p><p>求一种安排使得打通所有关卡的时间期望为$k$。</p><h1 id="解法"><a href="#解法" class="headerlink" title="解法"></a>解法</h1><p>以前从来没写过期望相关的题，这次直接懵逼。</p><p>我们设当前站在关卡$x$，通过$x$这一关需要的时间期望为$f_x$。</p><ul><li>先讨论只有1的情况：</li></ul><p>对$[1,1]$这个序列:可以列出</p><p>$f_1=1\cdot\frac{1}{2}+\frac{1}{2}\cdot(f_1+1)$</p><p>$f_2=1\cdot\frac{1}{2}+\frac{1}{2}\cdot(f_2+1)$</p><p>不难理解，前半部分是一次通过，后半部分是这次没通过，那么下次的尝试的期望就是$f_1+1$，$+1$是因为这次失败了，下次的时候就要多花一个时间。</p><p>所以如果对于一段长度为n全是1的序列，通关的期望就是$2\cdot n$</p><ul><li>接下来讨论有0的情况：</li></ul><p>对$[1,0,0]$这个序列:可以列出f_1</p><p>$f_1=1\cdot\frac{1}{2}+\frac{1}{2}\cdot(f_1+1)$</p><p>$f_2=1\cdot\frac{1}{2}+\frac{1}{2}\cdot(f_2+f_1+1)$，因为2没有存档点，所以要重新打第一关，加上$f_1$</p><p>化简出$f_2=f_1+2=4$</p><p>$f_3=1\cdot\frac{1}{2}+\frac{1}{2}\cdot(f_1+f_2+f_3+1)$</p><p>化简出$f_3=f_1+f_2+2=2\cdot f_1+4=8$</p><p>对于一段连续的0，第$i$个0的时间期望就是$2^i$。</p><p>—————————–分割线——————————-</p><p>如果全放1的话，长度只能放到2000，但是k可以到1e18这么大，所以不行</p><p>我们就尽量放0，然后这段0太大了就放个1重新从2开始。</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define INF 0x3f3f3f3fint a[2077];ll f[2020];ll k;void solve(){    cin&gt;&gt;k;    if(k%2){cout&lt;&lt;-1&lt;&lt;endl;return;}    ll now = 0;    a[1] = 1;    f[1] = 2;    now+=2;//a1    int i;    for(i=2;now&lt;k;i++){        if(now+f[i-1]*2&lt;=k){            a[i] = 0;            f[i] = f[i-1]*2;        }        else{            a[i] = 1;            f[i] = 2;        }        now+=f[i];    }    i--;    cout&lt;&lt;i&lt;&lt;endl;    rep(j,1,i){        cout&lt;&lt;a[j]&lt;&lt;' ';    }    cout&lt;&lt;endl;}int main(){    int z;    cin&gt;&gt;z;    while(z--) solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 题解 </tag>
            
            <tag> Codeforces </tag>
            
            <tag> 期望 </tag>
            
            <tag> 概率 </tag>
            
            <tag> 贪心 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Codeforces Round /#686 (Div. 3) F. Array Partition(ST,二分)</title>
      <link href="2020/12/14/cf-686-f/"/>
      <url>2020/12/14/cf-686-f/</url>
      
        <content type="html"><![CDATA[<h1 id="题目地址"><a href="#题目地址" class="headerlink" title="题目地址"></a>题目地址</h1><p><a href="https://codeforces.com/contest/1454/problem/F">https://codeforces.com/contest/1454/problem/F</a></p><h1 id="题目大意"><a href="#题目大意" class="headerlink" title="题目大意"></a>题目大意</h1><p>给一串序列，是否能有一种方案把序列划分成连续的三段$d1,d2,d3$，使得$max(d1)=min(d2)=max(d3)$。</p><p>比如序列$[1,2,3,3,3,4,4,3,4,2,1]$可以划分成$[1,2,3,3,3,4],[4],[3,4,2,1]$</p><h1 id="解法"><a href="#解法" class="headerlink" title="解法"></a>解法</h1><p>这题我是根本想不到要二分。这题的二分比较奇妙。</p><p>首先预处理两个$max$,从头开始的$max$值从结尾开始的$max$值（用于快速查询$d1$和$d3$)。再<strong>维护一个st表</strong>查询区间最小值$min$（用于快速查询$d2$）</p><p>设$d2$左右端点分别为$l,r$。</p><p>我们<strong>枚举</strong>$l$再<strong>二分</strong>寻找$r$,复杂度$O(n^2)$</p><ul><li><p>如果$min(d2)&gt;max(d1)$，说明<strong>d2太长</strong>或者d1太短，这时候<strong>l已经固定</strong>，所以只能<strong>让d2变短</strong>，即mid向左移动。</p></li><li><p>如果$max(d1)&gt;max(d3)$，说明<strong>d1太长</strong>或者d3太短，这时候<strong>l已经固定</strong>，所以只能<strong>让d3变长</strong>，即$mid$向左移动。</p></li></ul><p>再看看这句对不对？</p><ul><li>如果$min(d2)&gt;max(d3)$，说明$d2$太长或者$d3$太短，所以应该向左移动$mid$?</li></ul><p>不对，因为在向左移动$mid$的时候，$max(d3)$增大的同时$min(d2)$也在增大，并不是在向更接近的地方行进，所以不行！</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">while(l&lt;r){mid = (l+r)&gt;&gt;1;tmp = qry(i,mid);//这里我一开始多写了个条件就是max2和tmp比较的,这个不能有//因为扩的时候max2和tmp都可能下降，并不是往更接近的方向走    if(tmp&gt;max1[i-1]||max1[i-1]&lt;max2[mid+1])l = mid+1;else if(tmp&lt;max1[i-1]||max1[i-1]&gt;max2[mid+1])r = mid-1;else break;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;algorithm&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;vector&gt;#include&lt;queue&gt;#include&lt;cmath&gt;using namespace std;#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)#define ll long long#define log(x) (31-__builtin_clz(x))//谢谢hjtconst int MAXN = 2e5+5;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);const int LOGN = log(MAXN)/log(2); int M[MAXN][LOGN+3]; int a[MAXN];int z,m,n;int max1[MAXN];int max2[MAXN];void init(){//初始化，复杂度O(nlogn) for(int i=1;i&lt;=n;i++) M[i][0]=i;//长度为1的区间最值是自己 for(int j=1;j&lt;=LOGN;j++){for(int i=1;i&lt;=n-(1&lt;&lt;j)+1;i++){if(a[M[i][j-1]]&lt;a[M[i+(1&lt;&lt;(j-1))][j-1]]) M[i][j] = M[i][j-1];//这里以最小值为例 else M[i][j] = M[i+(1&lt;&lt;j-1)][j-1];}} }inline int query(int l,int r){int k = log(r-l+1)/log(2);//向下取整if(a[M[l][k]]&lt;a[M[r-(1&lt;&lt;k)+1][k]]) return M[l][k];else return M[r-(1&lt;&lt;k)+1][k];}inline int qry(int l,int r){    return a[query(l,r)];//直接返回数值}bool flag;void solve(){    int la,lb,lc;    flag = 0;    cin&gt;&gt;n;    rep(i,1,n) cin&gt;&gt;a[i];    init();    max1[0]=max2[n+1] = 0;    rep(i,1,n) max1[i] = max(max1[i-1],a[i]);//前缀最大值    repb(i,n,1) max2[i] = max(max2[i+1],a[i]);    int l,r,mid,tmp;    rep(i,2,n-1){//枚举中间段的开头        l = i,r = n-1;//二分        while(l&lt;r){            mid = (l+r)&gt;&gt;1;            tmp = qry(i,mid);            //这里我一开始多写了个条件就是max2和tmp比较的,这个不能有            //因为扩的时候max2和tmp都可能下降，并不是往更接近的方向走            if(tmp&gt;max1[i-1]||max1[i-1]&lt;max2[mid+1])                l = mid+1;            else if(tmp&lt;max1[i-1]||max1[i-1]&gt;max2[mid+1])                r = mid-1;            else break;        }        mid = (l+r)&gt;&gt;1;        if(mid&gt;=i&amp;&amp;mid&lt;=n-1){            tmp = qry(i,mid);            if(tmp==max1[i-1]&amp;&amp;tmp==max2[mid+1]){                flag = 1;                la = i-1,lb = mid-i+1,lc = n-mid;                break;            }        }    }    if(!flag) cout&lt;&lt;"NO"&lt;&lt;endl;    else{        cout&lt;&lt;"YES"&lt;&lt;endl;        cout&lt;&lt;la&lt;&lt;' '&lt;&lt;lb&lt;&lt;' '&lt;&lt;lc&lt;&lt;endl;    }}signed main(){    int z;    cin&gt;&gt;z;    while(z--) solve();}//二分yyds<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 题解 </tag>
            
            <tag> Codeforces </tag>
            
            <tag> ST </tag>
            
            <tag> 二分 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模板整理--图论(持续更新)</title>
      <link href="2020/12/13/graph/"/>
      <url>2020/12/13/graph/</url>
      
        <content type="html"><![CDATA[<h2 id="网络流相关"><a href="#网络流相关" class="headerlink" title="网络流相关"></a>网络流相关</h2><h3 id="SFPA"><a href="#SFPA" class="headerlink" title="SFPA"></a>SFPA</h3><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;queue&gt;#include&lt;vector&gt;using namespace std;#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define ll long longconst int MAXN = 114514;const int MAXM = 514514;const ll LINF = 1ll&lt;&lt;61;//前向星struct Edge{    int to,w,nxt;}edges[MAXM];int head[MAXN];int n,m,s;int cnt;inline void add_edge(int u,int v,int w){    edges[cnt].to = v;    edges[cnt].w = w;    edges[cnt].nxt = head[u];    head[u] = cnt++;}//SPFAqueue&lt;int&gt; q;ll dis[MAXN];//每个点当前最小距离bool vis[MAXN];//记录队列中是否有该点inline void init(){//初始化前向星和SPFA的dis数组    rep(i,1,n) dis[i]=LINF,head[i]=-1;    cnt = 0;}inline void solve(){    cin&gt;&gt;n&gt;&gt;m&gt;&gt;s;    init();//记得要初始化    int u,v,w;    rep(i,1,m){        cin&gt;&gt;u&gt;&gt;v&gt;&gt;w;        add_edge(u,v,w);    }    dis[s] = 0;    q.push(s);    int now;    while(!q.empty()){        now = q.front();        q.pop();        vis[now] = 0;//vis表示队中有无该点        //每次取出队首,并检查关联点是否可以进行松弛操作        for(int i=head[now];i!=-1;i=edges[i].nxt){            v = edges[i].to;            if(dis[v]&gt;dis[now]+edges[i].w){//松弛操作                dis[v] = dis[now]+edges[i].w;                if(!vis[v]){//如果队列中没有点v,则将v入队                    vis[v] = 1;                    q.push(v);                }            }        }    }    rep(i,1,n) cout&lt;&lt;dis[i]&lt;&lt;' ';    //我这里还没有对到达不了的特殊情况进行处理    cout&lt;&lt;endl;}int main(){    solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="费用流-SPFA找最短增广路"><a href="#费用流-SPFA找最短增广路" class="headerlink" title="费用流-SPFA找最短增广路"></a>费用流-SPFA找最短增广路</h3><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;using namespace std;#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define INF 0x7fffffffconst int N = 5e3+5;const int M = 114514;//因为是双向边,要记得开两倍大的数组!int n;//前向星存图int t=1;//总边数int u[M],v[M],c[M],co[M],nxt[M],head[M];inline void add_edge(int x,int y,int z,int zo){//起点终点流量费用    u[++t]=x;v[t]=y;//正向边    c[t]=z;co[t]=zo;    nxt[t]=head[x];head[x]=t;    u[++t]=y;v[t]=x;//反向边    c[t]=0;co[t]=-zo;    nxt[t]=head[y];head[y]=t;}//SPFAint S,T;int q[M],l,r;//双端队列int dis[N];bool in[N];//在不在queue中int f[N];//记录路径inline bool spfa(){    rep(i,1,n) in[i]=0,dis[i]=INF;//初始化    dis[S]=0;in[S]=1;    l=r=M&gt;&gt;1;q[l]=S;//手写队列    while(l&lt;=r){        int now=q[l++];//取队首        if(now==T) continue;        for(int i=head[now];i;i=nxt[i])            if(c[i]&amp;&amp;dis[now]+co[i]&lt;dis[v[i]]){//费用更小的路                dis[v[i]]=dis[now]+co[i];                f[v[i]]=i;//记录路径!增广路经过哪些边                if(!in[v[i]]){//如果队中没有则入队                    in[v[i]]=1;                    if(dis[v[i]]&lt;dis[q[l]])q[--l]=v[i];else q[++r]=v[i];                    //双端队列优化spfa                }            }        in[now]=0;    }    return dis[T]&lt;INF;//是否找到增广路}int i,tmp;inline void costflow(){    int ans=0,flow=0;//最小费用,最大流    while(spfa()){        //从终点根据记录的f找到增广路        for(tmp=INF,i=T;i!=S;i=u[f[i]]) if(tmp&gt;c[f[i]]) tmp=c[f[i]];        //求这一次流多少(所有边剩余流量中最小值)        for(ans+=dis[i=T]*tmp;i!=S;i=u[f[i]]) c[f[i]]-=tmp,c[f[i]^1]+=tmp;        //减去流量并加在反向边上,并且计算费用        flow+=tmp;    }    printf("%d %d\n",flow,ans);}int main(){    int m;    scanf("%d%d%d%d",&amp;n,&amp;m,&amp;S,&amp;T);    int u,v,z,zo;    while(m--){        scanf("%d%d%d%d",&amp;u,&amp;v,&amp;z,&amp;zo);        add_edge(u,v,z,zo);    }    costflow();}//参考claris费用流板子//洛谷P3381//https://www.luogu.com.cn/problem/P3381<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="连通性相关"><a href="#连通性相关" class="headerlink" title="连通性相关"></a>连通性相关</h2><h3 id="Tarjan求强连通分量"><a href="#Tarjan求强连通分量" class="headerlink" title="Tarjan求强连通分量"></a>Tarjan求强连通分量</h3><p>可以用于有向图问题的缩点。</p><p>$dfn[u]$记录当前节点u是第几个访问到的；</p><p>$low[u]$定义为$Subtree(u)$中$dfn(u)$最小的，或者$Subtree(u)$中的点通过**非树边(不在子树上的边)**（返祖边或者比$rt$还高的边？）$low[u]$初始值定u，如果通过边连到$rt$之上的点可更新，又递归通过儿子$low[v]$来更新。</p><p>连通图中有且仅有一个$dfn[u]=low[u]$,这个点是根节点。通过一个栈按照dfs的顺序入栈，回溯的时候判断如果一个点的$dfn[u]=low[u]$，则从这个点到栈顶的所有节点归入一个强连通分量。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;#include&lt;map&gt;#include&lt;queue&gt;#include&lt;cmath&gt;using namespace std;template&lt;class T&gt;inline void read(T &amp;x){x=0;char o,f=1;while(o=getchar(),o&lt;48)if(o==45)f=-f;do x=(x&lt;&lt;3)+(x&lt;&lt;1)+(o^48);while(o=getchar(),o&gt;47);x*=f;}int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=b;i--)#define INF 0x3f3f3f3f#define cendl printf("\n")ll gcd(ll a,ll b){ while(b^=a^=b^=a%=b); return a; }//#define INF 0x7fffffffconst int MAXN = 2e6+5;int n;vector&lt;int&gt; e[MAXN];//存图 int dfn[MAXN],low[MAXN];int dfncnt;//dfn自增下标 int s[MAXN],tp;//数组模拟栈,tp记录大小 bool in[MAXN];//记录该点是否在栈中 int scc[MAXN],sc;//节点i所在scc的编号,sc记录有几个强连通 int sz[MAXN];//强连通i的大小 int indg[MAXN];//记录缩点后的入度（这题才有的 void tarjan(int u){low[u]=dfn[u]=++dfncnt;//low初始值为自身dfns[++tp]=u;//推u入栈，从1开始 in[u]=1;//记录u点在栈中 for(auto v:e[u]){//访问到新点的情况 if(!dfn[v]){tarjan(v);low[u] = min(low[u],low[v]);//用low[v}更新low[u] }else if(in[v])//v被访问过，但是在栈中 low[u] = min(low[u],dfn[v]);}if(dfn[u]==low[u]){//u是连通分量的根节点sc++;//强连通数量++sz[sc] = 0; while(s[tp]!=u){//u和u之后的点全部出栈 scc[s[tp]] = sc;//这个点包含于第几个强连通sz[sc]++;//u为根的这个强连通的大小 in[s[tp]] = 0;//出栈 tp--; }scc[u] = sc;//给根节点标，属于第sc个强连通 sz[sc]++;in[u] = 0;tp--;}} void reset(){tp = sc = dfncnt =0;rep(i,1,n){in[i] = dfn[i] = 0;//low不用清空，sz在之后用到再清空 e[i].clear();}}int main(){cin&gt;&gt;n;reset();int v;rep(u,1,n){while(cin&gt;&gt;v&amp;&amp;v!=0) e[u].push_back(v);} rep(u,1,n)if(!dfn[u]) tarjan(u);rep(i,1,sc) indg[i] = 0;//这个不包含在tarjan里面，是这题记录入度的 rep(u,1,n){for(auto v:e[u]){if(scc[u]!=scc[v]) indg[scc[v]]++;}}int res = 0;rep(i,1,sc){if(indg[i]==0) res++;}cout&lt;&lt;res&lt;&lt;endl;}//洛谷P2835 刻录光盘 https://www.luogu.com.cn/problem/P2835#submit<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Tarjan求割点"><a href="#Tarjan求割点" class="headerlink" title="Tarjan求割点"></a>Tarjan求割点</h3><p>定义：去掉这个点，连通分量数量+1</p><p>无向图的$low[u]$在这里表述成：点$u$在<strong>不经过其父亲的情况下能达到的最小时间戳/过一条非树边达到的最小时间戳</strong>(最小的$dfn$)</p><p>对于某个顶点$u$，如果$u$的儿子$v$中，存在至少一个$low[v]\ge dfn[u]$，即v子树中无法回到祖先（即最高也只能回到父亲u），则u是割点。</p><p>如果是$dfn[u]=1$即祖先的点，满足儿子数量$\ge 2$即可。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)using namespace std;const int MAXN = 114514;int n,m;vector&lt;int&gt;e[MAXN];bool vis[MAXN];bool flag[MAXN];//标记割点int res;//记录割点数量int low[MAXN],dfn[MAXN],dfncnt;//老tarjan了void Tarjan(int u,int pre){//pre=-1时代表是祖先    vis[u] = 1;    low[u] = dfn[u] = ++dfncnt;    int cntchild=0;//儿子数量    for(auto v:e[u]){        if(!vis[v]){            cntchild++;            Tarjan(v,u);            low[u] = min(low[u],low[v]);            if(pre!=-1&amp;&amp;low[v]&gt;=dfn[u]&amp;&amp;!flag[u]){//算法核心                flag[u]=1;                res++;            }        }        else if(v!=pre)//往上的边,更新当前节点的low            low[u] = min(low[u],dfn[v]);    }    if(pre==-1&amp;&amp;cntchild&gt;=2&amp;&amp;!flag[u]){//u为祖先要另外讨论        flag[u]=1;        res++;    }}void solve(){    cin&gt;&gt;n&gt;&gt;m;    //初始化    rep(i,1,n){vis[i]=flag[i]=0;e[i].clear();}    res=dfncnt=0;    int u,v;    rep(i,1,m){        cin&gt;&gt;u&gt;&gt;v;        e[u].push_back(v);        e[v].push_back(u);    }    rep(i,1,n) if(!vis[i]) Tarjan(i,-1);//也可在这重置dfncnt    cout&lt;&lt;res&lt;&lt;endl;    rep(i,1,n){        if(flag[i]) cout&lt;&lt;i&lt;&lt;' ';    }    cout&lt;&lt;endl;}int main(){    solve();}//P3388 【模板】割点（割顶）//https://www.luogu.com.cn/problem/P3388<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Tarjan求桥-割边"><a href="#Tarjan求桥-割边" class="headerlink" title="Tarjan求桥(割边)"></a>Tarjan求桥(割边)</h3><p>类似求割点，和割点差不多，只要改一处：$low[v]&gt;dfn[u]$ 就可以了，因为从”&gt;=”变成了”=”，所以不需要考虑祖先节点的问题。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int low[MAXN], dfn[MAXN], iscut[MAXN], dfs_clock;bool isbridge[MAXN];vector&lt;int&gt; G[MAXN];int cnt_bridge;int father[MAXN];void tarjan(int u, int fa) {    father[u] = fa;    low[u] = dfn[u] = ++dfs_clock;    for (int i = 0; i &lt; G[u].size(); i++) {        int v = G[u][i];        if (!dfn[v]) {            tarjan(v, u);            low[u] = min(low[u], low[v]);            if (low[v] &gt; dfn[u]) {//算法核心                isbridge[v] = true;                ++cnt_bridge;            }        }         else if (dfn[v] &lt; dfn[u] &amp;&amp; v != fa) {            low[u] = min(low[u], dfn[v]);        }    }}//摘自OIWIKI//https://oi-wiki.org/graph/cut/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图论 </tag>
            
            <tag> ACM模板 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模板整理--数据结构(持续更新)</title>
      <link href="2020/11/29/data-structure/"/>
      <url>2020/11/29/data-structure/</url>
      
        <content type="html"><![CDATA[<h2 id="平衡树-无旋转"><a href="#平衡树-无旋转" class="headerlink" title="平衡树(无旋转)"></a>平衡树(无旋转)</h2><h3 id="替罪羊树"><a href="#替罪羊树" class="headerlink" title="替罪羊树"></a>替罪羊树</h3><p>是比较优雅的暴力(想到了莫队)。</p><p>根据定义的alpha等参数来判断树是否需要重构。重构的时候，先中序遍历得到序列，然后利用分治的思想，每次拎起中间的点作为当前子树的根节点，然后递归处理这个点左右两端的序列。</p><p>每次插入均摊到的复杂度是$O(log(size(T)))$</p><p>2020.11.26</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;#include&lt;cmath&gt;using namespace std;#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);const int MAXN = 1e5+5;const double alpha = 0.75;//平衡因子struct Node{    int l,r,val;    int size,fact;//子树大小,子树内存在的节点数    bool exist;//是否存在}tzy[MAXN];int cnt,root;//节点堆计数,根节点下标void newnode(int &amp;now,int val){//新建节点    now = ++cnt;    tzy[now].val=val;    tzy[now].size=tzy[now].fact = 1;//初始化    tzy[now].exist = 1;}bool imbalance(int now){//判断平衡    //如果左右子树中的一个重量占比超过平衡因子,或者子树中假的点超过30%    if(max(tzy[tzy[now].l].size,tzy[tzy[now].r].size)&gt;tzy[now].size*alpha    ||tzy[now].size-tzy[now].fact&gt;tzy[now].size*0.3)        return 1;    else return 0;}vector&lt;int&gt; v;void ldr(int now){//中序遍历    if(!now) return;//到底了    ldr(tzy[now].l);    if(tzy[now].exist) v.push_back(now);//存在的点拎起来,不存在的丢掉    ldr(tzy[now].r);}void lift(int l,int r,int &amp;now){//递归分治把中间的点拎起来    if(l==r){        now = v[l];        tzy[now].l=tzy[now].r = 0;//叶子节点        tzy[now].size=tzy[now].fact = 1;        return;    }    int m = (l+r)&gt;&gt;1;//找到中间节点;    //特殊情况,如果是x 5 5 x x的话,相同的值本应该放在右边,这里却放到了左边    while(l&lt;m&amp;&amp;tzy[v[m]].val==tzy[v[m-1]].val)//左移,直到找到合适的点        m--;    now = v[m];//找到应该被拎起来的点    //分治思想    if(l&lt;m) lift(l,m-1,tzy[now].l);//左边还有可以拎起来的,最后一个参数是为了把树接起来的    else tzy[now].l = 0;    lift(m+1,r,tzy[now].r);//右边一定有可以拎起来的    tzy[now].size = tzy[tzy[now].l].size+tzy[tzy[now].r].size+1;    tzy[now].fact = tzy[tzy[now].l].fact+tzy[tzy[now].r].fact+1;}void rebuild(int &amp;now){//之前这里now忘了写引用    v.clear();    ldr(now);//得到中序遍历序列    if(v.empty()){//所有的都是被假的点,特判        now = 0;        return;    }    lift(0,v.size()-1,now);}void update(int now,int end){//因为没有记录父亲的编号,所以只能头递归    if(!now) return;    if(tzy[end].val&lt;tzy[now].val) update(tzy[now].l,end);    else update(tzy[now].r,end);    tzy[now].size=tzy[tzy[now].l].size+tzy[tzy[now].r].size+1;}void check(int &amp;now,int end){//沿路径检查    if(now==end) return;    if(imbalance(now)){        rebuild(now);//重构        update(root,now);//重构后更新        return;    }    if(tzy[end].val&lt;tzy[now].val) check(tzy[now].l,end);    else check(tzy[now].r,end);}void ins(int &amp;now,int val){//递归插入节点    if(!now){//now=0是没有节点的地方(空儿子)        newnode(now,val);        check(root,now);//检查插入后是否需要重构        return;    }    tzy[now].size++;    tzy[now].fact++;    if(val&lt;tzy[now].val) ins(tzy[now].l,val);    else ins(tzy[now].r,val);}void del(int now,int val){    if(tzy[now].exist&amp;&amp;tzy[now].val==val){//找到要删的了        tzy[now].exist = 0;        tzy[now].fact--;        check(root,now);//检查删除后是否需要重构        return;    }    tzy[now].fact--;//路径上的节点的fact都需要更新    //这里是因为数据没有删除"不存在的点"的情况,所以可以这样写    if(val&lt;tzy[now].val) del(tzy[now].l,val);    else del(tzy[now].r,val);}int getrank(int val){//查询第几大    int now = root,rank = 1;    while(now){        if(val&lt;=tzy[now].val) now = tzy[now].l;        else{            rank+=tzy[now].exist+tzy[tzy[now].l].fact;//加上当前点和左子树            now = tzy[now].r;        }    }    return rank;}int getnum(int rank){//查询第k小    int now = root;    while(now){        if(tzy[now].exist&amp;&amp;tzy[now].exist+tzy[tzy[now].l].fact==rank) break;//找到        else if(tzy[tzy[now].l].fact&gt;=rank)            now = tzy[now].l;        else{            rank-=tzy[now].exist+tzy[tzy[now].l].fact;//左子树没有去边找            now = tzy[now].r;        }    }    return tzy[now].val;}void solve(){    int typ,hc;    cin&gt;&gt;typ;    if(typ==1){//插入        cin&gt;&gt;hc;        ins(root,hc);    }    else if(typ==2){//删除        cin&gt;&gt;hc;        del(root,hc);    }    else if(typ==3){//查询x的排名        cin&gt;&gt;hc;        cout&lt;&lt;getrank(hc)&lt;&lt;endl;    }    else if(typ==4){//查询排名为x的数        cin&gt;&gt;hc;        cout&lt;&lt;getnum(hc)&lt;&lt;endl;    }    else if(typ==5){//前驱        cin&gt;&gt;hc;        cout&lt;&lt;getnum(getrank(hc)-1)&lt;&lt;endl;        //注意前驱和后继查询的区别,理一下逻辑其实很简单    }    else if(typ==6){//后继        cin&gt;&gt;hc;        cout&lt;&lt;getnum(getrank(hc+1))&lt;&lt;endl;        //我一开始写的是getnum(getrank(hc)+1)        //如果hc不在树里,就会出错    }}int main(){    int t;    cin&gt;&gt;t;    while(t--) solve();}//我之前的一个疑问,tzy树有0这个假的点作为根,如果从root=0这个点重构了,root没了该怎么办//后来想了想,在重构里now用的是引用,所以引用root的时候,重构时root会被改变//即一开始root是一个val=0的假点,在root子树重构之后,root会被替换成一个真点//洛谷 P3369 【模板】普通平衡树//https://www.luogu.com.cn/problem/P3369<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="范浩强Treap"><a href="#范浩强Treap" class="headerlink" title="范浩强Treap"></a>范浩强Treap</h3><p>常数略大，因为有随机所以可以防毒瘤数据卡。</p><p>数据结构同时对节点的val维护平衡树性质，对节点的key(一个随机数值)则是维护大根堆的性质。每次操作对树进行分裂，传入参数val，分出一颗x小于等于val，另一颗y大于等于val。合并时要兼顾平衡树和大根堆的性质。</p><p>范浩强Treap的期望深度是$O(log(size(T)))$的。</p><p>2020.11.29</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;ctime&gt;#include&lt;random&gt;using namespace std;const int MAXN = 1e5+5;struct Node{    int l,r;    int val,key;//val维护平衡树,key维护大根堆    int size;}fhq[MAXN];int cnt,root;mt19937 rnd(233);//产生速度快,周期大的高质量随机数inline int newnode(int val){    fhq[++cnt].val = val;    fhq[cnt].key = rnd();    fhq[cnt].size = 1;    return cnt;//返回编号}inline void update(int now){//更新大小    fhq[now].size = fhq[fhq[now].l].size+fhq[fhq[now].r].size+1;}void split(int now,int val,int &amp;x,int &amp;y){//树分裂,通过传引用告诉函数应该接在哪里//x是x树的"接口",y是y树的"接口" : 分裂后x树的值&lt;=val,y树&gt;val    if(!now){x=y=0;return;}    if(fhq[now].val&lt;=val){        x = now;        //左子树都比val小,直接把当前节点连带左子树接到x上,下一步继续去右子树找        split(fhq[now].r,val,fhq[now].r,y);//x的接口变成r位置,y的接口不变    }    else{        y = now;        split(fhq[now].l,val,x,fhq[now].l);    }    update(now);}int merge(int x,int y){//传入时保证x树中的val全部小于y树!    if(!x||!y) return x+y;//如果有一方为0,返回另一方    if(fhq[x].key&gt;fhq[y].key){//维护key大根堆的性质,根一定比儿子大        fhq[x].r = merge(fhq[x].r,y);//y的val比x大,所以放右边        update(x);        return x;//返回值是和上一层接起来    }    else{        fhq[y].l = merge(x,fhq[y].l);        update(y);        return y;    }}int x,y,z;//因为之后会经常用到所以这里先定义inline void ins(int val){//按val分xz树,新节点y,合并xy,再和z合并    split(root,val,x,y);    root = merge(merge(x,newnode(val)),y);//第一次插入的点也会在这里被变成root,所以不用担心}inline void del(int val){//1.按val分成xz,按val-1把x分成xy 2.删掉y树的根(只删去一个val) 3.合并xyz    split(root,val,x,z);    split(x,val-1,x,y);    y = merge(fhq[y].l,fhq[y].r);    root = merge(merge(x,y),z);}inline int getrank(int val){//按照val-1分xy,x的大小+1即val排名    split(root,val-1,x,y);    int res = fhq[x].size+1;    root = merge(x,y);     return res;}inline int getnum(int rank){//普普通通找数字    int now = root;    while(now){//询问rank有可能大于总结点数量,这时候会返回0        if(fhq[fhq[now].l].size+1==rank)break;        else if(fhq[fhq[now].l].size&gt;=rank) now=fhq[now].l;        else{            rank-=fhq[fhq[now].l].size+1;            now = fhq[now].r;        }    }    return fhq[now].val;}inline int pre(int val){//按val-1分xy,找x中最大的点    split(root,val-1,x,y);    int now = x;    while(fhq[now].r) now = fhq[now].r;//一直往右边找    int res = fhq[now].val;    merge(x,y);    return res;}inline int nxt(int val){    split(root,val,x,y);    int now = y;    while(fhq[now].l) now = fhq[now].l;    int res = fhq[now].val;    merge(x,y);    return res;}int main(){    int t,typ,x;    cin&gt;&gt;t;    while(t--){        cin&gt;&gt;typ&gt;&gt;x;        if(typ==1) ins(x);        else if(typ==2) del(x);        else if(typ==3) cout&lt;&lt;getrank(x)&lt;&lt;endl;        else if(typ==4) cout&lt;&lt;getnum(x)&lt;&lt;endl;        else if(typ==5) cout&lt;&lt;pre(x)&lt;&lt;endl;        else if(typ==6) cout&lt;&lt;nxt(x)&lt;&lt;endl;    }}//同样思考了一下root初始值的事情,一开始的root是无效值0,这时候是不会执行split的//在插入第一个数值后,root就变成了这个新的节点//洛谷 P3369 【模板】普通平衡树//https://www.luogu.com.cn/problem/P3369<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="文艺平衡树"><a href="#文艺平衡树" class="headerlink" title="文艺平衡树"></a>文艺平衡树</h3><p>文艺平衡树是fhq_treap处理序列上问题(区间操作)的一个例子,进行区间操作一般是用这种通过节点个数进行分裂的split,这里与其他平衡树查询第k大的操作类似。而通过懒惰标记处理旋转区间的操作又与线段树比较相似。</p><p>对于有懒惰标记的点u进行pushdown时，交换左右儿子，并给左右儿子都打上懒惰标记。（妙啊）</p><p>洛谷的这道板子题是保证了是123456这样的的序列，第一是无需ins直接用merge(root,i)来插入,因为i一定比之前出现的数值都大,如果没有这种条件,应该也可以另外建一个数组通过下标来映射数值,或者给Node多加一个参数存实际值。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;ctime&gt;#include&lt;vector&gt;#include&lt;random&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)const int MAXN = 1e5+5;struct Node{    int l,r;    int val,key;    int size;    bool reverse;//旋转的懒惰标记}fhq[MAXN];int cnt,root;mt19937 rnd(233);vector&lt;int&gt;res;//因为行的末尾不能有空格所以我先用一个vector存着inline int newnode(int val){    fhq[++cnt].val = val;    fhq[cnt].key = rnd();    fhq[cnt].size=1;    return cnt;}inline void update(int now){    fhq[now].size = fhq[fhq[now].l].size + fhq[fhq[now].r].size+1;}inline void pushdown(int now){//如果当前节点的儿子有可能之后被动到,就需要向下传递reverse    swap(fhq[now].l,fhq[now].r);    fhq[fhq[now].l].reverse^=1;//儿子的懒惰标记0变1,1变0    fhq[fhq[now].r].reverse^=1;    fhq[now].reverse = 0;//之前疑惑这里为什么不用取反^1的    //其实是因为如果now没有标记reverse,就不用执行pushdown}void split(int now,int siz,int &amp;x,int &amp;y){//按照大小分裂//有点类似取第k大getnum的操作    if(!now){x=y=0;return;}    if(fhq[now].reverse) pushdown(now);//之后分裂操作可能碰到儿子,所以要pushdown    if(fhq[fhq[now].l].size&lt;siz){        x = now;        split(fhq[now].r,siz-fhq[fhq[now].l].size-1,fhq[now].r,y);    }    else{        y = now;        split(fhq[now].l,siz,x,fhq[now].l);    }    update(now);}int merge(int x,int y){    if(!x||!y) return x+y;    if(fhq[x].key&lt;fhq[y].key){//这里其实用什么符号都行        if(fhq[x].reverse) pushdown(x);        fhq[x].r = merge(fhq[x].r,y);        update(x);        return x;    }    else{        if(fhq[y].reverse) pushdown(y);        fhq[y].l = merge(x,fhq[y].l);        update(y);        return y;    }}void reverse(int l,int r){//区间反转//拆成三段,(1,l-1),(l,r),(r+1,n);    int x,y,z;    split(root,l-1,x,y);    split(y,r-l+1,y,z);    fhq[y].reverse^=1;    root=merge(merge(x,y),z);    }void ldr(int now){//最后用中序遍历找到结果    if(!now) return;    if(fhq[now].reverse) pushdown(now);    ldr(fhq[now].l);    res.push_back(fhq[now].val);//推入结果的序列    ldr(fhq[now].r);}int main(){    int n,m;    cin&gt;&gt;n&gt;&gt;m;    rep(i,1,n) root = merge(root,newnode(i));//因为i一定比之前出现的都大才可以这样    int l,r;    while(m--){        cin&gt;&gt;l&gt;&gt;r;        reverse(l,r);    }    res.clear();    ldr(root);    int siz = res.size();    rep(i,0,siz-1){        cout&lt;&lt;res[i];        if(i!=siz-1) cout&lt;&lt;' ';    }    cout&lt;&lt;endl;}//AgOH说,平衡树也是可以处理序列(区间操作)问题的,一般使用的split都是这种按个数siz分裂的split//这题里面有点线段树的味道,通过一个懒惰标记节省了很多没有必要的反转消耗的时间//洛谷P3391 【模板】文艺平衡树//https://www.luogu.com.cn/problem/P3391<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="树上启发式合并"><a href="#树上启发式合并" class="headerlink" title="树上启发式合并"></a>树上启发式合并</h3><p>用于没有修改只有查询的树形问题，且询问以所有节点为根子树（即对每个节点为根的子树都要询问）</p><p>第一遍dfs(init函数)先处理出每个节点的重儿子，第二遍dfs利用头递归处理轻儿子的信息得到子树上的答案，重儿子的信息在cnt里从儿子传给爸爸一直网上传，但他的爸爸不一定是爷爷的重儿子，所以这个dfs有一个opt参数，如果不是则opt=0要清空这颗子树的贡献。</p><p>用可以选择参数的dfs1函数，val=1时是加上轻儿子的贡献，=-1时删去轻儿子的贡献。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;vector&gt;using namespace std;#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define ll long longint cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);const int MAXN = 1e5+5;const int MAXM = 1e5+5;int sz[MAXN],son[MAXN];//子树大小和重儿子//int ans[MAXN];//存最多的颜色和编号和,这题没必要int c[MAXN];//每个点的颜色int cnt[MAXM];//存每种颜色的点数ll bhh[MAXN],bsum;//题目要求颜色的编号和int maxx;//临时变量,用来找最大颜色数量的int n;vector&lt;int&gt; e[MAXN];void init(int now,int pre){//预处理找重儿子    sz[now] = 1;son[now] = -1;    for(auto v:e[now]){        if(v==pre) continue;        init(v,now);        sz[now]+=sz[v];        if(son[now]==-1||sz[v]&gt;sz[son[now]]) son[now]=v;    }}void dfs1(int now,int pre,int val,int rt){//rt是dfs里面当前在统计的子树根//val=1的时候加上子节点的贡献,而-1时是dfs里面opt=1(代表轻儿子)时删去贡献    cnt[c[now]]+=val;    if(val==1){        if(cnt[c[now]]&gt;maxx) maxx=cnt[c[now]],bsum=c[now];        else if(cnt[c[now]]==maxx) bsum+=c[now];    }    //遍历子节点的时候看到什么颜色就检查这个颜色    for(auto v:e[now]) if(v!=pre)dfs1(v,now,val,rt);}void dfs(int now,int pre,int opt){//opt为1代表要清空    for(auto v:e[now]){        if(v!=son[now]&amp;&amp;v!=pre) dfs(v,now,1);//先轻儿子,需要删去    }    if(son[now]!=-1) dfs(son[now],now,0);//处理重儿子    //此时计算轻儿子的贡献,重儿子的在cnt里面祖传上来了    for(auto v:e[now]){        if(v!=pre&amp;&amp;v!=son[now]) dfs1(v,now,1,now);//统计轻儿子上的结果    }    cnt[c[now]]++;//也要加上当前节点    if(cnt[c[now]]&gt;maxx) maxx=cnt[c[now]],bsum=c[now];    //这里我一开始用的cnt[c[now]]&gt;cnt[ans[now]],但是c[now]和ans[rt]相同时出问题    else if(cnt[c[now]]==maxx) bsum+=c[now];    bhh[now] = bsum;//记录结果    //减去轻儿子的贡献    if(opt){        cnt[c[now]]--;        for(auto v:e[now])            if(v!=pre) dfs1(v,now,-1,now);        maxx = bsum = 0;//删去轻儿子这颗子树对bsum和maxx的贡献    }}int rt = 1;//题目默认1为根节点int main(){    cin&gt;&gt;n;    rep(i,1,n) cin&gt;&gt;c[i];    int u,v;    rep(i,1,n-1){        cin&gt;&gt;u&gt;&gt;v;        e[u].push_back(v);        e[v].push_back(u);    }    init(rt,-1);    dfs(rt,-1,1);    rep(i,1,n){        cout&lt;&lt;bhh[i];        if(i!=n) cout&lt;&lt;' ';      }    cout&lt;&lt;endl;}//没有修改只有查询的树形问题//且询问以所有节点为根子树（即对每个节点为根的子树都要询问）。//洛谷CF600E Lomsat gelral//https://www.luogu.com.cn/problem/CF600E<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="树形DP"><a href="#树形DP" class="headerlink" title="树形DP"></a>树形DP</h2><h3 id="树上背包"><a href="#树上背包" class="headerlink" title="树上背包"></a>树上背包</h3><p>每个节点要做一次背包，父节点有p个儿子节点时，每个儿子节点看作一组物品每组m个(儿子dp的时候的结果)，做分组背包即可。</p><p>状态转移方程为：$dp[x][i]=max(dp[son_i][j])+score(x)$。j从1到容量m，表示这组里选j个。</p><pre class="line-numbers language-C++" data-language="C++"><code class="language-C++">#include&lt;iostream&gt;#include&lt;cstring&gt;using namespace std;#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=(b);i--)int cansel_sync =(ios::sync_with_stdio(0),cin.tie(0),0);const int MAXN = 333;struct Node{int v,nxt;//next同一个父亲的下一个点int w;//权值,这题用不到}edges[MAXN];int head[MAXN];//记录u为起点的第一条边编号int cnt;//总边数int minn;inline void add(int u,int v){//增加边edges[++cnt] = Node{v,head[u]};head[u]=cnt;}int n,m;int dp[MAXN][MAXN],val[MAXN];void dfs(int now){dp[now][1] = val[now];//肯定要选定自己for(int i = head[now];i!=-1;i=edges[i].nxt){int v = edges[i].v;dfs(v);//分组背包repb(j,m,1){//当前u背包容量rep(k,0,j-1){//从v儿子里拿几个dp[now][j]=max(dp[now][j],dp[now][j-k]+dp[v][k]);//状态转移}}}}int main(){while(cin&gt;&gt;n&gt;&gt;m&amp;&amp;m&amp;&amp;n){memset(head,-1,sizeof(head));memset(dp,0,sizeof(dp));cnt = 0;int fa;//fa表示依赖的点rep(i,1,n){cin&gt;&gt;fa&gt;&gt;val[i];add(fa,i);}m++;//因为之后的dp[0][m]算上了0点,所以要+1val[0]=0;//构造虚节点0dfs(0);cout&lt;&lt;dp[0][m]&lt;&lt;endl;}}//洛谷P2014 [CTSC1997]选课//https://www.luogu.com.cn/problem/P2014<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACM模板 </tag>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Codeforces Round /#683 (Div. 1) B.Catching Cheaters(DP最大子段和+LCS)</title>
      <link href="2020/11/17/cf683-b/"/>
      <url>2020/11/17/cf683-b/</url>
      
        <content type="html"><![CDATA[<h1 id="题目地址"><a href="#题目地址" class="headerlink" title="题目地址"></a>题目地址</h1><p><a href="https://codeforces.com/problemset/problem/1446/B">https://codeforces.com/problemset/problem/1446/B</a></p><h1 id="题目大意"><a href="#题目大意" class="headerlink" title="题目大意"></a>题目大意</h1><p>定义$S(A,B)=4\cdot LCS(A,B)-\left|A\right|-\left|B\right|$</p><p>给两个串串A和B，求AB的两个的子串CD（substring，要求连续），使$S(C,D)$最大。</p><h1 id="解法"><a href="#解法" class="headerlink" title="解法"></a>解法</h1><p>一开始我想的是先求出lcs数组</p><p>再用$res_{i,j}=4\cdot lcs_{i,j}-i-j$记录类似lcs的前缀和。但是后来发现很容易就能推翻，lcs是不能使用这样前缀和的思想的。比如字符串aaa和aaaa，算C为[1,3],D为空结果的就立马会出现问题。</p><p>所以不能这样搞。</p><p>我们先思考如果这些<strong>子串必须都从头开始</strong>，即只能从末尾删去字符，该怎么写：</p><p>可以得到<strong>递推式</strong>$dp_{i,j}=dp_{i-1,j-1}+2$如果ai=bi,(增加了长度1,$1\cdot 4-2=2$)</p><p>$dp_{i,j}=max(dp_{i-1,j},dp_{i,j-1})-1$如果ai!=bi.</p><p>这时候增加一个:也可以从头删去字符，想到怎么写了吗？</p><p>就是利用了<strong>最大连续子段和</strong>的思想：如果在一个位置的$dp_{i,j}&lt;0$则把它归零。</p><p>完了。</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define ll long longint cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);ll gcd(ll a,ll b){ while(b^=a^=b^=a%=b); return a;}string a,b;int n,m;const int MAXN = 5005;int dp[MAXN][MAXN];int res;void solve(){    res = 0;    cin&gt;&gt;n&gt;&gt;m;    cin&gt;&gt;a&gt;&gt;b;    rep(i,1,n){        rep(j,1,m){            if(a[i-1]==b[j-1]) dp[i][j] = dp[i-1][j-1]+2;            else dp[i][j] = max(dp[i-1][j],dp[i][j-1])-1;            dp[i][j] = max(dp[i][j],0);            res = max(res,dp[i][j]);        }    }    cout&lt;&lt;res&lt;&lt;endl;}int main(){    solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 字符串 </tag>
            
            <tag> 题解 </tag>
            
            <tag> Codeforces </tag>
            
            <tag> DP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WSL (Windows Subsystem for Linux)</title>
      <link href="2020/11/15/wsl/"/>
      <url>2020/11/15/wsl/</url>
      
        <content type="html"><![CDATA[<h1 id="参考的几个帖子"><a href="#参考的几个帖子" class="headerlink" title="参考的几个帖子"></a>参考的几个帖子</h1><p>（黑体为重要帖子）</p><p><strong>windows安装wsl</strong></p><p><a href="https://www.jianshu.com/p/6b02948b3d37">https://www.jianshu.com/p/6b02948b3d37</a></p><p>username开头不能是大写</p><p><a href="https://zhuanlan.zhihu.com/p/111858170">https://zhuanlan.zhihu.com/p/111858170</a></p><p>vim基本操作</p><p><a href="https://www.cnblogs.com/chengjiawei/p/9339951.html">https://www.cnblogs.com/chengjiawei/p/9339951.html</a></p><p>vim保存退出</p><p><a href="https://www.cnblogs.com/chen-nn/p/11531932.html">https://www.cnblogs.com/chen-nn/p/11531932.html</a></p><p> <strong>vscode wsl</strong></p><p><a href="https://zhuanlan.zhihu.com/p/104060131?utm_source=wechat_session">https://zhuanlan.zhihu.com/p/104060131?utm_source=wechat_session</a></p>]]></content>
      
      
      <categories>
          
          <category> 计算机知识 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> vscode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用Git Bash命令行提交修改</title>
      <link href="2020/11/09/git-update/"/>
      <url>2020/11/09/git-update/</url>
      
        <content type="html"><![CDATA[<h1 id="SSH-Key的获取"><a href="#SSH-Key的获取" class="headerlink" title="SSH-Key的获取"></a>SSH-Key的获取</h1><p>这篇帖子讲的非常详细了</p><p><a href="https://www.cnblogs.com/duanming/p/11830251.html">https://www.cnblogs.com/duanming/p/11830251.html</a></p><h1 id="连接到github仓库"><a href="#连接到github仓库" class="headerlink" title="连接到github仓库"></a>连接到github仓库</h1><h2 id="初始化本地仓库"><a href="#初始化本地仓库" class="headerlink" title="初始化本地仓库"></a>初始化本地仓库</h2><ul><li>先在命令行进入想要作为仓库的位置，输入</li></ul><pre class="line-numbers language-none"><code class="language-none">git init<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>  初始化为本地仓库（之后文件夹里多了一个.git）</p><h2 id="连接到远程仓库"><a href="#连接到远程仓库" class="headerlink" title="连接到远程仓库"></a>连接到远程仓库</h2><ul><li>在github的仓库页面把这个东西复制下来</li></ul><p><img src="/img/git_update/git-ssh.png" alt="git-ssh"></p><ul><li>接着添加一个远程仓库，输入</li></ul><pre class="line-numbers language-none"><code class="language-none">git remote add origin 这里填刚刚复制的东西以我的为例就是git remote add origin git@github.com:REXWindW/REXWindW.git<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="关联分支并push"><a href="#关联分支并push" class="headerlink" title="关联分支并push"></a>关联分支并push</h2><ul><li>接着我们开始我们的第一次push，把当前分支master推送到远程仓库。</li></ul><pre class="line-numbers language-none"><code class="language-none">git push -u origin master<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>因为远程库是空的，第一次推送master增加一个<code>-u</code>参数，这样Git不仅仅会把本地的master分支的内容推送到远程GitHub仓库中新的master分支，还会<strong>把本地的master分支和远程的master分支关联起来</strong>。</p><p>在以后的推送和拉取就可以简化命令了，简化命令即</p><pre class="line-numbers language-none"><code class="language-none">git push<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>但是之后每次push之前其实还需要进行一些操作</strong></p><h1 id="之后要怎么push"><a href="#之后要怎么push" class="headerlink" title="之后要怎么push"></a>之后要怎么push</h1><h2 id="常用指令"><a href="#常用指令" class="headerlink" title="常用指令"></a>常用指令</h2><pre class="line-numbers language-none"><code class="language-none">git add .  //暂存所有的更改，注意后面有个点!这样才能把所有的更改暂存git checkout . // 丢弃所有的更改git status // 查看文件状态git commit -m "本次要提交的概要信息" //加上这次提交的备注git push //提交<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="操作流程"><a href="#操作流程" class="headerlink" title="操作流程"></a>操作流程</h2><p><img src="/img/git_update/update.png" alt="update"></p>]]></content>
      
      
      <categories>
          
          <category> 计算机知识 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mathjax内联公式出错的问题与解决方案</title>
      <link href="2020/11/08/matjax-tips/"/>
      <url>2020/11/08/matjax-tips/</url>
      
        <content type="html"><![CDATA[<h1 id="问题的出现"><a href="#问题的出现" class="headerlink" title="问题的出现"></a>问题的出现</h1><p>今天写了公式特别多的一篇博客：FFT学习笔记</p><p>然后我的博客的<strong>内联公式</strong>显示出了大量的问题。</p><p><img src="/img/mathjax_tips/error.png" alt="mathjax">  </p><h1 id="失败的尝试"><a href="#失败的尝试" class="headerlink" title="失败的尝试"></a>失败的尝试</h1><p>经过转爷的提醒，纠正了之前的错误写法</p><p>$ x=1 $</p><p>改成了正确的</p><p>$x=1$</p><p>后来还是不行，以为是大括号的问题</p><p>最后发现，并不是大括号的问题，而是mathjax中内联公式就最好不要用太多\frac之类的这种转义字符了</p><p><img src="/img/mathjax_tips/mathjax.png" alt="mathjax">  </p><h1 id="最后是怎么解决的呢？"><a href="#最后是怎么解决的呢？" class="headerlink" title="最后是怎么解决的呢？"></a>最后是怎么解决的呢？</h1><p>参考了这个帖子：<a href="https://www.jianshu.com/p/7ab21c7f0674">https://www.jianshu.com/p/7ab21c7f0674</a></p><p>这个帖子先提到了要更换Hexo的markdown渲染引擎，<a href="https://link.jianshu.com/?t=https://github.com/sun11/hexo-renderer-kramed">hexo-renderer-kramed</a>引擎是在默认的渲染引擎<a href="https://link.jianshu.com/?t=https://github.com/hexojs/hexo-renderer-marked">hexo-renderer-marked</a>的基础上修改了一些bug，两者比较接近，也比较轻量级。但是我的引擎已经是这个了，所以问题再下一步。</p><p>然后，跟换引擎后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为<a href="https://link.jianshu.com/?t=https://github.com/sun11/hexo-renderer-kramed">hexo-renderer-kramed</a>引擎也有语义冲突的问题。接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的<strong>escape变量</strong>的值做相应的修改：</p><pre class="line-numbers language-ruby" data-language="ruby"><code class="language-ruby"><span class="token operator">/</span><span class="token operator">/</span>  escape<span class="token punctuation">:</span> <span class="token regex">/^\\([\\`*{}\[\]()#$+\-.!_&gt;])/</span><span class="token punctuation">,</span>  escape<span class="token punctuation">:</span> <span class="token regex">/^\\([`*\[\]()#$+\-.!_&gt;])/</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这一步是在原基础上<strong>取消了对,{,}的转义(escape)<strong>。<br>同时把第20行的</strong>em变量</strong>也要做相应的修改。</p><pre class="line-numbers language-ruby" data-language="ruby"><code class="language-ruby"><span class="token operator">/</span><span class="token operator">/</span>  em<span class="token punctuation">:</span> <span class="token regex">/^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span><span class="token punctuation">,</span>  em<span class="token punctuation">:</span> <span class="token regex">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>重新启动hexo（先clean再generate）,问题完美解决。哦，如果不幸还没解决的话，看看是不是还需要在使用的主题中配置mathjax开关。</p><h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><p>在以上步骤之后，还是存在一些显示错误</p><p><img src="/img/mathjax_tips/cdot.png" alt="cdot"></p><p>我自己猜测的是mathjax最好不要用*号来乘，可以用点乘和叉乘</p><p>如下：</p><table><thead><tr><th>符号</th><th>写法</th></tr></thead><tbody><tr><td>叉乘</td><td>a \times b</td></tr><tr><td>点除</td><td>a \div b</td></tr><tr><td>点乘</td><td>a \cdot b</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 博客维护 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mathjax </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模板整理(持续更新)</title>
      <link href="2020/11/07/banzi/"/>
      <url>2020/11/07/banzi/</url>
      
        <content type="html"><![CDATA[<h1 id="小tips"><a href="#小tips" class="headerlink" title="小tips"></a>小tips</h1><h2 id="CF头"><a href="#CF头" class="headerlink" title="CF头"></a>CF头</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;#include&lt;map&gt;#include&lt;queue&gt;#include&lt;cmath&gt;using namespace std;template&lt;class T&gt;inline void read(T &amp;x){x=0;char o,f=1;while(o=getchar(),o&lt;48)if(o==45)f=-f;do x=(x&lt;&lt;3)+(x&lt;&lt;1)+(o^48);while(o=getchar(),o&gt;47);x*=f;}int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define ull unsigned long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=b;i--)#define mkp make_pair#define ft first#define sd second#define log(x) (31-__builtin_clz(x))#define INF 0x3f3f3f3ftypedef pair&lt;int,int&gt; pii;typedef pair&lt;ll,ll&gt; pll;ll gcd(ll a,ll b){ while(b^=a^=b^=a%=b); return a; }//#define INF 0x7fffffff<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="集合操作"><a href="#集合操作" class="headerlink" title="集合操作"></a>集合操作</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">//并交集vector&lt;int&gt; ANS;set_union(s1.begin(),s1.end(),s2.begin(),s2.end(),inserter(ANS,ANS.begin()));//set_intersection()//通过迭代器遍历集合set&lt;char&gt;::iterator iter = temp1.begin();while (iter!=temp1.end()){cout&lt;&lt;*iter;iter++;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="快读快写"><a href="#快读快写" class="headerlink" title="快读快写"></a>快读快写</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;class T&gt;inline void read(T &amp;x){x=0;char o,f=1;while(o=getchar(),o&lt;48)if(o==45)f=-f;do x=(x&lt;&lt;3)+(x&lt;&lt;1)+(o^48);while(o=getchar(),o&gt;47);x*=f;}template&lt;class T&gt;void wt(T x){//快写   if(x &lt; 0) putchar('-'), x = -x;   if(x &gt;= 10) wt(x / 10);   putchar('0' + x % 10);}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="int128"><a href="#int128" class="headerlink" title="__int128"></a>__int128</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">inline __int128 read(){    __int128 x=0,f=1;    char ch=getchar();    while(ch&lt;‘0‘||ch&gt;‘9‘){        if(ch==‘-‘)            f=-1;        ch=getchar();    }    while(ch&gt;=‘0‘&amp;&amp;ch&lt;=‘9‘){        x=x*10+ch-‘0‘;        ch=getchar();    }    return x*f;}inline void print(__int128 x){    if(x&lt;0){        putchar(‘-‘);        x=-x;    }    if(x&gt;9)        print(x/10);    putchar(x%10+‘0‘);}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><h2 id="单调队列"><a href="#单调队列" class="headerlink" title="单调队列"></a>单调队列</h2><h3 id="普通单调队列"><a href="#普通单调队列" class="headerlink" title="普通单调队列"></a>普通单调队列</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">struct node{//结构体，存储数值和位置 int data;int order;//原本数列里的位置 };const int MAXN = 1e6+5;deque&lt;node&gt;dq_min;deque&lt;node&gt;dq_max;int n,k;int a[MAXN];int res_min[MAXN];int res_max[MAXN];int main(){cin&gt;&gt;n&gt;&gt;k;for(int i=1;i&lt;=n;i++){cin&gt;&gt;a[i];}for(int i=1;i&lt;=n;i++){//如果队列头超过这k的范围，则出队 if(!dq_max.empty()&amp;&amp;dq_max.front().order&lt;i-k+1) dq_max.pop_front();if(!dq_min.empty()&amp;&amp;dq_min.front().order&lt;i-k+1) dq_min.pop_front();//新元素从队尾插进来，队尾没用的元素在这里出队 while(!dq_max.empty()&amp;&amp;dq_max.back().data&lt;=a[i]) dq_max.pop_back();dq_max.push_back(node{a[i],i});while(!dq_min.empty()&amp;&amp;dq_min.back().data&gt;=a[i]) dq_min.pop_back();dq_min.push_back(node{a[i],i});//存储区间最大最小值 res_max[i] = dq_max.front().data;res_min[i] = dq_min.front().data;}for(int i=k;i&lt;=n;i++){cout&lt;&lt;res_min[i];if(i!=n) cout&lt;&lt;' ';}cout&lt;&lt;endl;for(int i=k;i&lt;=n;i++){cout&lt;&lt;res_max[i];if(i!=n) cout&lt;&lt;' ';}cout&lt;&lt;endl;}//洛谷 P1886 滑动窗口 /【模板】单调队列 https://www.luogu.com.cn/problem/P1886<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="矩阵上的单调队列"><a href="#矩阵上的单调队列" class="headerlink" title="矩阵上的单调队列"></a>矩阵上的单调队列</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">   deque&lt;node&gt; dq;//每行进行单调队列//本来想建maxn个deque的，但是这里发现可以滚动for(int i=1;i&lt;=n;i++){for(int j=1;j&lt;=m;j++){if(!dq.empty()&amp;&amp;dq.front().order&lt;j-k+1) dq.pop_front();while(!dq.empty()&amp;&amp;a[i][j]&gt;=dq.back().data) dq.pop_back();dq.push_back(node{a[i][j],j});res[i][j] = dq.front().data;}dq.clear();}//对k开始的每列进行单调队列 for(int j=k;j&lt;=m;j++){for(int i=1;i&lt;=n;i++){if(!dq.empty()&amp;&amp;dq.front().order&lt;i-k+1) dq.pop_front();while(!dq.empty()&amp;&amp;dq.back().data&lt;=res[i][j]) dq.pop_back();dq.push_back(node{res[i][j],i});res[i][j] = dq.front().data;}dq.clear();}ll msum = 0;for(int i=k;i&lt;=n;i++){for(int j=k;j&lt;=m;j++){msum+=res[i][j];}}cout&lt;&lt;msum&lt;&lt;endl;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="主席树"><a href="#主席树" class="headerlink" title="主席树"></a>主席树</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#define mid ((l+r)&gt;&gt;1)const int MAXN = 2e5+5;int n,q,m,cnt=0;int a[MAXN],b[MAXN]; int T[MAXN];//记录每个时间上树的根节点 int sum[MAXN&lt;&lt;5];//记录节点覆盖区间内数字的数量 int L[MAXN&lt;&lt;5],R[MAXN&lt;&lt;5];//记录左右儿子 int build(int l,int r){//一开始通过递归建树int pos = ++cnt;//建立新的节点，节点数cnt自增sum[pos]=0;//时间0时是没有任何数字的if(l&lt;r){L[pos]=build(l,mid);R[pos]=build(mid+1,r);}return pos; }int update(int pre,int l,int r,int x){//pre是在pos位置上一个时间的节点编号int pos = ++cnt;L[pos]=L[pre];R[pos]=R[pre];//pre是上个时间点在如今pos位置上的节点//这里相当于是把这个新的节点接到相应的位置sum[pos] = sum[pre]+1;//这一整条链上各点的权值都+1(叶子上加了1，上面的都受影响)if(l&lt;r){//找到长度为一个数的节点就结束递归了if(x&lt;=mid) L[pos] = update(L[pre],l,mid,x);else R[pos] = update(R[pre],mid+1,r,x);//判断数值x的这个节点在当前节点的左儿子还是右儿子//这样一直沿着这条链往下，建立新的节点。//因为pos节点的儿子中新建立的节点需要返回编号}return pos;}int query(int u,int v,int l,int r,int k){if(l&gt;=r) return l;int x = sum[L[v]]-sum[L[u]];//计算左子树中数的数量x if(x&gt;=k) return query(L[u],L[v],l,mid,k);//x比k大，说明第k大的数在左子树中else return query(R[u],R[v],mid+1,r,k-x);}int main(){cin&gt;&gt;n&gt;&gt;q;rep(i,1,n){cin&gt;&gt;a[i];b[i]=a[i];}sort(b+1,b+1+n);m = unique(b+1,b+1+n)-b-1;//排序后可以用unique函数去重 T[0] = build(1,m);rep(i,1,n){int t = lower_bound(b+1,b+1+m,a[i]) - b;//找到对应的离散化后的数值T[i] = update(T[i-1],1,m,t);//建立新时间的根节点，并且建新的链并接上去 }while(q--){int u,v,k;cin&gt;&gt;u&gt;&gt;v&gt;&gt;k;//找u到v区间内第k大的数 int t = query(T[u-1],T[v],1,m,k); cout&lt;&lt;b[t]&lt;&lt;endl;//输出对应的原值} }//洛谷P3834 【模板】可持久化线段树 2（主席树）//https://www.luogu.com.cn/problem/P3834//其实就是拿的大爹板子然后自己加了很多注释：//https://www.luogu.com.cn/blog/bestFy0731/solution-p3834<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="LCA"><a href="#LCA" class="headerlink" title="LCA"></a>LCA</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#define log(x) (31-__builtin_clz(x))//谢谢hjtconstexpr int LOGN = log(MAXN)/log(2)+5;int z,m,n;int fa[MAXN],p[MAXN][LOGN];int L[MAXN];//记录深度 int lg[MAXN];vector&lt;int&gt; e[MAXN];void init_lg(){//预处理lg，给之后计算省时间 lg[1]=0;for(int i=2;i&lt;MAXN;i++) lg[i]=lg[i-1]+( (1&lt;&lt;(lg[i-1]+1))==i );} void dfs(int x){for(auto u:e[x]){L[u]=L[x]+1;dfs(u);}}void preprocess(int n){for(int i=1;i&lt;=n;i++)for(int j=0;1&lt;&lt;j&lt;=n;j++) p[i][j]=-1;//有些倍增后出界的，用-1标记 for(int i=1;i&lt;=n;i++)p[i][0]=fa[i];//倍增长度为1的时候指向格子的父节点for(int j=1;1&lt;&lt;j&lt;=n;j++)for(int i=1;i&lt;=n;i++)if(p[i][j-1]!=-1) p[i][j]=p[p[i][j-1]][j-1];//两端长度加起来正好是两倍即次数+1 }int LCA(int u,int v){if(L[u]&lt;L[v]) swap(u,v);//u成为离根节点更远的int log = lg[L[u]];//找到L[u]的二进制最高位 for(int i=log;i&gt;=0;i--)if(L[u]-(1&lt;&lt;i)&gt;=L[v]) u=p[u][i];//u往上爬到与v同高if(u==v) return u;for(int i=log;i&gt;=0;i--)if(p[u][i]!=-1&amp;&amp;p[u][i]!=p[v][i]){//找公共祖先,逼近它但是又不超过它 u=p[u][i]; v=p[v][i]; } return fa[u];} int main(){init_lg();//预处理log2x，给之后的计算省时间 int z,q,u,v;string aa,bb;cin&gt;&gt;z;while(z--){map&lt;string,int&gt;mp;cin&gt;&gt;n&gt;&gt;q;if(n==1){while(q--){cin&gt;&gt;aa&gt;&gt;bb;cout&lt;&lt;0&lt;&lt;endl;}continue;}int countt = 0;for(int i=1;i&lt;=n;i++){fa[i]=i;}for(int i=1;i&lt;n;i++){cin&gt;&gt;aa&gt;&gt;bb;int &amp;a = mp[aa];int &amp;b = mp[bb];if(!a) a=++countt;if(!b) b=++countt;fa[a]=b;e[b].push_back(a);}int rt;for(int i=1;i&lt;=n;i++){if(fa[i]==i) {rt=i;break;}}L[rt]=1;dfs(rt);preprocess(n);int res;while(q--){cin&gt;&gt;aa&gt;&gt;bb;int &amp;a = mp[aa];int &amp;b = mp[bb];int lc = LCA(a,b);res = L[a]-L[lc];if(lc!=b) res++;cout&lt;&lt;res&lt;&lt;endl;}for(int i=1;i&lt;=n;i++) e[i].clear();}}//HDOJ4547 http://acm.hdu.edu.cn/showproblem.php?pid=4547<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="树链剖分"><a href="#树链剖分" class="headerlink" title="树链剖分"></a>树链剖分</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">ll ans;int da[MAXN];//记录dfn序上的各点数值用来初始化线段树int n,m,rt,p;//节点数，询问数，根，模数 struct tree{int sum;int lazy;};struct St{//线段树 tree t[MAXN&lt;&lt;2];void pushup(int pos){ t[pos].sum=(t[pos&lt;&lt;1].sum+t[pos&lt;&lt;1|1].sum) %p;return;}void pushdown(int l,int r,int pos){if(!t[pos].lazy) return;int mid = (l+r)&gt;&gt;1;t[pos&lt;&lt;1].sum += t[pos].lazy*(mid-l+1);t[pos&lt;&lt;1].sum %= p;//取模 t[pos&lt;&lt;1|1].sum += t[pos].lazy*(r-(mid+1)+1);t[pos&lt;&lt;1|1].sum %= p;//取模 t[pos&lt;&lt;1].lazy += t[pos].lazy;t[pos&lt;&lt;1].lazy %= p;//取模 t[pos&lt;&lt;1|1].lazy += t[pos].lazy;t[pos&lt;&lt;1|1].lazy %= p;//取模 t[pos].lazy = 0; }void build(int l,int r,int pos){ t[pos].sum = t[pos].lazy = 0;if(l==r){t[pos].sum = da[l];return;}int mid = (l+r)&gt;&gt;1;build(l,mid,pos&lt;&lt;1);build(mid+1,r,pos&lt;&lt;1|1);pushup(pos);}void update(int L,int R,int l,int r,int pos,int v){if(L&lt;=l&amp;&amp;r&lt;=R){t[pos].sum += v*(r-l+1);t[pos].lazy += v;t[pos].lazy %= p;//取模 return;}if(r&lt;L||l&gt;R) return;pushdown(l,r,pos);int mid = (l+r)&gt;&gt;1;update(L,R,l,mid,pos&lt;&lt;1,v);update(L,R,mid+1,r,pos&lt;&lt;1|1,v);pushup(pos);}void query(int L,int R,int l,int r,int pos){if(L&lt;=l&amp;&amp;r&lt;=R){ans += t[pos].sum;ans%=p;//取模 return;}if(r&lt;L||R&lt;l) return;pushdown(l,r,pos);int mid = (l+r)&gt;&gt;1;query(L,R,l,mid,pos&lt;&lt;1);query(L,R,mid+1,r,pos&lt;&lt;1|1);return;}//查询和修改，为了简化参数，我又写了两个 ll tquery(int L,int R){ans = 0;query(L,R,1,n,1);return ans; }void tupdate(int L,int R,int v){update(L,R,1,n,1,v);} };//树结构 vector&lt;int&gt; e[MAXN];//记录边int a[MAXN];//记录编号对应节点的初始数值//树剖部分St segt;int si[MAXN],dep[MAXN],fa[MAXN],rem[MAXN],dfn[MAXN],top[MAXN];int dfn_num;void dfs1(int x,int faa){//预处理出fa,dep,si,remint ma = 0;//用来x的重儿子，记录最大的sizesi[x] = 1;for(auto v:e[x]){if(v==faa) continue;//跳过父亲节点 dep[v] = dep[x]+1;//更新儿子的dep dfs1(v,x);si[x] += si[v];//x的size加上当前儿子的sizefa[v] = x;//标记v的父节点为x if(si[v]&gt;ma){ma = si[v];rem[x] = v;//记录重儿子 } }} void dfs2(int x,int faa){//预处理出dfn，topif(rem[faa]==x) top[x] = top[faa];//同一条重链同一个top else top[x] = x;//否则为重链头dfn[x] = ++dfn_num;//更新树剖序，同时下标自增da[dfn_num] = a[x];if(rem[x]) dfs2(rem[x],x);//优先遍历重儿子for(auto v:e[x]){if(v==faa) continue;if(v==rem[x]) continue;//重儿子之前已经遍历过了dfs2(v,x); } }inline ll cal(int L,int R){return segt.tquery(L,R);}void init(){//初始化dfn_num=0;dfs1(rt,0);dfs2(rt,0);segt.build(1,n,1);//这里通过da来初始化线段树 }ll query(int x,int y){ll res=0;while(top[x]!=top[y]){//跳到同一条重链 if(dep[top[x]]&lt;dep[top[y]])swap(x,y);//重链深度更大的点优先往上跳res += cal(dfn[top[x]],dfn[x]);//算上这条重链的 res%=p;//取模 x = fa[top[x]];//跳到重链头的父节点上 }//跳出这个循环时，xy已经在同一个重链上了res += cal(min(dfn[x],dfn[y]),max(dfn[x],dfn[y]));res%=p;//取模 //xy不确定顺序对不对，所以取minmax return res;}void update(int x,int y,int v){ll res=0;while(top[x]!=top[y]){//跳到同一条重链 if(dep[top[x]]&lt;dep[top[y]])swap(x,y);//重链深度更大的点优先往上跳segt.tupdate(dfn[top[x]],dfn[x],v);//更新数值 x = fa[top[x]];//跳到重链头的父节点上 }//跳出这个循环时，xy已经在同一个重链上了segt.tupdate(min(dfn[x],dfn[y]),max(dfn[x],dfn[y]),v);//xy不确定顺序对不对，所以取minmax}int main(){cin&gt;&gt;n&gt;&gt;m&gt;&gt;rt&gt;&gt;p;//节点数，操作数，根节点序号，模数for(int i=1;i&lt;=n;i++){cin&gt;&gt;a[i];//记录初始数值 a[i] = a[i]%p;} int x,y;for(int i=1;i&lt;=n;i++) e[i].clear();for(int i=1;i&lt;n;i++){cin&gt;&gt;x&gt;&gt;y;e[x].push_back(y);e[y].push_back(x);}init();int v;int typ;while(m--){cin&gt;&gt;typ;if(typ==1){//简单路径上修改 cin&gt;&gt;x&gt;&gt;y&gt;&gt;v;update(x,y,v);}else if(typ==2){//简单路径上查询 cin&gt;&gt;x&gt;&gt;y;cout&lt;&lt;query(x,y)&lt;&lt;endl;}else if(typ==3){//子树上修改 cin&gt;&gt;x&gt;&gt;v;segt.tupdate(dfn[x],dfn[x]+si[x]-1,v);}else if(typ==4){//子树上查询 cin&gt;&gt;x;cout&lt;&lt;segt.tquery(dfn[x],dfn[x]+si[x]-1)&lt;&lt;endl;}}} //洛谷 P3384 【模板】轻重链剖分 https://www.luogu.com.cn/problem/P3384<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="splay"><a href="#splay" class="headerlink" title="splay"></a>splay</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int root,tot,n;struct Node{    int ch[2];//左右儿子    int val;//值    int fa;//父节点    int size;//子树大小    int cnt;//计数 }t[MAXN];inline void pushup(int x){//维护节点的size t[x].size = t[x].cnt;//自己重复的次数先累计if(t[x].ch[0]) t[x].size+=t[t[x].ch[0]].size;if(t[x].ch[1]) t[x].size+=t[t[x].ch[1]].size;//如果有儿子,把儿子的size加到自己//同线段树的pushup必须先处理儿子再处理父节点}void rotate(int x){//旋转操作int y=t[x].fa, z=t[y].fa;int k=(t[y].ch[1]==x);//x是y的左还是右儿子 t[z].ch[t[z].ch[1]==y] = x;//x换到y原来在的位置t[x].fa = z; t[y].ch[k] = t[x].ch[k^1];//y顶下来x的儿子，放在x原来对y的位置 t[t[x].ch[k^1]].fa = y;t[x].ch[k^1] = y;//y换到 x原来在y的 相对的 位置t[y].fa = x;pushup(y),pushup(x);//更新size,因为y是儿子,所以先y }//这里的异或是用来调整0和1(左右儿子) inline void splay(int x,int goal){//找到x并把x旋转为goal的儿子根节点 while(t[x].fa!=goal){//x一直旋转到成为goal的儿子int y=t[x].fa,z=t[y].fa; if(z!=goal)//如果y不是根节点,分两类讨论 (t[z].ch[0]==x)^(t[y].ch[0]==y)?rotate(x):rotate(y);//x和y分别是y和z的同一侧儿子,先转y再转x;不同则先转x转两次 rotate(x);//无论三种情况中的哪一种都要最后转x }if(goal==0) root=x;//若goal是0,根节点更新为x}inline void find(int x){//查找x的位置,并旋转x到根节点,类似二分,O(logn) int u = root;if(!u) return;//树是空的情况while(t[u].ch[x&gt;t[u].val]&amp;&amp;x!=t[u].val)//找到x,不一定找得到u = t[u].ch[x&gt;t[u].val];//左儿子小，右儿子大 splay(u,0);//当前位置旋转到根节点.根节点的fa存0}inline void insert(int x){//类似find,如果插入的数已经存在,可以在找到的节点计数 int u = root,fu = 0;//当前位置u,u的爸爸fu while(u&amp;&amp;t[u].val!=x){//找合适位置,u找到空的地方也会停止fu = u;u = t[u].ch[x&gt;t[u].val];}if(u) t[u].cnt++;//已有这个数字的情况,计数 else{u = ++tot;//节点总数tot+1if(fu)//这时候fu是上一个u即新插入u的父节点,如果父节点不是0t[fu].ch[x&gt;t[fu].val]=u;t[u].ch[0] = t[u].ch[1] = 0;//这个新节点没儿子t[u].fa = fu;//父亲t[u].val = x;//数值t[u].cnt = 1;//计数t[u].size  = 1;//大小}splay(u,0);}inline int Next(int x,int f){find(x); int u=root;//根节点，此时x的父节点(存在的话)就是根节点 //这里sls回答了我的疑问//"splay中不一定有x这个节点,那么它splay到根的就直接可以满足了"//"如果有这个点的话就要在splay上再找一波(因为当前的根就是x这个点)"if(t[u].val&gt;x&amp;&amp;f)return u;//如果当前节点的值大于x并且要查找的是后继    if(t[u].val&lt;x&amp;&amp;!f)return u;//如果当前节点的值小于x并且要查找的是前驱     //上面两个是x不在splay中的情况u = t[u].ch[f];//后继在根右边找,前驱在左边找while(t[u].ch[f^1]) u = t[u].ch[f^1];//左半边要往右跳找最大的,右半边往左跳 return u;}inline void Delete(int x){//删除x int last = Next(x,0);//查找x的前驱int next = Next(x,1);//找x的后继splay(last,0);splay(next,last);//前驱节点转到根节点,后继转到根的儿子上//操作完之后.后继是前驱的右儿子,x是前驱的左儿子,而且x是叶子节点 int del = t[next].ch[0];//后继的左儿子xif(t[del].cnt&gt;1){t[del].cnt--;//如果有多个x,则x的计数减少一个splay(del,0);//这个splay还重新pushup计算了del的子树}elset[next].ch[0]=0;//因为是左儿子是叶子节点,直接丢掉 }inline int kth(int x){//找第k小,改一下也可以找第k大 int u=root;return 0; while(1){int y = t[u].ch[0];//左儿子if(x&gt;t[y].size+t[u].cnt){//如果左儿子和当前点的size比要找的排名数小 x-=t[y].size+t[u].cnt;//数量减少,相当于把这个寻找排名的起点变成了当前节点 u=t[u].ch[1];//那么当前排名的数一定要往右儿子上找 }else if(t[y].size&gt;=x) u=y;//左儿子的size足够,儿子在左侧上找 else return t[u].val;//左儿子的size比x小,加上当前点u的size则比x大,说明第kA大的就是x }} int main(){tot=0;read(n);    insert(+2147483647);insert(-2147483647);//博客作者在这里先加了正负INF    int typ,x;    while(n--){        read(typ);        if(typ==1){read(x);insert(x);}//插入         else if(typ==2){read(x);Delete(x);}//删除         else if(typ==3){//查找 read(x);find(x);            printf("%d\n",t[t[root].ch[0]].size);        }        else if(typ==4){//第k小             read(x);printf("%d\n",kth(x+1));//之前插进去正负INF,所以要+1;         }        else if(typ==5){//前驱            read(x);printf("%d\n",t[Next(x,0)].val);        }        else if(typ==6){//后继             read(x);printf("%d\n",t[Next(x,1)].val);        }    }    return 0;}//基本是抄的这个大爹的博客https://www.cnblogs.com/cjyyb/p/7499020.html//再自己加了一点注解//全部代码https://paste.ubuntu.com/p/gH5mWpz669/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="多项式"><a href="#多项式" class="headerlink" title="多项式"></a>多项式</h1><h2 id="FFT"><a href="#FFT" class="headerlink" title="FFT"></a>FFT</h2><h3 id="递归FFT"><a href="#递归FFT" class="headerlink" title="递归FFT"></a>递归FFT</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">inline int read(){    char c=getchar();int x=0,f=1;    while(c&lt;'0'||c&gt;'9'){if(c=='-')f=-1;c=getchar();}    while(c&gt;='0'&amp;&amp;c&lt;='9'){x=x*10+c-'0';c=getchar();}    return x*f;}#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)const int MAXN = 3e6+5;const double Pi = acos(-1.0);struct cplx{//手写负数complex用起来会更快    double x,y;    cplx(double x=0,double y=0):x(x),y(y){}    friend cplx operator + (cplx a,cplx b){return cplx(a.x+b.x,a.y+b.y);}    friend cplx operator - (cplx a,cplx b){return cplx(a.x-b.x,a.y-b.y);}    friend cplx operator * (cplx a,cplx b){return cplx(a.x*b.x-a.y*b.y,a.x*b.y+a.y*b.x);}}a[MAXN],b[MAXN];void FFT(int limit,cplx *a,int type){//limit记录项数    if(limit==1) return;//只有一个常数项结束递归(即只剩下0次的)    cplx a1[limit&gt;&gt;1],a2[limit&gt;&gt;1];//按照奇偶分组    for(int i=0;i&lt;limit;i+=2)        a1[i&gt;&gt;1] = a[i],a2[i&gt;&gt;1] = a[i+1];    FFT(limit&gt;&gt;1,a1,type);    FFT(limit&gt;&gt;1,a2,type);    cplx Wn(cos(2*Pi/limit),type*sin(2*Pi/limit));//单位根    //这里type挺重要的,反变换的时候用-1,因为是-k    cplx w(1,0);//一会儿算单位根幂的时候用w存    cplx tmp;    for(int i=0;i&lt;(limit&gt;&gt;1);i++,w=w*Wn){//w相当于公式中的w_n^k        tmp = w*a2[i];//蝴蝶操作        a[i]=a1[i]+tmp;//偶        a[i+(limit&gt;&gt;1)]=a1[i]-tmp;//O(1)算另外一部分    }}int main(){    int n=read(),m=read();    rep(i,0,n) a[i].x=read();    rep(i,0,m) b[i].x=read();    int limit = 1;    while(limit&lt;=n+m) limit&lt;&lt;=1;//这里非常精髓    //把长度补到2的幂，这样就不用考虑%2余数的情况    //而且不必担心高次项的系数，因为默认为0    FFT(limit,a,1);    FFT(limit,b,1);    //1表示FFT，-1则是反变换       rep(i,0,limit) a[i] = a[i]*b[i];//转换为点值后直接相乘    FFT(limit,a,-1);//现在变回去    rep(i,0,n+m) printf("%d ",(int)(a[i].x/limit+0.5));//还要除以n的    printf("\n");}//P3803 【模板】多项式乘法（FFT）//https://www.luogu.com.cn/problem/P3803<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="迭代FFT"><a href="#迭代FFT" class="headerlink" title="迭代FFT"></a>迭代FFT</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">const double Pi = acos(-1.0);inline int read(){    char c=getchar();int x=0,f=1;    while(c&lt;'0'||c&gt;'9'){if(c=='-')f=-1;c=getchar();}    while(c&gt;='0'&amp;&amp;c&lt;='9'){x=x*10+c-'0';c=getchar();}    return x*f;}struct cplx{    double x,y;    cplx(double x=0,double y=0):x(x),y(y){}    friend cplx operator + (cplx a,cplx b){return cplx(a.x+b.x,a.y+b.y);}    friend cplx operator - (cplx a,cplx b){return cplx(a.x-b.x,a.y-b.y);}    friend cplx operator * (cplx a,cplx b){return cplx(a.x*b.x-a.y*b.y,a.x*b.y+a.y*b.x);}}a[MAXN],b[MAXN];int la,lb;int rev[MAXN],lim,len;//len存二进制位数//和递归FFT的limit不一样,这里的lim在FFT执行中是不变的inline void FFT(cplx a[],int type){    rep(i,0,lim-1) if(i&lt;rev[i]) swap(a[i],a[rev[i]]);    //按照位逆序后的数字大小排好序,因为是置换所以On就行    rep(dep,1,log2(lim)){//第一层枚举层数        int m = 1&lt;&lt;dep;//得到dep层每组的元素个数        cplx Wn(cos(2.0*Pi/m),type*sin(2.0*Pi/m));//单位根        for(int k=0;k&lt;lim;k+=m){//第二层k表示每个m元组开始的点            cplx w(1,0);            rep(j,0,(m&gt;&gt;1)-1){                cplx t = w*a[k+j+m/2];//蝴蝶操作,两两进行运算                cplx u = a[k+j];                //之前因为这句卡了好久,没这句的话a[k+j]的值改变了,a[k+j+m/2]就错了                a[k+j] = a[k+j]+t;                a[k+j+m/2] = a[k+j]-t;                w = w*Wn;            }        }    }}inline void init_rev(){//预处理rev位逆序置换数组    rep(i,0,lim-1) rev[i] = (rev[i&gt;&gt;1]&gt;&gt;1) | ((i&amp;1)&lt;&lt;(len-1));//|和+作用是一样的    //后面几位是上一个子问题的解向右移动一位,第一位即最底层按奇偶得出}int main(){    la = read(),lb = read();    rep(i,0,la) a[i].x = read();    rep(i,0,lb) b[i].x = read();    lim = 1,len = 0;    while(lim&lt;=la+lb) lim&lt;&lt;=1,len++;//一定要是2的幂    init_rev();//计算位逆序顺序    FFT(a,1);    FFT(b,1);    rep(i,0,lim-1) a[i] = a[i]*b[i];//点值表示法直接相乘    FFT(a,-1);//现在变回去    rep(i,0,la+lb) printf("%d ",(int)(a[i].x/lim+0.5));//还要除以n的    printf("\n");}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>//待改</strong></p><h1 id="图论"><a href="#图论" class="headerlink" title="图论"></a>图论</h1><h2 id="优先队列优化Dijkstra"><a href="#优先队列优化Dijkstra" class="headerlink" title="优先队列优化Dijkstra"></a>优先队列优化Dijkstra</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#define LINF 1ll&lt;&lt;60const int MAXN = 2e5+10;typedef pair&lt;int,ll&gt; pli;vector&lt;pli&gt; e[MAXN];//first是目标的点，second是距离dis bool vis[MAXN];ll d[MAXN];int n,m,s,t;void dijkstra(){rep(i,1,n){d[i] = LINF;vis[i] = 0;}d[s] = 0;priority_queue&lt; pli,vector&lt;pli&gt;,greater&lt;pli&gt; &gt;q;//first存d[x],second存x的编号 q.push(make_pair(0,s));while(!q.empty()){//进行类似bfs的操作 int now = q.top().second;q.pop();if(vis[now])continue;//可以看到下面的操作是都先推进去的，所以可能重复遇到now点 vis[now] = 1;int siz = e[now].size();for(auto x:e[now]){//遍历now的所有边 int v = x.second;//到达的点if(d[v]&gt;d[now]+x.first){d[v] = d[now] + x.first;q.push(make_pair(d[v],v));//推入优先队列 } }}}int main(){cin&gt;&gt;n&gt;&gt;m&gt;&gt;s;int u,v;ll dis;rep(i,1,m){cin&gt;&gt;u&gt;&gt;v&gt;&gt;dis;e[u].push_back(make_pair(dis,v));//e[v].push_back(make_pair(u,dis));//双向通行的情况 }dijkstra();rep(i,1,n){cout&lt;&lt;d[i];//输出到各个点的最短路径 if(i!=n) cout&lt;&lt;' ';}} <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Dinic"><a href="#Dinic" class="headerlink" title="Dinic"></a>Dinic</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">struct Edge{int from,to;ll cap,flow;};int n,m,s,t;//节点数，边数(含反向弧)，源点与汇点编号 vector&lt;Edge&gt; edges;//边长，edges[e]和edges[e^1]互为反向弧 vector&lt;int&gt; G[MAXN];//邻接表，G[i][j]表示节点i的第j条边在edges中的编号int d[MAXN];//dist层数，即d[i]为起点到i的距离（层数）,同时也起到vis的作用防止重复走int cur[MAXN];//这个cur是dfs的时候用来省略重复步骤的void add_edge(int from,int to,ll cap){edges.push_back((Edge){from,to,cap,0});edges.push_back((Edge){to,from,0,0});int siz = edges.size();G[from].push_back(siz-2);//正向边G[to].push_back(siz-1);//反向边}bool bfs(){    memset(d,-1,sizeof(d));queue&lt;int&gt; q;q.push(s);d[s]=0;//dist(s) = 0while(!q.empty()){int x=q.front();q.pop();rep(i,0,G[x].size()-1){Edge &amp;e = edges[G[x][i]];if(d[e.to]==-1&amp;&amp;e.cap&gt;e.flow){//只考虑残量网络中的弧即这个边还没被填满d[e.to]=d[x]+1;//标记层次dist(x)q.push(e.to);}}}return d[t]!=-1;//找到到t节点的路径则回1，找不到则回0} ll dfs(int x,ll a){//DFS除了当前节点x外，还要传入"目前为止所有边的最小残量"即"水流到这里还剩多少"if(x==t||a==0) return a;//也是很简洁一句话 ，结束增广  ll flow = 0,f;for(int &amp;i=cur[x];i&lt;G[x].size();i++){//上一次阻塞流已经把cur[x]之前的弧都排除了 Edge &amp;e = edges[G[x][i]];if(d[x]+1==d[e.to]&amp;&amp;(f=dfs(e.to,min(a,e.cap-e.flow)))&gt;0){e.flow+=f;edges[G[x][i]^1].flow-=f;flow+=f;a-=f;if(a==0) break;//这句及时退出很影响效率 } }return flow; }ll Maxflow(){ll flow=0;while(bfs()){memset(cur,0,sizeof(cur));//因为有新的反向边引入，即"正向边"更新了 ,有些实际上是反,但dfs里面当正的用 flow+=dfs(s,INF);//对当前阻塞流dfs； }return flow;} int main(){int u,v;ll cap;scanf("%d%d%d%d",&amp;n,&amp;m,&amp;s,&amp;t);for(int i=1;i&lt;=m;i++){scanf("%d%d%lld",&amp;u,&amp;v,&amp;cap);add_edge(u,v,cap);}cout&lt;&lt;Maxflow()&lt;&lt;endl;}//P3376 【模板】网络最大流 https://www.luogu.com.cn/problem/P3376<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="二分匹配匈牙利"><a href="#二分匹配匈牙利" class="headerlink" title="二分匹配匈牙利"></a>二分匹配匈牙利</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int uN,vN;//u,v的数目bool g[MAXN][MAXN];//邻接矩阵,一般情况下uv最大范围相同 int linker[MAXN];//存右节点的对象 bool used[MAXN]; //右点是否访问过bool dfs(int u){for(int v=0;v&lt;vN;v++){if(g[u][v]&amp;&amp;!used[v]){//判断是否有边，是否被用过used[v]=true;//标记访问if(linker[v]==-1||dfs(linker[v])){linker[v]=u;//很精妙，记录新配对，在进行dfs后，连接左右节点的边其实方向反了一反 return true; } }}return false; } int hungary(){//匈牙利算法 int res=0;memset(linker,-1,sizeof(linker));//初始化,因为节点uv从0开始，所以linker初始化为-1 for(int u=0;u&lt;uN;u++){memset(used,false,sizeof(used));if(dfs(u)) res++; } return res;}int main(){int e;cin&gt;&gt;uN&gt;&gt;vN&gt;&gt;e;int u,v;memset(g,0,sizeof(g));while(e--){scanf("%d%d",&amp;u,&amp;v);g[u][v]=1;}cout&lt;&lt;hungary()&lt;&lt;endl;} /*匈牙利算法可以解决的问题：1.二分图的最大匹配数(如婚配问题)2.最小顶点覆盖------用最少的点覆盖所有的边(如HDOJ禁止早恋,任务安排)结论：最小顶点覆盖数==最大匹配数量 3. DAG(有向无环图)的最小路径覆盖(如HDOJ空袭，所有路单行，并且所有街是两个路口相连，已知不会形成回路。问最少空降几个伞兵可以访问所有路口)拆点法：1拆成1和1',数字x放左节点，x'放右节点DAG图的最小路径覆盖数 == 节点数(n) - 最大匹配数 */ <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="kruskal最小生成树"><a href="#kruskal最小生成树" class="headerlink" title="kruskal最小生成树"></a>kruskal最小生成树</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">struct Edge{    int u,v,k;    friend bool operator &lt; (Edge a,Edge b){        return a.k&lt;b.k;    }}edges[MAXM];int fa[MAXN];int rankk[MAXN];//树高优化并查集int find(int x){    if(fa[x]==x) return x;    return fa[x] = find(fa[x]);}void merge(int a,int b){    a = find(a);b = find(b);    if(a==b) return;    if(rankk[a]&gt;rankk[b]) fa[b] = a;    else{        fa[a] = b;        if(rankk[a]==rankk[b])rankk[b]++;     }}int n,m;int main(){    cin&gt;&gt;n&gt;&gt;m;    rep(i,1,m) cin&gt;&gt;edges[i].u&gt;&gt;edges[i].v&gt;&gt;edges[i].k;    rep(i,1,n) fa[i] = i,rankk[i] = 0;    sort(edges+1,edges+1+m);    ll res=0;    rep(i,1,m){        if(find(edges[i].u)!=find(edges[i].v)){//检查两点是否在同一集合内            res+=edges[i].k;            merge(edges[i].u,edges[i].v);        }    }    bool flag = 1;    rep(i,2,n){        if(find(n)!=find(n-1)) {flag = 0;break;}    }    if(!flag) cout&lt;&lt;"orz"&lt;&lt;endl;    else cout&lt;&lt;res&lt;&lt;endl;}//P3366 【模板】最小生成树 https://www.luogu.com.cn/problem/P3366<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="tarjan求强连通分量"><a href="#tarjan求强连通分量" class="headerlink" title="tarjan求强连通分量"></a>tarjan求强连通分量</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int n;vector&lt;int&gt; e[MAXN];//存图 int dfn[MAXN],low[MAXN];int dfncnt;//dfn自增下标 int s[MAXN],tp;//数组模拟栈,tp记录大小 bool in[MAXN];//记录该点是否在栈中 int scc[MAXN],sc;//节点i所在scc的编号,sc记录有几个强连通 int sz[MAXN];//强连通i的大小 int indg[MAXN];//记录缩点后的入度（这题才有的 void tarjan(int u){low[u]=dfn[u]=++dfncnt;//low初始值为自身dfns[++tp]=u;//推u入栈，从1开始 in[u]=1;//记录u点在栈中 for(auto v:e[u]){//访问到新点的情况 if(!dfn[v]){tarjan(v);low[u] = min(low[u],low[v]);//用low[v}更新low[u] }else if(in[v])//v被访问过，但是在栈中 low[u] = min(low[u],dfn[v]);}if(dfn[u]==low[u]){//u是连通分量的根节点sc++;//强连通数量++sz[sc] = 0; while(s[tp]!=u){//u和u之后的点全部出栈 scc[s[tp]] = sc;//这个点包含于第几个强连通sz[sc]++;//u为根的这个强连通的大小 in[s[tp]] = 0;//出栈 tp--; }scc[u] = sc;//给根节点标，属于第sc个强连通 sz[sc]++;in[u] = 0;tp--;}} void reset(){tp = sc = dfncnt =0;rep(i,1,n){in[i] = dfn[i] = 0;//low不用清空，sz在之后用到再清空 e[i].clear();}}int main(){cin&gt;&gt;n;reset();int v;rep(u,1,n){while(cin&gt;&gt;v&amp;&amp;v!=0) e[u].push_back(v);} rep(u,1,n)if(!dfn[u]) tarjan(u);rep(i,1,sc) indg[i] = 0;//这个不包含在tarjan里面，是这题记录入度的 rep(u,1,n){for(auto v:e[u]){if(scc[u]!=scc[v]) indg[scc[v]]++;}}int res = 0;rep(i,1,sc){if(indg[i]==0) res++;}cout&lt;&lt;res&lt;&lt;endl;}//洛谷P2835 刻录光盘 https://www.luogu.com.cn/problem/P2835#submit<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="拓扑排序"><a href="#拓扑排序" class="headerlink" title="拓扑排序"></a>拓扑排序</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#define LINF 1ll&lt;&lt;60const int MAXN = 2e5+10;typedef pair&lt;int,ll&gt; pli;vector&lt;pli&gt; e[MAXN];//first是目标的点，second是距离dis bool vis[MAXN];ll d[MAXN];int n,m,s,t;void dijkstra(){rep(i,1,n){d[i] = LINF;vis[i] = 0;}d[s] = 0;priority_queue&lt; pli,vector&lt;pli&gt;,greater&lt;pli&gt; &gt;q;//first存d[x],second存x的编号 q.push(make_pair(0,s));while(!q.empty()){//进行类似bfs的操作 int now = q.top().second;q.pop();if(vis[now])continue;//可以看到下面的操作是都先推进去的，所以可能重复遇到now点 vis[now] = 1;int siz = e[now].size();for(auto x:e[now]){//遍历now的所有边 int v = x.second;//到达的点if(d[v]&gt;d[now]+x.first){d[v] = d[now] + x.first;q.push(make_pair(d[v],v));//推入优先队列 } }}}int main(){cin&gt;&gt;n&gt;&gt;m&gt;&gt;s;int u,v;ll dis;rep(i,1,m){cin&gt;&gt;u&gt;&gt;v&gt;&gt;dis;e[u].push_back(make_pair(dis,v));//e[v].push_back(make_pair(u,dis));//双向通行的情况 }dijkstra();rep(i,1,n){cout&lt;&lt;d[i];//输出到各个点的最短路径 if(i!=n) cout&lt;&lt;' ';}} //Uva10305 https://vjudge.net/problem/UVA-10305<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="HASH"><a href="#HASH" class="headerlink" title="HASH"></a>HASH</h1><h2 id="拉链法"><a href="#拉链法" class="headerlink" title="拉链法"></a>拉链法</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">const int med = 1000007;//模数 const int MAXN = 2e6+5;//数据总量 struct Hash_table{struct Node{int next,value,key;}data[MAXN];//数据的总量,从1开始 int head[med],size;//head记录每个哈希值的链表的第一个节点//size记录节点总数 int f(int key){ return key%med; }//求哈希值int find(int key){for(int p = head[f(key)];p;p=data[p].next)//遍历这个哈希值上的链表 if(data[p].key==key) return data[p].value;return -1; }int update(int key,int value){//更新value for(int p = head[f(key)];p;p=data[p].next)if(data[p].key==key) return data[p].value = value;return -1;}int add(int key,int value){if(find(key)!=-1) return -1;//这个值已经被插入了data[++size] = (Node){head[f(key)],value,key};//从链表头部插入 head[f(key)] = size;//标记该链表的第一个节点 return value; }};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="几何"><a href="#几何" class="headerlink" title="几何"></a>几何</h1><h2 id="三个点求圆心"><a href="#三个点求圆心" class="headerlink" title="三个点求圆心"></a>三个点求圆心</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">struct point{double x;double y;};point cal(point a,point b,point c){double x1 = a.x;double y1 = a.y;double x2 = b.x;double y2 = b.y;double x3 = c.x; double y3 = c.y;double a1 = 2*(x2-x1); double a2 = 2*(x3-x2);double b1 = 2*(y2-y1); double b2 = 2*(y3-y2);double c1 = x2*x2 + y2*y2 - x1*x1 - y1*y1;double c2 = x3*x3 + y3*y3 - x2*x2 - y2*y2;double rx = (c1*b2-c2*b1)/(a1*b2-a2*b1);double ry = (c2*a1-c1*a2)/(a1*b2-a2*b1);return point{rx,ry};}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h1><h2 id="组合数板子"><a href="#组合数板子" class="headerlink" title="组合数板子"></a>组合数板子</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">const int MAXN = 3e5+5;const int med = 998244353;ll jc[MAXN];ll qpow(ll d,ll c){//快速幂    ll res = 1;    while(c){        if(c&amp;1) res=res*d%med;        d=d*d%med;c&gt;&gt;=1;    }return res;}inline ll niyuan(ll x){return qpow(x,med-2);}void initjc(){//初始化阶乘    jc[0] = 1;    rep(i,1,MAXN-1) jc[i] = jc[i-1]*i%med;}inline int C(int n,int m){//n是下面的    if(n&lt;m) return 0;    return jc[n]*niyuan(jc[n-m])%med*niyuan(jc[m])%med;}int main(){    initjc();    int n,m;    while(cin&gt;&gt;n&gt;&gt;m) cout&lt;&lt;C(n,m)&lt;&lt;endl;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="高斯消元"><a href="#高斯消元" class="headerlink" title="高斯消元"></a>高斯消元</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">double a[MAXN][MAXN];int n,pl;int main(){scanf("%d",&amp;n);rep(i,1,n)rep(j,1,n+1) scanf("%lf",&amp;a[i][j]); rep(i,1,n){pl = i;while(a[pl][i]==0&amp;&amp;pl&lt;=n) pl++;//找到每列的第一个非0元素if(pl==n+1){//无解的情况(存在空列 cout&lt;&lt;"No Solution"&lt;&lt;endl;return 0;} rep(j,1,n+1) swap(a[i][j],a[pl][j]);//保证i行i列必不是0double k = a[i][i];//第二步，使a[i][i]变成1 rep(j,1,n+1) a[i][j]/=k;//i行所有元素除a[i][i]rep(ii,1,n){if(ii==i) continue;//枚举不同的两行double ki = a[ii][i];rep(m,1,n+1) a[ii][m]-=ki*a[i][m]; }  }rep(i,1,n) printf("%.2lf\n",a[i][n+1]);}//洛谷P3389 【模板】高斯消元法 https://www.luogu.com.cn/problem/P3389<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="矩阵快速幂"><a href="#矩阵快速幂" class="headerlink" title="矩阵快速幂"></a>矩阵快速幂</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int n;const ll mod = 998;struct Matrix{ll a[MAXN][MAXN];Matrix(ll x=0){//感觉是特别好的初始化,从hjt那里学(抄)来的 for(int i=0;i&lt;n;i++){for(int j=0;j&lt;n;j++){a[i][j]=x*(i==j);//这句特简洁}}}Matrix operator *(const Matrix &amp;b)const{//通过重载运算符实现矩阵乘法 Matrix res(0);for(int i=0;i&lt;n;i++){for(int j=0;j&lt;n;j++){for(int k=0;k&lt;n;k++){ll &amp;ma = res.a[i][j];ma = (ma+a[i][k]*b.a[k][j])%mod;}}}return res;}};Matrix qpow(Matrix d,ll m){//底数和幂次数 Matrix res(1);//构造E单位矩阵 while(m){if(m&amp;1){m--;//其实这句是可以不要的 res=res*d;}d=d*d;m&gt;&gt;=1;}return res; }int main(){int p;Matrix inp;cin&gt;&gt;n&gt;&gt;p;for(int i=-0;i&lt;n;i++){for(int j=0;j&lt;n;j++){scanf("%lld",&amp;inp.a[i][j]);}}Matrix res = qpow(inp,p);for(int i=0;i&lt;n;i++){for(int j=0;j&lt;n;j++){cout&lt;&lt;res.a[i][j];if(j!=n-1) cout&lt;&lt;' ';}cout&lt;&lt;endl;}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="数论"><a href="#数论" class="headerlink" title="数论"></a>数论</h1><h2 id="线性求逆元"><a href="#线性求逆元" class="headerlink" title="线性求逆元"></a>线性求逆元</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void init(int p){inv[1] = 1;for(int i=2;i&lt;=n;i++){inv[i] = (ll)(p-p/i)*inv[p%i]%p;}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="单个欧拉函数"><a href="#单个欧拉函数" class="headerlink" title="单个欧拉函数"></a>单个欧拉函数</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void init(int p){inv[1] = 1;for(int i=2;i&lt;=n;i++){inv[i] = (ll)(p-p/i)*inv[p%i]%p;}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="EXCRT扩展中国剩余"><a href="#EXCRT扩展中国剩余" class="headerlink" title="EXCRT扩展中国剩余"></a>EXCRT扩展中国剩余</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;class T&gt;void wt(T x){//快写   if(x &lt; 0) putchar('-'), x = -x;   if(x &gt;= 10) wt(x / 10);   putchar('0' + x % 10);}void Exgcd(ll a,ll b,ll &amp;d,ll &amp;x,ll &amp;y){//扩展欧几里得if(!b){d=a;x=1;y=0;}else{Exgcd(b,a%b,d,y,x);y-=x*(a/b);}//先交换了xy的位置，实现y1=x2-(a/b)*x2 }inline ll mul(ll a,ll b,ll mo){//取模乘法    return ((a%mo)*(b%mo)%mo+mo)%mo; }const int MAXN = 1e5+5;ll c[MAXN],m[MAXN];//每组同余式的余数和模数int n;ll Excrt(ll m[],ll c[],int n){    ll mnow = m[1],cnow = c[1];//记录每次合成后的模数余数    rep(i,2,n){        ll p1,p2,gcdd;        ll m1 = mnow,m2 = m[i];//m1p1+c1 = m2p2+c2        ll dc = (c[i]-cnow%m2+m2)%m2;//dc在保证同余的情况下变成最小的正数        Exgcd(m1,m2,gcdd,p1,p2);        if(dc%gcdd) {cout&lt;&lt;i&lt;&lt;endl;return -1;}        p1 = mul(p1,dc/gcdd,m2/gcdd);//p1存的实际是p1*m1,这里的模数比较讲究        //一会儿要对lcm(m1,m2)取模,最终结果是[p1*m1*(dc/gcdd)] % [m2/gcdd*m1]        //m1还没乘上去,这时候先对m2/gcdd取模        cnow += p1*m1;//更新cnow和mnow        mnow = m1/gcdd*m2;        cnow = (cnow%mnow+mnow)%mnow;    }    return cnow;  } int main(){    cin&gt;&gt;n;    rep(i,1,n) read(m[i]),read(c[i]);    wt(Excrt(m,c,n));}//洛谷P4777 【模板】扩展中国剩余定理（EXCRT）//https://www.luogu.com.cn/problem/P4777<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="EXGCD扩展欧几里得"><a href="#EXGCD扩展欧几里得" class="headerlink" title="EXGCD扩展欧几里得"></a>EXGCD扩展欧几里得</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">ll Exgcd(ll a,ll b,ll &amp;x,ll &amp;y){if(!b){x=1;y=0;return a;}//边界条件结束递归 ll d = Exgcd(b,a%b,x,y);//gcd ll t = x;x=y;y=t-(a/b)*y;//通过x2y2求得x1y1,层层返回 return d;}//紫书里面刘汝佳的简短的版本void Exgcd(ll a,ll b,ll &amp;d,ll &amp;x,ll &amp;y){//不同的是，这里的d使用引用来实现 if(!b){d=a;x=1;y=0;}else{Exgcd(b,a%b,d,y,x);y-=x*(a/b);}//先交换了xy的位置，实现y1=x2-(a/b)*x2 } <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h1><h2 id="AC自动机"><a href="#AC自动机" class="headerlink" title="AC自动机"></a>AC自动机</h2><h3 id="统计是否出现"><a href="#统计是否出现" class="headerlink" title="统计是否出现"></a>统计是否出现</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">const int MAXN = 1e6+5;inline int idx(char c){return c-'a';}struct Node{    int son[26],flag,fail;}trie[MAXN*10];int n,cntt;string s;queue&lt;int&gt; q;void insert(string &amp;s){//字典树插入模式串    int siz = s.size(),v,u = 1;//根节点开始    rep(i,0,siz-1){        v = idx(s[i]);        if(!trie[u].son[v]) trie[u].son[v] = ++cntt;        u = trie[u].son[v];    }    trie[u].flag++;//记数}void getfail(){//处理失配指针    rep(i,0,25) trie[0].son[i] = 1;//虚节点0    q.push(1);trie[1].fail = 0;    //建一个虚节点0号节点,将1的所有儿子指向1，然后1的fail指向0就OK了    int u,v,ufail;    while(!q.empty()){        u = q.front();q.pop();        rep(i,0,25){            v = trie[u].son[i];            ufail = trie[u].fail;            if(!v){trie[u].son[i]=trie[ufail].son[i];continue;}            //如果这个分支不满足，则会和失配的情况类似去跳转            trie[v].fail = trie[ufail].son[i];            //((他父亲节点)的失配指针指向的节点)的(和这个节点字母相同的儿子)            q.push(v);        }    }}int query(string &amp;s){//匹配    int siz = s.size(),u = 1,v,k,ans = 0;    rep(i,0,siz-1){        v = idx(s[i]);        k = trie[u].son[v];//k用来跳fail        while(k&amp;&amp;trie[k].flag!=-1){//找到了没标记的单词            ans += trie[k].flag;trie[k].flag = -1;//计数,并标记走过            k = trie[k].fail;//跳fail,如果一个串匹配成功,那它的fail一定也能匹配        }        u = trie[u].son[v];    }    return ans;}int main(){    cntt = 1;//初始化cnt    cin&gt;&gt;n;    string hc;    rep(i,1,n){        cin&gt;&gt;s;insert(s);    }    getfail();    cin&gt;&gt;s;    cout&lt;&lt;query(s)&lt;&lt;endl;}//P3808 【模板】AC自动机（简单版）https://www.luogu.com.cn/problem/P3808//https://www.luogu.com.cn/blog/juruohyfhaha/solution-p3808<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="统计出现次数-比较慢一点"><a href="#统计出现次数-比较慢一点" class="headerlink" title="统计出现次数(比较慢一点)"></a>统计出现次数(比较慢一点)</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">const int MAXN = 1e5+5;int jdbh[MAXN];//记录第i个模式串对应的节点编号int cntcx[MAXN];//记录第i个模式串出现的次数inline int idx(char c){return c-'a';}struct Node{    int son[26],flag,fail;//cnt记录次数,flag记录编号    void clr(){        memset(son,0,sizeof(son));        flag=0;    }}trie[MAXN*10];int n,cntt;//cntt记录总点数string s,ms[166];int maxx;queue&lt;int&gt;q;inline void insert(string &amp;s,int num){    int siz = s.size(),v,u=1;    rep(i,0,siz-1){        v = idx(s[i]);        if(!trie[u].son[v]){trie[u].son[v] = ++cntt;trie[cntt].clr();}        u = trie[u].son[v];    }    trie[u].flag = num;//标记为单词,flag记录编号    //保证每个模式串只出现一次    cntcx[num] = 0;    jdbh[num] = u;//记录当前单词对应的节点编号}inline void getfail(){    rep(i,0,25) trie[0].son[i] = 1;    trie[0].flag = 0;    q.push(1);    trie[1].fail = 0;    int u,v,ufail;    while(!q.empty()){        u = q.front();q.pop();        rep(i,0,25){            v = trie[u].son[i];            ufail = trie[u].fail;            if(!v){trie[u].son[i]=trie[ufail].son[i];continue;}//画好一条跳fail的路            trie[v].fail = trie[ufail].son[i];            q.push(v);        }    }}inline void query(string &amp;s){    int siz = s.size(),u = 1,v,k;    rep(i,0,siz-1){        v = idx(s[i]);        k = trie[u].son[v];        while(k){            if(trie[k].flag){                cntcx[trie[k].flag]++;//计数                maxx = max(maxx,cntcx[trie[k].flag]);            }            k = trie[k].fail;//跳fail        }        u = trie[u].son[v];//这一句其实也有跳fail的功能，很精妙    }}inline void solve(){    cntt = 1;    trie[0].clr();    trie[1].clr();    rep(i,1,n){        cin&gt;&gt;ms[i];        insert(ms[i],i);    }    getfail();    cin&gt;&gt;s;    maxx = 0;    query(s);    cout&lt;&lt;maxx&lt;&lt;endl;    rep(i,1,n){        if(cntcx[i]==maxx) cout&lt;&lt;ms[i]&lt;&lt;endl;    }}int main(){    while(cin&gt;&gt;n&amp;&amp;n!=0) solve();}//洛谷 P3796 【模板】AC自动机（加强版）//https://www.luogu.com.cn/problem/P3796<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="KMP字符串匹配"><a href="#KMP字符串匹配" class="headerlink" title="KMP字符串匹配"></a>KMP字符串匹配</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">const int MAXN = 2e6+5;int pi[MAXN];//MAXN记得开大一点,因为这里要存到m+n+1长度的 vector&lt;int&gt; res;//储存答案 void getpi(const string &amp;s){ //求s的前缀函数pi[0]=0;int j=0;rep(i,1,s.length()-1){while(j&gt;0&amp;&amp;s[i]!=s[j]) j=pi[j-1];//找到合适且最长的j if(s[i]==s[j])j++;//能成功匹配的情况 pi[i]=j;}}void kmp(string s,string t){ //在主串t中找模式串s getpi(s+'#'+t);int n=(int)s.length(),m=(int)t.length();rep(i,n+1,m+n+1-1)if(pi[i]==n) res.push_back(i-2*s.size()); //i-2n计算得左端点 }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="字符串哈希"><a href="#字符串哈希" class="headerlink" title="字符串哈希"></a>字符串哈希</h2><h3 id="双哈希判断是否相等"><a href="#双哈希判断是否相等" class="headerlink" title="双哈希判断是否相等"></a>双哈希判断是否相等</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">typedef pair&lt;ull,ull&gt; pll;const int MAXN = 1e4+5;const int M1 = 1e9+7;//第一个模数 const int M2 = 1e9+9;//第二个模数 const int b = 131;int n;pll a[MAXN];pll gethash(string s){ull res1=0,res2=0;int siz = s.length();rep(i,0,siz-1){res1=(res1*b%M1+s[i])%M1;//i位乘以b^ires2=(res2*b%M2+s[i])%M2;//f(s)=Σ s[i] * b^i;}return make_pair(res1,res2);}int main(){cin&gt;&gt;n;string s;rep(i,1,n){cin&gt;&gt;s;a[i]=gethash(s);}sort(a+1,a+n+1);int res = 1;rep(i,2,n){if(a[i]!=a[i-1]) res++;}cout&lt;&lt;res&lt;&lt;endl;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="配合快速幂取子串的哈希值"><a href="#配合快速幂取子串的哈希值" class="headerlink" title="配合快速幂取子串的哈希值"></a>配合快速幂取子串的哈希值</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">const int b = 131;const int MAXN = 1e5 + 5;typedef unsigned long long ull;ull h[MAXN], pw[MAXN]; // h[k]存储字符串前k个字母的哈希值, pw[k]存储 b^k mod 2^64//这里的模数M取的就是ull的上限2^64char str[MAXN];void init(int n){//初始化     pw[0] = 1;    for (int i = 1; i &lt;= n; i ++ ) {        h[i] = h[i-1]*b + str[i];//做每个前缀的哈希值         pw[i] = pw[i-1]*b;//预处理b^k的值     }}// 计算子串 str[l ~ r] 的哈希值ull get(int l, int r) {    return h[r] - h[l-1]*pw[r-l+1];}int main() {    int n, m;    scanf("%d%d%s",&amp;n,&amp;m,str+1);//这样读入字符串第一位从1开始     init(n);    while (m--) {        int l1,r1,l2,r2;        scanf("%d%d%d%d",&amp;l1,&amp;r1,&amp;l2,&amp;r2);        if(get(l1,r1)!=get(l2,r2))        printf("No\n");        else printf("Yes\n");    }    return 0;}//ACWing 841 给两个区间查询这两个子串是否相同<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Trie字典树"><a href="#Trie字典树" class="headerlink" title="Trie字典树"></a>Trie字典树</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#define MAXN 10000const int maxnode=2e6+5;const int sigma_size=26;//字母开26，如果是数字之类的则开10 char s[MAXN];struct Trie{int ch[maxnode][sigma_size];int val[maxnode];int sz;Trie(){sz=1;memset(ch[0],0,sizeof(ch[0]));}//初始化，此时只有根节点int idx(char c){return c-'a';//同理如果是数字则换成c-'0' }void insert(char *s,int v){int u=0,n=strlen(s);for(int i=0;i&lt;n;i++){int c=idx(s[i]);if(!ch[u][c]){//节点不存在(u没有子节点字母为c)这个u和c的意义全然不同,u是节点编号,c是字符编号 memset(ch[sz],0,sizeof(ch[sz]));val[sz]=0;//中间节点，附加信息为0，当然也可以用其他的来标记比如-1 ch[u][c]=sz++;//画好边，并且总结点数++ }u=ch[u][c];//光标往下走 }//这里出来就是u已经到单词节点(即一个单词的末尾字符)上了 val[u] = v;//insert时的第二个参数v,可以附加信息 } bool find(char *s,int len){//查找串s长度不超过len的前缀,改一改成int也可以返回附加值 int u=0;for(int i=0;i&lt;len;i++){if(s[i]=='\0') break;int c=idx(s[i]);if(!ch[u][c]) break;u=ch[u][c];if(val[u]!=0) return true;} return false; } }T;//之前因为T定义在main里面所以一直运行不了，发现这个结构体和数组有点类似//定义为全局函数可以开得更大！ int main(){int n;scanf("%d",&amp;n);getchar();Trie T;while(n--){scanf("%s",s);getchar();T.insert(s,1);}scanf("%d",&amp;n);getchar();while(n--){scanf("%s",s);getchar();if(T.find(s,strlen(s))) cout&lt;&lt;"yes!"&lt;&lt;endl;else cout&lt;&lt;"no!"&lt;&lt;endl;}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="动态规划DP"><a href="#动态规划DP" class="headerlink" title="动态规划DP"></a>动态规划DP</h1><h2 id="数位DP"><a href="#数位DP" class="headerlink" title="数位DP"></a>数位DP</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int l,r;const int MAXN = 25;int dp[MAXN][2];//第二维记录前面一个是否为2int a[MAXN];int n,m;int dfs(int pos,bool stat,int pre,bool limit){    //limit记录之前是否都在上界，state记录之前的是不是6(记忆化作第二维)，pre记录上一个数字if(pos==-1) return 1;if(!limit&amp;&amp;dp[pos][stat]!=-1) return dp[pos][stat];    int up = limit?a[pos]:9;//如果前面的都在上界则只能到a[pos]    int ans = 0;    for(int i=0;i&lt;=up;i++){        if(pre==6&amp;&amp;i==2) continue;//62的情况        if(i==4) continue;ans += dfs(pos-1,i==6,i,limit&amp;&amp;i==a[pos]);    }if(!limit) dp[pos][stat] = ans;    return ans;}int solve(int x){//先按位转化到数组    int px = 0;    while(x){a[px++] = x%10;        x/=10;    }    memset(dp,-1,sizeof(dp));//清空记忆化数组    return dfs(px-1,0,-1,1);}int main(){while(cin&gt;&gt;l&gt;&gt;r){        if(l==0&amp;&amp;r==0) break;    //cout&lt;&lt;solve(r)&lt;&lt;' '&lt;&lt;solve(l-1)&lt;&lt;endl;        cout&lt;&lt;solve(r)-solve(l-1)&lt;&lt;endl;    }}//HDOJ2089 不要62<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="母函数"><a href="#母函数" class="headerlink" title="母函数"></a>母函数</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;using namespace std;#define rep(i,a,b) for(int (i)=a;i&lt;=b;i++)const int MAXN = 1e4;int c1[MAXN+1];int c2[MAXN+1];int main(){int n;while(cin&gt;&gt;n){ for(int i=0;i&lt;=n;i++){//初始化 c1[i]=0;c2[i]=0;}for(int i=0;i&lt;=n;i++){c1[i]=1;//面值为一元的}for(int i=2;i&lt;=n;i++){//枚举邮票的面值(一共有几组括号 for(int j=0;j&lt;=n;j++){//枚举左边次数为0到次数为n的项 for(int k=0;j+k&lt;=n;k+=i){//右边的乘过来，枚举放几枚//次数大于n的就不用管了 c2[j+k]+=c1[j];}}for(int i=0;i&lt;=n;i++){//最左边两个括号完成处理 c1[i]=c2[i];//把c2算出来的值挪到左边作为下一次的左边 c2[i]=0;//清空c2记录下一次括号相乘的结果 }}cout&lt;&lt;c1[n]&lt;&lt;endl; } } <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="并查集"><a href="#并查集" class="headerlink" title="并查集"></a>并查集</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int par[MAXN];//记录父亲节点 int rank[MAXN];//记录树高,根节点(祖先节点)那层是不算进去的  void init(int n){for(int i=0;i&lt;n;i++){par[i]=i;rank[i]=0;}} int find(int x){//查询根操作，同时进行路径压缩 if(par[x]==x) return x;else return par[x]=find(par[x]);//这句很精简,在递归查询根节点的同时路i压缩 } //虽然路径压缩的操作会对树高产生影响，导致rank的数值不准确，但这样还是能有效地提高运行效率的 void merge(int x,int y){x=find(x);y=find(y);if(x==y) return;///如果已经具有相同的祖先，则不进行合并操作if(rank[x]&lt;rank[y]) par[x]=y;//y树比x树高的情况，把x并为y的儿子节点 else{//y比x高，或同高的情况下，把y并为x的儿子节点 par[y]=x;if(rank[x]==rank[y]) rank[x]++;//如果合并的两树同高，则合并后树高+1; } } int main(){int n,m;cin&gt;&gt;n&gt;&gt;m;init(n);int a,b;while(m--){cin&gt;&gt;a&gt;&gt;b;merge(a,b);}cin&gt;&gt;m;while(m--){cin&gt;&gt;a&gt;&gt;b;if(find(a)==find(b)) cout&lt;&lt;"YES"&lt;&lt;endl;else cout&lt;&lt;"NO"&lt;&lt;endl;}} //路径压缩+树高优化<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="莫队"><a href="#莫队" class="headerlink" title="莫队"></a>莫队</h2><h3 id="普通莫队"><a href="#普通莫队" class="headerlink" title="普通莫队"></a>普通莫队</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">const int MAXN = 5e4+5;int cnt[MAXN];//记录数字在区间[l,r]内出现的次数int pos[MAXN],a[MAXN];ll ans[MAXN];int n,m,k,res;struct Q{    int l,r,k;//k记录原来的编号    friend bool operator &lt; (Q x,Q y){//同一个分块内r小的排前面;不同分块则按分块靠前的        return pos[x.l]==pos[y.l]?x.r&lt;y.r:pos[x.l]&lt;pos[y.l];        //return (pos[a.l]^pos[b.l])?pos[a.l]&lt;pos[b.l]:((pos[a.l]&amp;1)?a.r&lt;b.r:a.r&gt;b.r);        //这条第一个和==是一样的,后面的是对于左端点在同一奇数块的区间，右端点按升序排列，反之降序    }}q[MAXN];void Add(int pos){    res -= cnt[a[pos]]*cnt[a[pos]];    cnt[a[pos]]++;    res += cnt[a[pos]]*cnt[a[pos]];}void Sub(int pos){    res -= cnt[a[pos]]*cnt[a[pos]];    cnt[a[pos]]--;    res += cnt[a[pos]]*cnt[a[pos]];}int main(){    cin&gt;&gt;n&gt;&gt;m&gt;&gt;k;//k为数字范围    memset(cnt,0,sizeof(cnt));    int siz = sqrt(n);//每个分块的大小    rep(i,1,n){        cin&gt;&gt;a[i];        pos[i] = i/siz;//分块    }    rep(i,1,m){        cin&gt;&gt;q[i].l&gt;&gt;q[i].r;        q[i].k = i;//记录原来的编号,用于打乱顺序后的还原    }    sort(q+1,q+1+m);    res = 0;//初始化res    int l = 1,r = 0;//当前知道的区间    //因为是闭区间,如果是[1,1]的话则一开始就包含一个元素了    rep(i,1,m){//莫队的核心,注意加减的顺序        while(q[i].l&lt;l) Add(--l);        while(q[i].l&gt;l) Sub(l++);        while(q[i].r&lt;r) Sub(r--);        while(q[i].r&gt;r) Add(++r);        ans[q[i].k] = res;    }    rep(i,1,m) cout&lt;&lt;ans[i]&lt;&lt;endl;}//洛谷P2709 小B的询问//https://www.luogu.com.cn/problem/P2709<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="带修莫队"><a href="#带修莫队" class="headerlink" title="带修莫队"></a>带修莫队</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int cnt[1000010] = {0};const int MAXN = 2e5+5;int a[MAXN],b[MAXN];//a读入一开始的序列,b记录修改后的int pos[MAXN];//分块int cq,cr;//统计查询修改次数int R[MAXN][3];//0记位置,1记原本的值,2记修改后的值ll res;int ans[MAXN];//记录结果int n,m;void Add(int x){if(cnt[x]==0)res++;cnt[x]++;}//带修莫队的add和sub有区别void Sub(int x){if(cnt[x]==1)res--;cnt[x]--;}struct Q{    int l,r,k,t;    friend bool operator &lt; (Q a,Q b){        return (pos[a.l]^pos[b.l])?pos[a.l]&lt;pos[b.l]:((pos[a.r]^pos[b.r])?a.r&lt;b.r:a.t&lt;b.t);        //增加第三关键字,询问的先后顺序,用t或者k应该都行    }}q[MAXN];int main(){    cin&gt;&gt;n&gt;&gt;m;    cq = cr = 0;    int siz = pow(n,2.0/3.0);//这么分块最好,别问    rep(i,1,n){        cin&gt;&gt;a[i];        b[i]=a[i];        pos[i] = i/siz;    }    char hc;    rep(i,1,m){//读入修改和询问        cin&gt;&gt;hc;        if(hc=='Q'){            cin&gt;&gt;q[cq].l&gt;&gt;q[cq].r;            q[cq].k=cq;q[cq].t=cr;//注意这时候R[cr]还是没有的,这次询问是在R[cr-1]之后的            cq++;        }        else{            cin&gt;&gt;R[cr][0]&gt;&gt;R[cr][2];            R[cr][1] = b[R[cr][0]];            b[R[cr][0]] = R[cr][2];//在b数组中记录更改            cr++;        }    }    sort(q,q+cq);    int l=1,r=0,sjc=0;//时间戳    res = 0;    rep(i,0,cq-1){        while(sjc&lt;q[i].t){            if(l&lt;=R[sjc][0]&amp;&amp;R[sjc][0]&lt;=r)//判断修改是否在该区间内                Sub(R[sjc][1]),Add(R[sjc][2]);            a[R[sjc][0]] = R[sjc][2];//在a上也进行更改            sjc++;        }        while(sjc&gt;q[i].t){            sjc--;            if(l&lt;=R[sjc][0]&amp;&amp;R[sjc][0]&lt;=r)//判断修改是否在该区间内                Sub(R[sjc][2]),Add(R[sjc][1]);            a[R[sjc][0]] = R[sjc][1];//在a上也进行更改        }        while(l&gt;q[i].l) Add(a[--l]);        while(l&lt;q[i].l) Sub(a[l++]);        while(r&lt;q[i].r) Add(a[++r]);        while(r&gt;q[i].r) Sub(a[r--]);        ans[q[i].k] = res;    }    rep(i,0,cq-1) cout&lt;&lt;ans[i]&lt;&lt;endl;}//洛谷P1903 [国家集训队]数颜色 / 维护队列//https://www.luogu.com.cn/problem/P1903<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="扫描线Scanline"><a href="#扫描线Scanline" class="headerlink" title="扫描线Scanline"></a>扫描线Scanline</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define ll long long #define ls (x&lt;&lt;1)#define rs (x&lt;&lt;1|1)//这种方法感觉还挺好的const int MAXN = 2e5+5;//这里要开n的两倍//线结构体struct Line{    ll l,r,h;    int qz;//记录位置和权值    bool operator &lt; (Line &amp;rhs){        return h &lt; rhs.h;    }}line[MAXN];int n;ll x1,y1,x2,y2;ll X[MAXN];//线段树struct Segt{    int l,r;//是X的下标,即离散化后的    int sum;//sum是被完全覆盖的次数    ll len;//len是区间内被盖住的长度    //因为每次查询都是查询根节点,所以这边不需要懒惰标记}t[MAXN&lt;&lt;3];//一个边有两个点,所以这里要开8倍void build(int x,int l,int r){    t[x].l = l;t[x].r = r;    t[x].len = t[x].sum = 0;    if(l==r) return;//到了叶子节点    int mid = (l+r)&gt;&gt;1;    build(ls,l,mid);    build(rs,mid+1,r);}void push_up(int x){    int l = t[x].l,r = t[x].r;    if(t[x].sum) t[x].len = X[r+1]-X[l];//x的区间是X[l]到X[r+1]-1    else t[x].len = t[ls].len + t[rs].len;//合并儿子的信息}void update(int x,int L,int R,int v){//这里的LR存的是实际值    //这里如果是线段L,R,线段树上是L到R-1的部分维护    int l = t[x].l,r = t[x].r;    if(X[r+1]&lt;=L||R&lt;=X[l]) return;//加等于,不然会搞到无辜的线    if(L&lt;=X[l]&amp;&amp;X[r+1]&lt;=R){        t[x].sum += v;//修改覆盖次数        push_up(x);        return;    }    update(ls,L,R,v);    update(rs,L,R,v);    push_up(x);}int main(){    cin&gt;&gt;n;    rep(i,1,n){        cin&gt;&gt;x1&gt;&gt;y1&gt;&gt;x2&gt;&gt;y2;        X[2*i-1] = x1,X[2*i] = x2;//一会儿离散化要用的,这里存实际值        line[2*i-1] = Line{x1,x2,y1,1};//开始的线        line[2*i] = Line{x1,x2,y2,-1};//结束的线    }    n&lt;&lt;=1;//line的数量是四边形数量的2倍    sort(line+1,line+1+n);    sort(X+1,X+1+n);    int tot = unique(X+1,X+n+1)-(X+1);//去除重复相邻元素,并且tot记录总数    build(1,1,tot-1);//为什么是tot-1?    //因为线段树只需要维护X[1]到X[tot]-1这一段的,实际长度是向右贴的    ll res = 0;    rep(i,1,n-1){//每次高度是line[i+1].h-line[i].h,所以是到n-1就行        update(1,line[i].l,line[i].r,line[i].qz);//扫描线加入线段树        res += t[1].len*(line[i+1].h-line[i].h);    }    cout&lt;&lt;res&lt;&lt;endl;}//洛谷P5490 【模板】扫描线//https://www.luogu.com.cn/problem/P5490<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="CDQ分治"><a href="#CDQ分治" class="headerlink" title="CDQ分治"></a>CDQ分治</h2><p>处理三位偏序问题，以luogu陌上花开为例：<a href="https://www.luogu.com.cn/problem/P3810">https://www.luogu.com.cn/problem/P3810</a></p><p>序列上每个node有a,b,c三个属性，定义函数f(i)为对位置i的序列，存在多少个$j\leq i$使得$a_j\leq a_i,b_j\leq b_i,c_j\leq c_i$。统计完成后问对每个$0&lt;d&lt;n$，有多少个$i$使得$f(i)=d$.</p><p>方法是先对第一维度a进行排序,之后第二维满足$b_j\leq b_i$的每一对i,j，我们分成三种情况:ij都在左半边，ij都在右半边，i在左半边j在右半边。</p><p>前两个通过递归求解，求解完左右子区间后，根据第二关键字对两个子区间分别进行排序，这样一来左半边的a一定小于右半边的a，而且两个区间内的b都呈递增。我们用双指针的方法，右边的pxi每往右移动一次，左边的pxj对应移动，并且把这些j都推入树状数组中，之后根据pxi指向的节点的c属性，查询线段树中c比c[pxi]小的数量即可。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)const int MAXN = 214514;struct node{    int a,b,c,cnt,ans;}s1[MAXN],s2[MAXN];int ans[MAXN];//存对每个d,f[i]==d的i数量int n,k,m;//树状数组int mx;int treec[MAXN];inline int lowbit(int x){return x&amp;-x;}inline void add(int x,int t){    while(x&lt;=mx) treec[x]+=t,x+=lowbit(x);}inline int query(int x){    int ret = 0;    while(x) ret+=treec[x],x-=lowbit(x);    return ret;}//cdq分治bool cmp1(node x,node y){//以a为关键字从小打到排序    if(x.a==y.a){        if(x.b==y.b) return x.c&lt;y.c;        return x.b&lt;y.b;    }    return x.a&lt;y.a;}bool cmp2(node x,node y){//根据b排序,便只剩下c的顺序没有满足    if(x.b==y.b) return x.c&lt;y.c;    return x.b&lt;y.b;}void cdq(int l,int r){    if(l==r) return;    int mid = (l+r)&gt;&gt;1;    //处理前两种情况:ij同在左/右    cdq(l,mid);cdq(mid+1,r);    //处理第三种情况:i在左j在右    sort(s2+l,s2+mid+1,cmp2);    sort(s2+mid+1,s2+r+1,cmp2);    int pxj=l,pxi=mid+1;//双指针,对应pxi的移动把pxj插入树状数组中    for(pxi=mid+1;pxi&lt;=r;pxi++){        while(s2[pxj].b&lt;=s2[pxi].b&amp;&amp;pxj&lt;=mid){            add(s2[pxj].c,s2[pxj].cnt);//带次数插入树状数组中            pxj++;        }        s2[pxi].ans+=query(s2[pxi].c);        //查询树状数组中c比他小的    }    rep(j,l,pxj-1) add(s2[j].c,-s2[j].cnt);//之前错在这里,注意只有pxj前面的才被加过,pxj没有    //遍历pxj走过的区间,清空树状数组}int main(){    cin&gt;&gt;n&gt;&gt;k;    m=0,mx=k;//树状数组的最大数字(类似权值线段树)    rep(i,1,n) cin&gt;&gt;s1[i].a&gt;&gt;s1[i].b&gt;&gt;s1[i].c;    sort(s1+1,s1+1+n,cmp1);//以a为关键字从小到大排序    int tmp=0;//当前这个项出现了几次    rep(i,1,n){//合并相同的项并且记录出现次数进cnt        tmp++;//当前项出现了几次        if(s1[i].a!=s1[i+1].a||s1[i].b!=s1[i+1].b||s1[i].c!=s1[i+1].c){//判断两个node是否不同            s2[++m]=s1[i];//m统计元素个数            s2[m].cnt = tmp;            tmp = 0;        }    }    cdq(1,m);//递归计算    //题目问的是f(x)=满足条件ij对数,对应每个x有几个,把结果存到ans[x]中    rep(i,1,m) ans[s2[i].ans+s2[i].cnt-1]+=s2[i].cnt;    //s2[i].cnt-1是这些数字自己之间满足的对数    rep(i,0,n-1) cout&lt;&lt;ans[i]&lt;&lt;endl;}//洛谷P3810陌上花开//https://www.luogu.com.cn/problem/P3810<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACM模板 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速傅里叶变换-FFT学习笔记</title>
      <link href="2020/11/07/fft/"/>
      <url>2020/11/07/fft/</url>
      
        <content type="html"><![CDATA[<h1 id="理论部分"><a href="#理论部分" class="headerlink" title="理论部分"></a>理论部分</h1><h2 id="是这篇博客的学习笔记（自己看的整理总结，侵删）"><a href="#是这篇博客的学习笔记（自己看的整理总结，侵删）" class="headerlink" title="是这篇博客的学习笔记（自己看的整理总结，侵删）"></a>是这篇博客的学习笔记（自己看的整理总结，侵删）</h2><p><a href="https://www.cnblogs.com/zwfymqz/p/8244902.html">https://www.cnblogs.com/zwfymqz/p/8244902.html</a></p><p><a href="https://www.luogu.com.cn/blog/attack/solution-p3803">https://www.luogu.com.cn/blog/attack/solution-p3803</a></p><p>神犇博客</p><p>看完了才发现B站上有个特别好的视频BV1Y7411W73U</p><p><u>非常好非常好非常好非常牛逼非常牛逼非常牛逼这个视频</u></p><h2 id="多项式知识"><a href="#多项式知识" class="headerlink" title="多项式知识"></a>多项式知识</h2><h3 id="负数运算法则"><a href="#负数运算法则" class="headerlink" title="负数运算法则"></a>负数运算法则</h3><p>$(a+bi)*(c+di)=ac+adi+bci-bd=(ac-bd)+(bc+ad)i$</p><h3 id="单位根"><a href="#单位根" class="headerlink" title="单位根"></a>单位根</h3><p>复平面上把这个圆分成n等分的终点，角度最小的哪一个就是n次单位根，记作$w_{n}$</p><p>单位根$w_{n}^{k}$的数值即$\cos(k\cdot\frac{2\pi}{n})+\sin(k\cdot\frac{2\pi}{n})i$</p><p>有性质$w^{k}<em>{n}=w^{2k}</em>{2n}$，易证。</p><h2 id="FFT"><a href="#FFT" class="headerlink" title="FFT"></a>FFT</h2><h3 id="拆两半"><a href="#拆两半" class="headerlink" title="拆两半"></a>拆两半</h3><p>给多项式<br>$A(x)=a_{0}+a_{1}x+a_{2}x^{2}+\dots+a_{n-2}x^{n-2}+a_{n-1}x^{n-1}$</p><p>按照<strong>奇次偶次项分</strong>，得到</p><p>$A(x)=(a_{0}+a_{2}x^{2}\dots a_{n-2}x^{n-2}) + (a_{1}x\dots a_{n-1}x^{n-1})$</p><p>则有$A(x) = A_1(x^{2}) + x A_2(x^{2})$</p><p>把$w_n ^k (k&lt; \frac{n}{2})$带入有</p><p>$A( w_{n} ^{k} ) = A_{1}( w_{n} ^{2k}) +  w_{n} ^{k} A_2( w_{n} ^{2k})$</p><p>$= A_1(w_{ \frac{n}{2}}^{k}) + w_n ^k A_2(w_{ \frac{n}{2}}^{k})$<strong>式子1</strong></p><p>把$w_{n} ^{k + \frac{n}{2}}$带入得到</p><p>$A( w_n ^{k + \frac{n}{2} } ) = A_1( (w_n ^{k + \frac{n}{2} })^2 ) +  w_n ^{k + \frac{n}{2} } A_2( (w_n ^{k + \frac{n}{2} })^2 )$</p><p>$= A_1(w_{n}^{2k+n}) - w_n ^k A_2(w_{n}^{2k+n})$//+号因为正好去掉一个$\pi$所以变成-了,里面把平方去了</p><p>$= A_1(w_{n}^{2k}) - w_n ^k A_2(w_{n}^{2k})$//n可以去掉</p><p>$= A_1(w_{ \frac{n}{2}}^{k}) - w_n ^k A_2(w_{ \frac{n}{2}}^{k})$<strong>式子2</strong></p><h3 id="处理一个的同时得到另外一个"><a href="#处理一个的同时得到另外一个" class="headerlink" title="处理一个的同时得到另外一个"></a>处理一个的同时得到另外一个</h3><p>这时候可以找到规律，两个式子只有一个常数项不同(也只是正负号的区别)</p><p>所以算完式子1，只需要$O(1)$ 就有式子2的结果了</p><p>第一个式子$k$在取遍$ [0,\frac{n}{2}−1]$ 时,第二个式子也<strong>跟着取了</strong>$k+ \frac{n}{2}$即$[\frac{n}{2},n-1]$</p><p>问题的复杂度直接缩小一半。</p><p>这之后直接递归处理，当多项式只剩下一个项的时候返回就行了</p><h3 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h3><p>$O(nlogn)$</p><h2 id="快速傅里叶逆变换"><a href="#快速傅里叶逆变换" class="headerlink" title="快速傅里叶逆变换"></a>快速傅里叶逆变换</h2><p> 上面已经变成点值表示法了，之后还要学怎么从点值变回系数表示法</p><h3 id="把点值作为系数得到B-x"><a href="#把点值作为系数得到B-x" class="headerlink" title="把点值作为系数得到B(x)"></a>把点值作为系数得到B(x)</h3><p>$(y_0,y_1,y_2,\dots,y_{n−1})$为$(a_0,a_1,a_2,\dots,a_{n-1})$的傅里叶变换</p><p>设$c_k=\sum_{i=0}^{n-1}y_i(\omega_n^{-k})^i$即$c_k$是k单位根的</p><p>$B(x)=y_0,y_1x,y_2x^2,\dots,y_{n-1}x^{n-1}$</p><p>$k = 1,2,\dots,n-1$</p><p>在$x= \omega_n^{0},\omega_n^{-1},\omega_n^{-2},\dots,\omega_{n-1}^{-(n-1)}$这$B(x)$这n个点上的点值</p><h3 id="Ck变一下形"><a href="#Ck变一下形" class="headerlink" title="Ck变一下形"></a>Ck变一下形</h3><p>$c_k=\sum_{i=0}^{n-1}y_i(\omega_n^{-k})^i$<br>$=\sum_{i=0}^{n-1}(\sum_{j=0}^{n-1}a_j(\omega_n^i)^j)(\omega_n^{-k})^i$<br>$=\sum_{i=0}^{n-1}(\sum_{j=0}^{n-1}a_j(\omega_n^j)^i)(\omega_n^{-k})^i$//为了和后面的i次一样，这里把i和j交换一下<br>$=\sum_{i=0}^{n-1}(\sum_{j=0}^{n-1}a_j(\omega_n^j)^i(\omega_n^{-k})^i)$<br>$=\sum_{i=0}^{n-1}\sum_{j=0}^{n-1}a_j(\omega_n^j)^i(\omega_n^{-k})^i$<br>$=\sum_{i=0}^{n-1}\sum_{j=0}^{n-1}a_j(\omega_n^{j-k})^i$//把同样是i次的两个合并了<br>$=\sum_{j=0}^{n-1}a_j(\sum_{i=0}^{n-1}(\omega_n^{j-k})^i)$</p><h3 id="先简化成S-x-讨论"><a href="#先简化成S-x-讨论" class="headerlink" title="先简化成S(x)讨论"></a>先简化成S(x)讨论</h3><p>接下来设$S(x) = \sum_{i=0}^{n-1} x^i$即从0到n-1次的x相加</p><p>即 $S(x) = \sum_{i=0}^{n-1} (\omega_{n}^{k})^i$</p><p>带入$\omega_n^k$时，如果$k = 0$时显然有$S(\omega_{n}^{k}) = n$</p><p>$k != 0$时，这时候该怎么算</p><p>用类似高中数列的方法，叫啥来着好像叫？反正乘上去做差就完了</p><p>$\omega_{n}^{k}*S(x) - S(x)$</p><p>$=(\omega_n^k)^n-1$</p><p>变形得到</p><p>$S(x) = \frac{1-1}{\omega_n^k -1}$//这里中间省掉了好多的步骤，反正推一下也简单的</p><p>所以可以看出来$k!=0$时，$S(x) = 0$</p><h3 id="讨论完S-x-回去看之前的Ck"><a href="#讨论完S-x-回去看之前的Ck" class="headerlink" title="讨论完S(x),回去看之前的Ck"></a>讨论完S(x),回去看之前的Ck</h3><p>结合之前$ S(x) $的结论，可以知道，$j-k != 0$时，$c_k = 0$ </p><p>即只有$j=k$的时候才不是0，而是n。</p><p>$\sum_{j=0}^{n-1}a_j(\sum_{i=0}^{n-1}(\omega_n^{j-k})^i)$中后面那一项<strong>其实就是n</strong>，还是比较好理解的</p><p>所以有$c_k = n*a_k$</p><p>$a_k = \frac{c_k}{n}$</p><p>这样就得到了点值和系数之间的关系</p><p>其实从点值到系数的这一步也是通过之前的FFT去实现的，非常牛逼</p><h1 id="实现部分"><a href="#实现部分" class="headerlink" title="实现部分"></a>实现部分</h1><h2 id="AC代码"><a href="#AC代码" class="headerlink" title="AC代码"></a>AC代码</h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;cmath&gt;#include&lt;cstdio&gt;using namespace std;inline int read(){    char c=getchar();int x=0,f=1;    while(c&lt;'0'||c&gt;'9'){if(c=='-')f=-1;c=getchar();}    while(c&gt;='0'&amp;&amp;c&lt;='9'){x=x*10+c-'0';c=getchar();}    return x*f;}#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)const int MAXN = 3e6+5;const double Pi = acos(-1.0);struct cplx{//手写负数complex用起来会更快    double x,y;    cplx(double x=0,double y=0):x(x),y(y){}    friend cplx operator + (cplx a,cplx b){return cplx(a.x+b.x,a.y+b.y);}    friend cplx operator - (cplx a,cplx b){return cplx(a.x-b.x,a.y-b.y);}    friend cplx operator * (cplx a,cplx b){return cplx(a.x*b.x-a.y*b.y,a.x*b.y+a.y*b.x);}}a[MAXN],b[MAXN];void FFT(int limit,cplx *a,int type){//limit记录项数    if(limit==1) return;//只有一个常数项结束递归(即只剩下0次的)    cplx a1[limit&gt;&gt;1],a2[limit&gt;&gt;1];//按照奇偶分组    for(int i=0;i&lt;limit;i+=2)        a1[i&gt;&gt;1] = a[i],a2[i&gt;&gt;1] = a[i+1];    FFT(limit&gt;&gt;1,a1,type);    FFT(limit&gt;&gt;1,a2,type);    cplx Wn(cos(2*Pi/limit),type*sin(2*Pi/limit));//单位根    //这里type挺重要的,反变换的时候用-1,因为是-k    cplx w(1,0);//一会儿算单位根幂的时候用w存    for(int i=0;i&lt;(limit&gt;&gt;1);i++,w=w*Wn){//w相当于公式中的w_n^k        a[i]=a1[i]+w*a2[i];//偶        a[i+(limit&gt;&gt;1)]=a1[i]-w*a2[i];//O(1)算另外一部分    }}int main(){    int n=read(),m=read();    rep(i,0,n) a[i].x=read();    rep(i,0,m) b[i].x=read();    int limit = 1;    while(limit&lt;=n+m) limit&lt;&lt;=1;//这里非常精髓    //把长度补到2的幂，这样就不用考虑%2余数的情况    //而且不必担心高次项的系数，因为默认为0    FFT(limit,a,1);    FFT(limit,b,1);    //1表示FFT，-1则是反变换       rep(i,0,limit) a[i] = a[i]*b[i];//转换为点值后直接相乘    FFT(limit,a,-1);//现在变回去    rep(i,0,n+m) printf("%d ",(int)(a[i].x/limit+0.5));//还要除以n的    printf("\n");}//P3803 【模板】多项式乘法（FFT）//https://www.luogu.com.cn/problem/P3803<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="大佬笔误"><a href="#大佬笔误" class="headerlink" title="大佬笔误"></a>大佬笔误</h3><p>发现大佬的笔误在拆分奇偶的地方。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">complex a1[limit&gt;&gt;1],a2[limit&gt;&gt;1];    for(int i=0;i&lt;=limit;i+=2)//根据下标的奇偶性分类        a1[i&gt;&gt;1]=a[i],a2[i&gt;&gt;1]=a[i+1];<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这里显然会出现数组越界的情况，要么数组再开大点，要么&lt;=limit改成&lt;limit。验证过两种都行，不过显然第二种会比较好一点。</p><h1 id="算法优化"><a href="#算法优化" class="headerlink" title="算法优化"></a>算法优化</h1><h2 id="蝴蝶操作"><a href="#蝴蝶操作" class="headerlink" title="蝴蝶操作"></a>蝴蝶操作</h2><h3 id="优化计算了两次的部分"><a href="#优化计算了两次的部分" class="headerlink" title="优化计算了两次的部分"></a>优化计算了两次的部分</h3><p>上面在计算$A_1$和$A_2$时：</p><p>$A( w_{n} ^{k} ) =A_1(w_{ \frac{n}{2}}^{k}) + w_n ^k A_2(w_{ \frac{n}{2}}^{k})$<strong>式子1</strong></p><p>$A( w_n ^{k + \frac{n}{2} } )= A_1(w_{ \frac{n}{2}}^{k}) - w_n ^k A_2(w_{ \frac{n}{2}}^{k})$<strong>式子2</strong></p><p>前一项完全相同，后一项也仅仅是相差一个正负号。但是在之前的代码里我们这一步计算了两次。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">for(int i=0;i&lt;(limit&gt;&gt;1);i++,w=w*Wn){//w相当于公式中的w_n^ka[i]=a1[i]+w*a2[i];//偶a[i+(limit&gt;&gt;1)]=a1[i]-w*a2[i];//O(1)算另外一部分}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>是吧，这里显然是可以优化的</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cplx tmp;for(int i=0;i&lt;(limit&gt;&gt;1);i++,w=w*Wn){//w相当于公式中的w_n^ktmp = w*a2[i];//蝴蝶操作a[i]=a1[i]+tmp;//偶a[i+(limit&gt;&gt;1)]=a1[i]-tmp;//O(1)算另外一部分}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="迭代实现FFT"><a href="#迭代实现FFT" class="headerlink" title="迭代实现FFT"></a>迭代实现FFT</h2><h3 id="观察多次奇偶拆分后下标的变化"><a href="#观察多次奇偶拆分后下标的变化" class="headerlink" title="观察多次奇偶拆分后下标的变化"></a>观察多次奇偶拆分后下标的变化</h3><p><img src="/img/fft/xiabiao.png" alt="xiabiao"></p><p><img src="/img/fft/xiabiao2.png" alt="xiabiao2"></p><p>是按照位逆序置换排列的，$O(n)$来预处理</p><h3 id="如何O-n-预处理位逆序置换"><a href="#如何O-n-预处理位逆序置换" class="headerlink" title="如何O(n)预处理位逆序置换"></a>如何O(n)预处理位逆序置换</h3><p><img src="/img/fft/wnx.jpg" alt="wnx"></p><p>有递推式<code>rev[i] = (rev[i&gt;&gt;1]&gt;&gt;1) | ((i&amp;1)&lt;&lt;(len-1))</code></p><p>解释一下这段代码：</p><ul><li>如图中所说的，每组数字的位逆序置换的<strong>后面几位都是上一级问题的子问题的解</strong></li></ul><p>1为100，2为010 而 4为001，可以看到这一层一层网上推的时候就是把上一层的<strong>往后移动一位</strong></p><p>最前面的<code>(rev[i&gt;&gt;1]&gt;&gt;1)</code>中<code>rev[i&gt;&gt;1]</code>是上一级子问题的解，最后再<code>&gt;&gt;1</code>是往后移动一位</p><ul><li>第一位即<strong>最底层的子问题</strong>——模2余1还是余0，确定首位是1还是0</li></ul><p><code>(i&amp;1)</code>取来下标的最后一位，<code>&lt;&lt;(len-1)</code>把这个数字<strong>移动到首位</strong></p><h3 id="按照位逆序从底层往上进行蝴蝶操作"><a href="#按照位逆序从底层往上进行蝴蝶操作" class="headerlink" title="按照位逆序从底层往上进行蝴蝶操作"></a>按照位逆序从底层往上进行蝴蝶操作</h3><p><img src="/img/fft/wnx2.png" alt="wnx2"></p><p>第二层是2个一组，第三层是4个一组每组里面，即每层每组的个数m为1&lt;&lt;dep</p><ul><li><p>枚举m的数值之后就是每层分成limit/m个<strong>m个元素的组</strong></p></li><li><p>外面一个循环：k表示<strong>每个m元组的起始位置</strong>，每次k要+=m；</p></li><li><p>再来一个循环，j从0到m/2 -1循环：</p></li></ul><p>这样一来，就是k+j和k+j+m/2两两进行蝴蝶操作</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>（基本上就是抄BV1Y7411W73U的，这个视频非常nice）</p><p>之前一直错，结果发现是错在这个地方</p><p><img src="/img/fft/jgxh.png" alt="jgxh"></p><p>a[k+j]的数值已经更改后,算a[k+j+m/2]就有问题</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;cmath&gt;#include&lt;cstdio&gt;using namespace std;inline int read(){    char c=getchar();int x=0,f=1;    while(c&lt;'0'||c&gt;'9'){if(c=='-')f=-1;c=getchar();}    while(c&gt;='0'&amp;&amp;c&lt;='9'){x=x*10+c-'0';c=getchar();}    return x*f;}#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)const int MAXN = 3e6+5;const double Pi = acos(-1.0);struct cplx{//手写负数complex用起来会更快    double x,y;    cplx(double x=0,double y=0):x(x),y(y){}    friend cplx operator + (cplx a,cplx b){return cplx(a.x+b.x,a.y+b.y);}    friend cplx operator - (cplx a,cplx b){return cplx(a.x-b.x,a.y-b.y);}    friend cplx operator * (cplx a,cplx b){return cplx(a.x*b.x-a.y*b.y,a.x*b.y+a.y*b.x);}}a[MAXN],b[MAXN];void FFT(int limit,cplx *a,int type){//limit记录项数    if(limit==1) return;//只有一个常数项结束递归(即只剩下0次的)    cplx a1[limit&gt;&gt;1],a2[limit&gt;&gt;1];//按照奇偶分组    for(int i=0;i&lt;limit;i+=2)        a1[i&gt;&gt;1] = a[i],a2[i&gt;&gt;1] = a[i+1];    FFT(limit&gt;&gt;1,a1,type);    FFT(limit&gt;&gt;1,a2,type);    cplx Wn(cos(2*Pi/limit),type*sin(2*Pi/limit));//单位根    //这里type挺重要的,反变换的时候用-1,因为是-k    cplx w(1,0);//一会儿算单位根幂的时候用w存    cplx tmp;    for(int i=0;i&lt;(limit&gt;&gt;1);i++,w=w*Wn){//w相当于公式中的w_n^k        tmp = w*a2[i];//蝴蝶操作        a[i]=a1[i]+tmp;//偶        a[i+(limit&gt;&gt;1)]=a1[i]-tmp;//O(1)算另外一部分    }}int main(){    int n=read(),m=read();    rep(i,0,n) a[i].x=read();    rep(i,0,m) b[i].x=read();    int limit = 1;    while(limit&lt;=n+m) limit&lt;&lt;=1;//这里非常精髓    //把长度补到2的幂，这样就不用考虑%2余数的情况    //而且不必担心高次项的系数，因为默认为0    FFT(limit,a,1);    FFT(limit,b,1);    //1表示FFT，-1则是反变换       rep(i,0,limit) a[i] = a[i]*b[i];//转换为点值后直接相乘    FFT(limit,a,-1);//现在变回去    rep(i,0,n+m) printf("%d ",(int)(a[i].x/limit+0.5));//还要除以n的    printf("\n");}//P3803 【模板】多项式乘法（FFT）//https://www.luogu.com.cn/problem/P3803<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法学习笔记 </tag>
            
            <tag> 数学 </tag>
            
            <tag> 多项式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Codeforces Round /#679 (Div. 2) C.Perform Easily(二分/集合)</title>
      <link href="2020/10/28/cf-679-c/"/>
      <url>2020/10/28/cf-679-c/</url>
      
        <content type="html"><![CDATA[<h1 id="题目地址"><a href="#题目地址" class="headerlink" title="题目地址"></a>题目地址</h1><p><a href="http://codeforces.com/contest/1435/problem/C">http://codeforces.com/contest/1435/problem/C</a></p><h1 id="大致题意"><a href="#大致题意" class="headerlink" title="大致题意"></a>大致题意</h1><p>有一个长度为6的数组a，代表每根弦的初始值。</p><p>有另外一个数组b，代表需要弹奏的每个音符。</p><p>每根琴弦的琴格从1开始。（在弹吉他里应该叫x品吧）</p><p>如果我要弹音符$b_i$,找到$x$满足$b_i=x+a_j $则可以按琴弦j的琴格$x$来弹出这个音。（$j$弦$x$品）</p><p>问怎么弹所有音符可以让最大的$x$和最小的$x$之间的差值最少？</p><h1 id="题目思路"><a href="#题目思路" class="headerlink" title="题目思路"></a>题目思路</h1><h2 id="方法1：二分"><a href="#方法1：二分" class="headerlink" title="方法1：二分"></a>方法1：二分</h2><p>方法来自大佬的博客：<a href="https://www.cnblogs.com/acceptedzhs/p/13876630.html">https://www.cnblogs.com/acceptedzhs/p/13876630.html</a></p><p>先对ab两个数组进行排序并且去重（这里a的去重是必要的，并不仅仅是为了速度）</p><p>设a的长度为m，b的长度为n，我们枚举n和m，复杂度$O(n) $</p><p>即对音符和琴弦进行枚举，$minx = b_j - a_i $,设$minx$就是之后最小的琴格，比$minx $还小的就是不可行的了。</p><p>我们定义”最悲惨的音符”是那些“刚好可以使用琴弦a[k]却无法使用琴弦$a_{k-1} $中的音符”里最大的。</p><p><img src="%5Cimg%5Ccf-679-c%5Ctj.png" alt="tj"></p><p>显然这样的音符最多只有$m-1 $个。（之前去重就是因为这里，不然$a_k = a_{k-1} $就出错了。</p><p>如果$b_1 &lt; minx + a_1 $,则不可能存在”能使用$a_1 $但无法使用$a_2 $的音符”，对于这种情况我们直接判定这个$minx$不合法，跳过。</p><p>琴格的最大位置一定在这些“最悲惨的音符”和最大的那个音符中产生。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;#include&lt;map&gt;#include&lt;queue&gt;#include&lt;cmath&gt;using namespace std;template&lt;class T&gt;inline void read(T &amp;x){x=0;char o,f=1;while(o=getchar(),o&lt;48)if(o==45)f=-f;do x=(x&lt;&lt;3)+(x&lt;&lt;1)+(o^48);while(o=getchar(),o&gt;47);x*=f;}int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define ull unsigned long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=b;i--)#define mkp make_pair#define ft first#define sd second#define log(x) (31-__builtin_clz(x))#define INF 0x3f3f3f3ftypedef pair&lt;int,int&gt; pii;typedef pair&lt;ll,ll&gt; pll;ll gcd(ll a,ll b){ while(b^=a^=b^=a%=b); return a; }//#define INF 0x7fffffffconst int MAXN = 1e5+5;ll a[8];ll b[MAXN];int n;void solve(){int m = 6;rep(i,1,m) cin&gt;&gt;a[i];sort(a+1,a+1+m);m = unique(a+1,a+1+m)-a-1;cin&gt;&gt;n;rep(i,1,n) cin&gt;&gt;b[i];sort(b+1,b+1+n);n = unique(b+1,b+1+n)-b-1;//读入两个数组，排序去重ll minx,maxx,minn = 1ll&lt;&lt;62;rep(i,1,n){rep(j,1,m){maxx = 0;minx = b[i]-a[j];if(b[1]-a[1]&lt;minx) continue;rep(k,1,m){//注意琴格从1开始//b[?]-a[k]&gt;=minx 变形 b[?]&gt;=minx+a[k]int px = lower_bound(b+1,b+1+n,minx+a[k])-b-1;//找到最后一个可以用a[k-1]但一定用不了a[k]的maxx = max(b[px]-a[k-1],maxx);}repb(k,m,1){if(b[n]-a[k]&gt;=minx){//和音符中的最大值进行比较,并且找一根尽可能大的琴弦maxx = max(maxx,b[n]-a[k]);break;}}minn = min(maxx-minx,minn);}}cout&lt;&lt;minn&lt;&lt;endl;}int main(){solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="方法2：集合"><a href="#方法2：集合" class="headerlink" title="方法2：集合"></a>方法2：集合</h2><p>来自另一个大佬的博客：<a href="https://blog.csdn.net/I_have_a_world/article/details/109287448">https://blog.csdn.net/I_have_a_world/article/details/109287448</a></p><p>我更喜欢这第二种方法，复杂度上应该都是$O(nlogn)$的。但是这个方法真的挺简洁易懂的。</p><p>两个数组可以不去重,但是去重的话效率应该会更高。</p><p>搞一个in[MAXN]数组，记录当前这个音符所在的弦。</p><p>设排序去重后有n个音符，m个琴弦。</p><p>一开始每个音符都在弦m上，用pair&lt;ll,ll&gt;来记录，分别记录$b_i - a_{in[i]} $和编号$i$。</p><p>把这些二元组都存在集合set里，然后每次取出first最小的(即$b_i - a_{in[i]} $)最大的元素i，和集合中first最大的做差。在这些数值中找最小值。</p><p>这样以后，把最小的i对应的in[i]–，即让他变得更大再加入集合。</p><p>重复这些步骤直到最小的元素也是用的1弦。为什么？因为这时候其他弦如果再往下换弦，也只能使$b_i - a_{in[i]} $变得更大，使最大-最小的值变大或者不变，所以到这里就没必要继续往下了。</p><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;#include&lt;set&gt;#include&lt;cmath&gt;using namespace std;int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=b;i--)#define mkp make_pair#define ft first#define sd secondtypedef pair&lt;ll,ll&gt; pll;const int MAXN = 1e5+5;ll a[8];ll b[MAXN];int in[MAXN];int n;set&lt;pll&gt;st;void solve(){int m = 6;rep(i,1,m) cin&gt;&gt;a[i];sort(a+1,a+1+m);m = unique(a+1,a+1+m)-a-1;cin&gt;&gt;n;rep(i,1,n) cin&gt;&gt;b[i];    sort(b+1,b+1+n);n = unique(b+1,b+1+n)-b-1;//这里其实没太大必要去重，但是我想了一下还是去重了    rep(i,1,n){        in[i] = m;        st.insert(mkp(b[i]-a[m],(ll)i));//一开始都是用m弦弹的    }    ll minn = st.rbegin()-&gt;ft-st.begin()-&gt;ft;    while(1){        pll now = *st.begin();        st.erase(st.begin());//这边的参数是迭代器,这样删复杂度更小        if(in[now.sd]==1) break;        //如果最小的都是1,那么其他的音符换弦也不可能让结果更小了,因为往下换只会让bi-aj更大        in[now.sd]--;//每次换成更小的弦        st.insert(mkp(b[now.sd]-a[in[now.sd]],now.sd));        minn = min(minn,st.rbegin()-&gt;ft-st.begin()-&gt;ft);    }    cout&lt;&lt;minn&lt;&lt;endl;}int main(){    solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 题解 </tag>
            
            <tag> Codeforces </tag>
            
            <tag> 二分 </tag>
            
            <tag> 集合 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【周记】2020年10月25日</title>
      <link href="2020/10/25/zhouji1/"/>
      <url>2020/10/25/zhouji1/</url>
      
        <content type="html"><![CDATA[<h1 id="每日小记"><a href="#每日小记" class="headerlink" title="每日小记"></a>每日小记</h1><h2 id="10-21"><a href="#10-21" class="headerlink" title="10-21"></a>10-21</h2><p>今天上午睡觉和上概率论，概率论课上有点开小差了，不过还好今天的课比较简单。</p><p>下午大职课Div3水了四题，下课之后到攀峰自习，补了raif round的E题。</p><p>晚上开始看F，发现是扫描线算法，开始学扫描线。</p><p>回寝室之后摸鱼玩异星工场了。</p><h2 id="10-22"><a href="#10-22" class="headerlink" title="10-22"></a>10-22</h2><p>运动会四天休假的第一天。</p><p>中午到攀峰学习。</p><p>看板子的时候发现unique函数可以取出相邻重复元素，返回值是序列末尾的迭代器。</p><p>14:44 摸完了扫描线板子，WA了好久就因为线段树没开足够大，因为一条线有两个端点，所以线段树需要开到扫描线数量的8倍，也就是四边形数量的16倍，我少开了2倍。扫描线真的是很奇妙的算法，思路非常简洁，然后具体实现的时候一些细节(边位置上的处理)比较细节。<br>$$<br>线段树上点k代表的区间其实是 [x_{k},x_{k+1}),x_{k}是k离散化前的数值。<br>$$<br>15:15 开了一场vp-cf#672)最近打算多vp练英语和代码能力。</p><p>16:12 前面三题做的比较顺利，C2没什么想法，故摸了</p><h2 id="10-23"><a href="#10-23" class="headerlink" title="10-23"></a>10-23</h2><p>运动会四天休假的第二天。</p><p>上完实验课，15:42来到攀峰。</p><p>16:00开始vp-cf#660，这场很拉垮，AB还算正常，C一开始就想错了，发现自己树形DP是真的不行,这几天可以加强一下。</p><p>晚上吃完饭回来18:40又开了一场vp cf#655 AB题很顺利，C比较水，到D题没啥想法了看了看题解知道是分奇偶做前缀和然后就过了。</p><p>20:30开了vp-cf-gr9 AB很顺利，C一开始想的太复杂了，一小时的时候删了一大段代码结果过了，这时候是21:38，打算回寝室休息了。</p><p>今天打了三场vp，感觉身体被掏空，最后整理一下就回寝室了。</p><h2 id="10-24"><a href="#10-24" class="headerlink" title="10-24"></a>10-24</h2><p>运动会四天休假的第三天。</p><p>今天稍微有点怠惰，去电信营业厅把wifi密码固定了，之后就不用再每次发短信问密码了。</p><p>下午15:30打了vp-cf#581，还是照例过了ABC三题。这几天练下来发现自己切简单题的速度快了不少，基本最差一个小时内都能搞定。</p><p>晚上22:05有一场cf 678 。ABC都比较顺利，D题非常惨，一直交一直错，也是树形DP题，真的该练练，那就明天练吧。</p><h2 id="10-25"><a href="#10-25" class="headerlink" title="10-25"></a>10-25</h2><p>现在是1:18，计划今天把前几天的题补了，并且写一下树形DP，先定个小目标，5题树形DP，有空余时间的话把FFT结束一下。</p><h1 id="一周总结"><a href="#一周总结" class="headerlink" title="一周总结"></a>一周总结</h1><h1 id="大王真可爱"><a href="#大王真可爱" class="headerlink" title="大王真可爱"></a>大王真可爱</h1><p><img src="/img/zhouji1/altera.jpg" alt="altera"></p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 周记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Codeforces Round /#676 (Div. 2) D. Hexagons 题解</title>
      <link href="2020/10/20/cf-676/"/>
      <url>2020/10/20/cf-676/</url>
      
        <content type="html"><![CDATA[<h1 id="题意"><a href="#题意" class="headerlink" title="题意"></a>题意</h1><p><img src="/img/cf-676/cft.png" alt="cft"> </p><p>​        有这样一个六边形坐标系，并给了六个移动方向C1-6。</p><p><img src="/img/cf-676/cf2.png" alt="cf2"> </p><p>​        输入给定目标坐标xy，并且给定C1-6每个方向的费用，求从(0,0)到(x,y)的最小费用。</p><h1 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h1><p>​        我们可以发现，要进行一次C2的移动，可以通过一次C1和一次C3合成，同理如果向进行一次C1移动，也可以通过C6和C2进行合成。所以我们先进行一次类似最短路的操作，如果一个方向x的费用比它左右x+1,x-1相加的还要多，则需要把这个费用替换。</p><p>​        这样操作之后，我们可以得到如果要从(0,0)到(x,y)，最多只需要走用到两个方向。（注意，这里的方向经过了之前的操作，其实和一开始的移动是不一样的）</p><p>​        之后我们枚举选哪两种方向就行了。我比赛的时候选择的方法比较傻逼，是给定一个坐标(x,y)，我先枚举：要先搞定x还是先搞定y，然后判断x(或y)变成0之后，需要用哪一个方向，套了两重循环，比较不科学。后来看了大佬们的代码发现有效率更高更好写的方法。</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>比赛时的代码</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;cstdio&gt;#include&lt;algorithm&gt;#include&lt;vector&gt;#include&lt;map&gt;#include&lt;queue&gt;#include&lt;cmath&gt;using namespace std;template&lt;class T&gt;inline void read(T &amp;x){x=0;char o,f=1;while(o=getchar(),o&lt;48)if(o==45)f=-f;do x=(x&lt;&lt;3)+(x&lt;&lt;1)+(o^48);while(o=getchar(),o&gt;47);x*=f;}int cansel_sync=(ios::sync_with_stdio(0),cin.tie(0),0);#define ll long long#define ull unsigned long long#define rep(i,a,b) for(int i=(a);i&lt;=(b);i++)#define repb(i,a,b) for(int i=(a);i&gt;=b;i--)#define log(x) (31-__builtin_clz(x))#define INF 0x3f3f3f3f#define int llll gcd(ll a,ll b){ while(b^=a^=b^=a%=b); return a; }//#define INF 0x7fffffffint dir[6][2] = {1,1,0,1, -1,0,-1,-1, 0,-1,1,0};ll c[6];ll tx,ty;ll res;inline ll cd(int x){ return c[ (x+6)%6 ]; }inline bool th(ll a,ll b){//判断同号if(a==b&amp;&amp;a==0) return 1;else if(a&gt;0&amp;&amp;b&gt;0) return 1;else if(a&lt;0&amp;&amp;b&lt;0) return 1;return 0;}void solve(){res = 0;cin&gt;&gt;tx&gt;&gt;ty;rep(i,0,5) cin&gt;&gt;c[i];//费用;rep(i,0,5){//最短路思想if(cd(i-1)+cd(i+1)&lt;cd(i)) c[i] = cd(i-1)+cd(i+1);}ll minn = 1LL&lt;&lt;62;//我懂了，最多只需要两种方向。//先把tx弄到相同的方案//先搞x再搞y的情况rep(i,0,5){res = 0;ll tty = ty;ll ttx = tx;if(dir[i][0]==0) continue;if(th(dir[i][0],ttx)||ttx==0){ll tim = abs(ttx);tty -= tim*dir[i][1];ttx = 0;res += c[i]*tim;rep(j,0,5){if(dir[j][0]!=0) continue;if(dir[j][1]==0) continue;if(th(dir[j][1],tty)||tty==0){ll tim = abs(tty);ll res2 = res + c[j]*tim;minn = min(minn,res2);//cout&lt;&lt;i&lt;&lt;' '&lt;&lt;j&lt;&lt;':'&lt;&lt;res&lt;&lt;' '&lt;&lt;res2&lt;&lt;endl;}}}}//先搞y再搞xrep(i,0,5){res = 0;ll tty = ty;ll ttx = tx;if(dir[i][1]==0) continue;if(th(dir[i][1],tty)||tty==0){ll tim = abs(tty);ttx -= tim*dir[i][0];tty = 0;res += c[i]*tim;rep(j,0,5){if(dir[j][1]!=0) continue;if(dir[j][0]==0) continue;if(th(dir[j][0],ttx)||ttx==0){ll tim = abs(ttx);ll res2 = res + c[j]*tim;minn = min(minn,res2);//cout&lt;&lt;i&lt;&lt;' '&lt;&lt;j&lt;&lt;':'&lt;&lt;res&lt;&lt;' '&lt;&lt;res2&lt;&lt;endl;}}}}cout&lt;&lt;minn&lt;&lt;endl;}signed main(){int z;cin&gt;&gt;z;while(z--) solve();}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 题解 </tag>
            
            <tag> Codeforces </tag>
            
            <tag> 思维 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>To Begin With</title>
      <link href="2020/10/20/first-blog/"/>
      <url>2020/10/20/first-blog/</url>
      
        <content type="html"><![CDATA[<h2 id="为什么要写博客"><a href="#为什么要写博客" class="headerlink" title="为什么要写博客"></a>为什么要写博客</h2><p>​        其实没有什么非写不可的理由，只是看lhd他们啥都会，就想着自己也学着搞点东西。</p><p>​        之前都是在CSDN上用的博客，方便是方便，但是看转爷他们的博客那么好看其实还挺羡慕的，所以今天闲着就想着搭一个博客看看。</p><h2 id="参考的帖子"><a href="#参考的帖子" class="headerlink" title="参考的帖子"></a>参考的帖子</h2><p>Hexo的基本使用参考了</p><p><a href="https://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html">https://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.html</a></p><p>Hexo环境变量配置</p><p><a href="https://blog.csdn.net/weixin_30673611/article/details/94825385">https://blog.csdn.net/weixin_30673611/article/details/94825385</a></p><p><em>一开始没有加到path里面而是新建了一个变量，后来发现要加到path里面才行</em></p><p>用Gitee更快clone仓库</p><p><a href="https://blog.csdn.net/huitailang2020/article/details/104813219">https://blog.csdn.net/huitailang2020/article/details/104813219</a></p><p>本博客用的主题</p><p><a href="https://zhuanlan.zhihu.com/p/137679728">https://zhuanlan.zhihu.com/p/137679728</a></p><p>fluid主题的配置指南</p><p><a href="https://fluid-dev.github.io/hexo-fluid-docs/guide/">https://fluid-dev.github.io/hexo-fluid-docs/guide/</a></p><p>npm安装</p><p><a href="https://www.cnblogs.com/lgx5/p/10732016.html">https://www.cnblogs.com/lgx5/p/10732016.html</a></p><p>渲染MathJax数学公式-解决了我内联公式渲染失败的问题</p><p><a href="https://www.jianshu.com/p/7ab21c7f0674">https://www.jianshu.com/p/7ab21c7f0674</a></p><p>给文章加密</p><p><a href="https://www.jianshu.com/p/44e211829447?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation">https://www.jianshu.com/p/44e211829447?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation</a></p><p><a href="https://blog.csdn.net/weixin_45750972/article/details/111396124">https://blog.csdn.net/weixin_45750972/article/details/111396124</a></p>]]></content>
      
      
      <categories>
          
          <category> 博客维护 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客维护 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
